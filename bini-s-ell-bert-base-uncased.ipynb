{"cells":[{"cell_type":"code","execution_count":null,"id":"3311fbe8","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-10-17T06:19:49.859700Z","iopub.status.busy":"2022-10-17T06:19:49.859130Z","iopub.status.idle":"2022-10-17T06:19:49.881597Z","shell.execute_reply":"2022-10-17T06:19:49.880505Z"},"papermill":{"duration":0.06758,"end_time":"2022-10-17T06:19:49.888165","exception":false,"start_time":"2022-10-17T06:19:49.820585","status":"completed"},"tags":[],"id":"3311fbe8"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5edd7hxDkJjw","executionInfo":{"status":"ok","timestamp":1666692591435,"user_tz":-540,"elapsed":25869,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"1a66e07d-b8f4-4178-f678-4caee8c816f7"},"id":"5edd7hxDkJjw","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"082695ee","metadata":{"papermill":{"duration":0.025377,"end_time":"2022-10-17T06:20:02.296892","exception":false,"start_time":"2022-10-17T06:20:02.271515","status":"completed"},"tags":[],"id":"082695ee"},"source":["# Import Dataset"]},{"cell_type":"code","source":["submission = pd.read_csv('/content/drive/MyDrive/ㄱ/sample_submission.csv')\n","train = pd.read_csv('/content/drive/MyDrive/ㄱ/train.csv')\n","test = pd.read_csv('/content/drive/MyDrive/ㄱ/test.csv')"],"metadata":{"id":"fbYj_ijTkInF"},"id":"fbYj_ijTkInF","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"d19736a9","metadata":{"papermill":{"duration":0.024954,"end_time":"2022-10-17T06:19:49.954424","exception":false,"start_time":"2022-10-17T06:19:49.929470","status":"completed"},"tags":[],"id":"d19736a9"},"source":["# GPU"]},{"cell_type":"code","execution_count":null,"id":"6d8bb353","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:19:50.016447Z","iopub.status.busy":"2022-10-17T06:19:50.015950Z","iopub.status.idle":"2022-10-17T06:19:51.722201Z","shell.execute_reply":"2022-10-17T06:19:51.721185Z"},"papermill":{"duration":1.745302,"end_time":"2022-10-17T06:19:51.724673","exception":false,"start_time":"2022-10-17T06:19:49.979371","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"6d8bb353","executionInfo":{"status":"ok","timestamp":1666692597686,"user_tz":-540,"elapsed":2310,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"033ba7cf-8e7a-4a08-b290-efcabe51c407"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}],"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"markdown","id":"8b3da8c3","metadata":{"papermill":{"duration":0.024875,"end_time":"2022-10-17T06:19:51.775286","exception":false,"start_time":"2022-10-17T06:19:51.750411","status":"completed"},"tags":[],"id":"8b3da8c3"},"source":["# installing Transformers"]},{"cell_type":"code","execution_count":null,"id":"154445f3","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:19:51.828511Z","iopub.status.busy":"2022-10-17T06:19:51.828011Z","iopub.status.idle":"2022-10-17T06:20:02.242765Z","shell.execute_reply":"2022-10-17T06:20:02.241539Z"},"papermill":{"duration":10.444237,"end_time":"2022-10-17T06:20:02.245303","exception":false,"start_time":"2022-10-17T06:19:51.801066","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"154445f3","executionInfo":{"status":"ok","timestamp":1666692607717,"user_tz":-540,"elapsed":10033,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"8cb44753-fb13-48ee-f603-621d381db604"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n","\u001b[K     |████████████████████████████████| 5.3 MB 34.6 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 73.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 70.3 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","id":"8d391673","metadata":{"papermill":{"duration":0.030285,"end_time":"2022-10-17T06:20:02.633487","exception":false,"start_time":"2022-10-17T06:20:02.603202","status":"completed"},"tags":[],"id":"8d391673"},"source":["# Parsing the dataset\n","### 판다스 사용해서 train 데이터를 구문 분석하고 속성 및 데이터 포인트 살펴보기"]},{"cell_type":"code","execution_count":null,"id":"4bef1e31","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:20:02.690278Z","iopub.status.busy":"2022-10-17T06:20:02.689233Z","iopub.status.idle":"2022-10-17T06:20:02.705999Z","shell.execute_reply":"2022-10-17T06:20:02.705083Z"},"papermill":{"duration":0.045855,"end_time":"2022-10-17T06:20:02.708261","exception":false,"start_time":"2022-10-17T06:20:02.662406","status":"completed"},"tags":[],"id":"4bef1e31"},"outputs":[],"source":["#grammar data만 사용하기 위해서 나머지 평가항목 컬럼은 드롭\n","train.drop(['cohesion','syntax','vocabulary','phraseology','conventions'], inplace=True, axis=1)"]},{"cell_type":"code","execution_count":null,"id":"db8f0cd7","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:20:02.761951Z","iopub.status.busy":"2022-10-17T06:20:02.761629Z","iopub.status.idle":"2022-10-17T06:20:02.783450Z","shell.execute_reply":"2022-10-17T06:20:02.781300Z"},"papermill":{"duration":0.052334,"end_time":"2022-10-17T06:20:02.786693","exception":false,"start_time":"2022-10-17T06:20:02.734359","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"db8f0cd7","executionInfo":{"status":"ok","timestamp":1666692607718,"user_tz":-540,"elapsed":10,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"c055aad1-1bc0-49a4-df50-47717813a202"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of trainig sentences: 3,911\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["           text_id                                          full_text  grammar\n","1082  52DC3B7C3952  Working From Home\\n\\nShould students be allowe...      5.0\n","3411  EA42A6028804  People argue whether or not its a good idea fo...      3.0\n","1909  8FF22F2804A3  A city council is debating the adoption of a 1...      2.0\n","3801  FB4D3D883D49  Cellphones has already taken place of your cal...      2.0\n","257   12CA89146E91  It is true that your character will be what yo...      2.5\n","277   144671E445D0  A British naturalist and politician John Lubbo...      2.5\n","2848  CEF302996231  Dear Dr. Generic_Name,\\n\\nThank you for taking...      3.0\n","3579  F1CB29904BBD  My dad is someone who has shared wisdom and ex...      2.5\n","3826  FC831CB27E25  Although some people might say, if the student...      2.5\n","1990  9685DD18847A  Some students don't have an idea of what caree...      3.0"],"text/html":["\n","  <div id=\"df-e0fd205b-d509-4981-bcca-301f9d2e245f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_id</th>\n","      <th>full_text</th>\n","      <th>grammar</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1082</th>\n","      <td>52DC3B7C3952</td>\n","      <td>Working From Home\\n\\nShould students be allowe...</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>3411</th>\n","      <td>EA42A6028804</td>\n","      <td>People argue whether or not its a good idea fo...</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>1909</th>\n","      <td>8FF22F2804A3</td>\n","      <td>A city council is debating the adoption of a 1...</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3801</th>\n","      <td>FB4D3D883D49</td>\n","      <td>Cellphones has already taken place of your cal...</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>257</th>\n","      <td>12CA89146E91</td>\n","      <td>It is true that your character will be what yo...</td>\n","      <td>2.5</td>\n","    </tr>\n","    <tr>\n","      <th>277</th>\n","      <td>144671E445D0</td>\n","      <td>A British naturalist and politician John Lubbo...</td>\n","      <td>2.5</td>\n","    </tr>\n","    <tr>\n","      <th>2848</th>\n","      <td>CEF302996231</td>\n","      <td>Dear Dr. Generic_Name,\\n\\nThank you for taking...</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>3579</th>\n","      <td>F1CB29904BBD</td>\n","      <td>My dad is someone who has shared wisdom and ex...</td>\n","      <td>2.5</td>\n","    </tr>\n","    <tr>\n","      <th>3826</th>\n","      <td>FC831CB27E25</td>\n","      <td>Although some people might say, if the student...</td>\n","      <td>2.5</td>\n","    </tr>\n","    <tr>\n","      <th>1990</th>\n","      <td>9685DD18847A</td>\n","      <td>Some students don't have an idea of what caree...</td>\n","      <td>3.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0fd205b-d509-4981-bcca-301f9d2e245f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e0fd205b-d509-4981-bcca-301f9d2e245f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e0fd205b-d509-4981-bcca-301f9d2e245f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}],"source":["print('Number of trainig sentences: {:,}\\n'.format(train.shape[0]))\n","train.sample(10)"]},{"cell_type":"markdown","id":"4acfafe0","metadata":{"papermill":{"duration":0.026931,"end_time":"2022-10-17T06:20:02.841539","exception":false,"start_time":"2022-10-17T06:20:02.814608","status":"completed"},"tags":[],"id":"4acfafe0"},"source":["문법의 점수를 예측할 것이기 때문에 full_tex와 grammar만 남겨둠"]},{"cell_type":"markdown","id":"98668419","metadata":{"papermill":{"duration":0.061168,"end_time":"2022-10-17T06:20:02.953751","exception":false,"start_time":"2022-10-17T06:20:02.892583","status":"completed"},"tags":[],"id":"98668419"},"source":["훈련 세트의 문장과 레이블을 numpy ndarray로 추출"]},{"cell_type":"code","execution_count":null,"id":"c4e4983a","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:20:03.049302Z","iopub.status.busy":"2022-10-17T06:20:03.048732Z","iopub.status.idle":"2022-10-17T06:20:03.056154Z","shell.execute_reply":"2022-10-17T06:20:03.055158Z"},"papermill":{"duration":0.056577,"end_time":"2022-10-17T06:20:03.061145","exception":false,"start_time":"2022-10-17T06:20:03.004568","status":"completed"},"tags":[],"id":"c4e4983a"},"outputs":[],"source":["# Get the lists of sentences and their labels.\n","full_text = train.full_text.values\n","labels = train.grammar.values"]},{"cell_type":"markdown","id":"5f19962c","metadata":{"papermill":{"duration":0.046031,"end_time":"2022-10-17T06:20:03.155983","exception":false,"start_time":"2022-10-17T06:20:03.109952","status":"completed"},"tags":[],"id":"5f19962c"},"source":["# Bert Tokenizer\n","\n","### Bert에 텍스트를 넣으려면 토큰으로 분할되어야 하고, 그런 다음 토큰을 토크나이저 어휘의 인덱스에 매핑해야 함."]},{"cell_type":"markdown","id":"f28498f3","metadata":{"papermill":{"duration":0.041003,"end_time":"2022-10-17T06:20:03.242182","exception":false,"start_time":"2022-10-17T06:20:03.201179","status":"completed"},"tags":[],"id":"f28498f3"},"source":["#### Tokenization(토큰화)는 BERT에 포함된 토크나이저에서 수행함. +\"uncased\" 버전 사용"]},{"cell_type":"code","execution_count":null,"id":"0beee057","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:20:03.325213Z","iopub.status.busy":"2022-10-17T06:20:03.324803Z","iopub.status.idle":"2022-10-17T06:20:06.819810Z","shell.execute_reply":"2022-10-17T06:20:06.817735Z"},"papermill":{"duration":3.541179,"end_time":"2022-10-17T06:20:06.824698","exception":false,"start_time":"2022-10-17T06:20:03.283519","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":131,"referenced_widgets":["bd0f95f7219840d79aa28b4c10ba7f9f","0b4647c678b342cb90b0c3f12a2cea35","f0c794c598354ff1a32f2693a4616d25","08beaba592f24cffaa0bceb2cbf22543","146c1ece5c934e4ead5a79e16c6c083b","b23c63eb24404de592859f483666f8ab","ad60a578e60e4ac8a45f4d57bb84553d","2cba5445cbf349d8ade2563d159c7367","d5178dc046714a459a7939c0c75da907","02ed582a2f8d4279b03eacaf17c608f1","6cf05efd7c2e49cabe5bbf9116d9db0d","d1b574c56b264ca2af87375c8fca9abd","8c93d3b58588453bbd096672d25f162e","bf61d9eeff534fbca45a295f2e509b6e","af9de7feb5fb4f1ba3ffbeea54754506","6d55c4511dfa4a0a85882fec96a099c5","9df4c73bb5f5459b82f13716325f809b","27b1dbce02a548828050665a820243ac","a20319801e91467fabf58c53254fac25","58b0b9b603004a6abe0eeb2a67c8ae97","e961b5d6068f4ed3b26e19538f7743da","db62e87b0a774949a42668ff62ee6cdb","05757a97bb81436b87385bc79ff1694e","2fdd488e020c4ace9b53859ebecddb46","cfe4295a0ece412fbb4b21bc5effb740","5c513beab1d04e78b7a7822aad19f394","d54abf991b164dd38732ab5415cc6ec4","8638a2e18dad493683d8f1bf6b1d5604","6f45571411e14bfe8172d4497f6f5e61","88bf39642baa419ca39abb0f1b902236","704a26a08d564e84aab058fe887691b6","ba04869b467f41bc8164de85d8411f8f","cf264868eb18473dabbdd3ed71052129"]},"id":"0beee057","executionInfo":{"status":"ok","timestamp":1666692617105,"user_tz":-540,"elapsed":9393,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"c4ba05d6-9e47-499a-f898-539448537dc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Bert Tokenizer...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0f95f7219840d79aa28b4c10ba7f9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1b574c56b264ca2af87375c8fca9abd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05757a97bb81436b87385bc79ff1694e"}},"metadata":{}}],"source":["from transformers import BertTokenizer\n","\n","print(\"Loading Bert Tokenizer...\")\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"]},{"cell_type":"markdown","id":"3c4c2fb1","metadata":{"papermill":{"duration":0.038336,"end_time":"2022-10-17T06:20:06.919024","exception":false,"start_time":"2022-10-17T06:20:06.880688","status":"completed"},"tags":[],"id":"3c4c2fb1"},"source":["### 한 문장에 토크나이저 적용 해보기"]},{"cell_type":"code","execution_count":null,"id":"bf192718","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:20:07.007208Z","iopub.status.busy":"2022-10-17T06:20:07.006757Z","iopub.status.idle":"2022-10-17T06:20:07.038943Z","shell.execute_reply":"2022-10-17T06:20:07.037813Z"},"papermill":{"duration":0.084604,"end_time":"2022-10-17T06:20:07.041705","exception":false,"start_time":"2022-10-17T06:20:06.957101","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"bf192718","executionInfo":{"status":"ok","timestamp":1666692617105,"user_tz":-540,"elapsed":4,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"d194dd74-1f02-43e5-a11e-8ef088eb236c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original :  I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortable at home.\n","\n","The hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and stay home and you wont need to stress about what to wear.\n","\n","most students usually take showers before school. they either take it before they sleep or when they wake up. some students do both to smell good. that causes them do miss the bus and effects on there lesson time cause they come late to school. when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go.\n","\n","when your home your comfortable and you pay attention. it gives then an advantage to be smarter and even pass there classmates on class work. public schools are difficult even if you try. some teacher dont know how to teach it in then way that students understand it. that causes students to fail and they may repeat the class.              \n","Tokenized :  ['i', 'think', 'that', 'students', 'would', 'benefit', 'from', 'learning', 'at', 'home', ',', 'because', 'they', 'won', '##t', 'have', 'to', 'change', 'and', 'get', 'up', 'early', 'in', 'the', 'morning', 'to', 'shower', 'and', 'do', 'there', 'hair', '.', 'taking', 'only', 'classes', 'helps', 'them', 'because', 'at', 'there', 'house', 'they', \"'\", 'll', 'be', 'pay', 'more', 'attention', '.', 'they', 'will', 'be', 'comfortable', 'at', 'home', '.', 'the', 'hardest', 'part', 'of', 'school', 'is', 'getting', 'ready', '.', 'you', 'wake', 'up', 'go', 'brush', 'your', 'teeth', 'and', 'go', 'to', 'your', 'closet', 'and', 'look', 'at', 'your', 'cloth', '##s', '.', 'after', 'you', 'think', 'you', 'picked', 'a', 'outfit', 'u', 'go', 'look', 'in', 'the', 'mirror', 'and', 'you', '##ll', 'either', 'not', 'like', 'it', 'or', 'you', 'look', 'and', 'see', 'a', 'stain', '.', 'then', 'you', \"'\", 'll', 'have', 'to', 'change', '.', 'with', 'the', 'online', 'classes', 'you', 'can', 'wear', 'anything', 'and', 'stay', 'home', 'and', 'you', 'won', '##t', 'need', 'to', 'stress', 'about', 'what', 'to', 'wear', '.', 'most', 'students', 'usually', 'take', 'showers', 'before', 'school', '.', 'they', 'either', 'take', 'it', 'before', 'they', 'sleep', 'or', 'when', 'they', 'wake', 'up', '.', 'some', 'students', 'do', 'both', 'to', 'smell', 'good', '.', 'that', 'causes', 'them', 'do', 'miss', 'the', 'bus', 'and', 'effects', 'on', 'there', 'lesson', 'time', 'cause', 'they', 'come', 'late', 'to', 'school', '.', 'when', 'u', 'have', 'online', 'classes', 'u', 'won', '##t', 'need', 'to', 'miss', 'lessons', 'cause', 'you', 'can', 'get', 'everything', 'set', 'up', 'and', 'go', 'take', 'a', 'shower', 'and', 'when', 'u', 'get', 'out', 'your', 'ready', 'to', 'go', '.', 'when', 'your', 'home', 'your', 'comfortable', 'and', 'you', 'pay', 'attention', '.', 'it', 'gives', 'then', 'an', 'advantage', 'to', 'be', 'smarter', 'and', 'even', 'pass', 'there', 'classmates', 'on', 'class', 'work', '.', 'public', 'schools', 'are', 'difficult', 'even', 'if', 'you', 'try', '.', 'some', 'teacher', 'don', '##t', 'know', 'how', 'to', 'teach', 'it', 'in', 'then', 'way', 'that', 'students', 'understand', 'it', '.', 'that', 'causes', 'students', 'to', 'fail', 'and', 'they', 'may', 'repeat', 'the', 'class', '.']\n","Token IDs :  [1045, 2228, 2008, 2493, 2052, 5770, 2013, 4083, 2012, 2188, 1010, 2138, 2027, 2180, 2102, 2031, 2000, 2689, 1998, 2131, 2039, 2220, 1999, 1996, 2851, 2000, 6457, 1998, 2079, 2045, 2606, 1012, 2635, 2069, 4280, 7126, 2068, 2138, 2012, 2045, 2160, 2027, 1005, 2222, 2022, 3477, 2062, 3086, 1012, 2027, 2097, 2022, 6625, 2012, 2188, 1012, 1996, 18263, 2112, 1997, 2082, 2003, 2893, 3201, 1012, 2017, 5256, 2039, 2175, 8248, 2115, 4091, 1998, 2175, 2000, 2115, 9346, 1998, 2298, 2012, 2115, 8416, 2015, 1012, 2044, 2017, 2228, 2017, 3856, 1037, 11018, 1057, 2175, 2298, 1999, 1996, 5259, 1998, 2017, 3363, 2593, 2025, 2066, 2009, 2030, 2017, 2298, 1998, 2156, 1037, 21101, 1012, 2059, 2017, 1005, 2222, 2031, 2000, 2689, 1012, 2007, 1996, 3784, 4280, 2017, 2064, 4929, 2505, 1998, 2994, 2188, 1998, 2017, 2180, 2102, 2342, 2000, 6911, 2055, 2054, 2000, 4929, 1012, 2087, 2493, 2788, 2202, 23442, 2077, 2082, 1012, 2027, 2593, 2202, 2009, 2077, 2027, 3637, 2030, 2043, 2027, 5256, 2039, 1012, 2070, 2493, 2079, 2119, 2000, 5437, 2204, 1012, 2008, 5320, 2068, 2079, 3335, 1996, 3902, 1998, 3896, 2006, 2045, 10800, 2051, 3426, 2027, 2272, 2397, 2000, 2082, 1012, 2043, 1057, 2031, 3784, 4280, 1057, 2180, 2102, 2342, 2000, 3335, 8220, 3426, 2017, 2064, 2131, 2673, 2275, 2039, 1998, 2175, 2202, 1037, 6457, 1998, 2043, 1057, 2131, 2041, 2115, 3201, 2000, 2175, 1012, 2043, 2115, 2188, 2115, 6625, 1998, 2017, 3477, 3086, 1012, 2009, 3957, 2059, 2019, 5056, 2000, 2022, 25670, 1998, 2130, 3413, 2045, 19846, 2006, 2465, 2147, 1012, 2270, 2816, 2024, 3697, 2130, 2065, 2017, 3046, 1012, 2070, 3836, 2123, 2102, 2113, 2129, 2000, 6570, 2009, 1999, 2059, 2126, 2008, 2493, 3305, 2009, 1012, 2008, 5320, 2493, 2000, 8246, 1998, 2027, 2089, 9377, 1996, 2465, 1012]\n"]}],"source":["# 오리지널 문장\n","print('Original : ', full_text[0])\n","# 토큰으로 분할 된 문장\n","print('Tokenized : ',tokenizer.tokenize(full_text[0]))\n","# token ids에 매핑된 문장\n","# input_ids는 문장의 각 토큰에 해당하는 인덱스입니다.\n","print('Token IDs : ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(full_text[0])))"]},{"cell_type":"markdown","id":"840f94d2","metadata":{"papermill":{"duration":0.038541,"end_time":"2022-10-17T06:20:07.121710","exception":false,"start_time":"2022-10-17T06:20:07.083169","status":"completed"},"tags":[],"id":"840f94d2"},"source":["# Tokenize the dataset"]},{"cell_type":"markdown","id":"6d8ec053","metadata":{"papermill":{"duration":0.037803,"end_time":"2022-10-17T06:20:07.209389","exception":false,"start_time":"2022-10-17T06:20:07.171586","status":"completed"},"tags":[],"id":"6d8ec053"},"source":["### Encode_plus 가 하는 일:\n","1. 문장 토크나이즈\n","2. 시작 부분에 [CLS]토큰 추가\n","3. 끝 부분에 [SEP] 토큰 추가\n","4. 토큰에 ID 매핑\n","5. 문장을 'max_length'로 채우거나 자른다.\n","6. [PAD]토큰에 대한 attention mask 생성"]},{"cell_type":"code","execution_count":null,"id":"8a67001d","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:20:07.292725Z","iopub.status.busy":"2022-10-17T06:20:07.292167Z","iopub.status.idle":"2022-10-17T06:20:51.497921Z","shell.execute_reply":"2022-10-17T06:20:51.496754Z"},"papermill":{"duration":44.281097,"end_time":"2022-10-17T06:20:51.529539","exception":false,"start_time":"2022-10-17T06:20:07.248442","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"8a67001d","executionInfo":{"status":"ok","timestamp":1666692670962,"user_tz":-540,"elapsed":53860,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"44c465fe-c84b-48ae-9a12-ff3d9357e57a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortable at home.\n","\n","The hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and stay home and you wont need to stress about what to wear.\n","\n","most students usually take showers before school. they either take it before they sleep or when they wake up. some students do both to smell good. that causes them do miss the bus and effects on there lesson time cause they come late to school. when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go.\n","\n","when your home your comfortable and you pay attention. it gives then an advantage to be smarter and even pass there classmates on class work. public schools are difficult even if you try. some teacher dont know how to teach it in then way that students understand it. that causes students to fail and they may repeat the class.              \n","Token IDs:  tensor([  101,  1045,  2228,  2008,  2493,  2052,  5770,  2013,  4083,  2012,\n","         2188,  1010,  2138,  2027,  2180,  2102,  2031,  2000,  2689,  1998,\n","         2131,  2039,  2220,  1999,  1996,  2851,  2000,  6457,  1998,  2079,\n","         2045,  2606,  1012,  2635,  2069,  4280,  7126,  2068,  2138,  2012,\n","         2045,  2160,  2027,  1005,  2222,  2022,  3477,  2062,  3086,  1012,\n","         2027,  2097,  2022,  6625,  2012,  2188,  1012,  1996, 18263,  2112,\n","         1997,  2082,  2003,  2893,  3201,  1012,  2017,  5256,  2039,  2175,\n","         8248,  2115,  4091,  1998,  2175,  2000,  2115,  9346,  1998,  2298,\n","         2012,  2115,  8416,  2015,  1012,  2044,  2017,  2228,  2017,  3856,\n","         1037, 11018,  1057,  2175,  2298,  1999,  1996,  5259,  1998,  2017,\n","         3363,  2593,  2025,  2066,  2009,  2030,  2017,  2298,  1998,  2156,\n","         1037, 21101,  1012,  2059,  2017,  1005,  2222,  2031,  2000,  2689,\n","         1012,  2007,  1996,  3784,  4280,  2017,  2064,  4929,  2505,  1998,\n","         2994,  2188,  1998,  2017,  2180,  2102,  2342,  2000,  6911,  2055,\n","         2054,  2000,  4929,  1012,  2087,  2493,  2788,  2202, 23442,  2077,\n","         2082,  1012,  2027,  2593,  2202,  2009,  2077,  2027,  3637,  2030,\n","         2043,  2027,  5256,  2039,  1012,  2070,  2493,  2079,  2119,  2000,\n","         5437,  2204,  1012,  2008,  5320,  2068,  2079,  3335,  1996,  3902,\n","         1998,  3896,  2006,  2045, 10800,  2051,  3426,  2027,  2272,  2397,\n","         2000,  2082,  1012,  2043,  1057,  2031,  3784,  4280,  1057,  2180,\n","         2102,  2342,  2000,  3335,  8220,  3426,  2017,  2064,  2131,  2673,\n","         2275,  2039,  1998,  2175,  2202,  1037,  6457,  1998,  2043,  1057,\n","         2131,  2041,  2115,  3201,  2000,  2175,  1012,  2043,  2115,  2188,\n","         2115,  6625,  1998,  2017,  3477,  3086,  1012,  2009,  3957,  2059,\n","         2019,  5056,  2000,  2022, 25670,  1998,  2130,  3413,  2045, 19846,\n","         2006,  2465,  2147,  1012,  2270,  2816,  2024,  3697,  2130,  2065,\n","         2017,  3046,  1012,  2070,  3836,  2123,  2102,  2113,  2129,  2000,\n","         6570,  2009,  1999,  2059,  2126,  2008,  2493,  3305,  2009,  1012,\n","         2008,  5320,  2493,  2000,  8246,  1998,  2027,  2089,  9377,  1996,\n","         2465,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"]}],"source":["# 모든 문장을 토큰화하고 토큰을 해당 단어 ID에 매핑\n","input_ids = []\n","attention_masks = []\n","\n","for text in full_text:\n","    encoded_dict = tokenizer.encode_plus(\n","                    text,#문장을 encode\n","                    add_special_tokens = True, #[CLS][SEP]토큰 추가\n","                    max_length = 320, #모든 문장 자르고 채우기 \n","                    # 'max_length'의 숫자는 어떻게 정하는 거지?\n","                    truncation=True,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,#Attention mask 만들기\n","                    return_tensors = 'pt' #파이토치 텐서로 리턴\n","                    )\n","    # 인코딩된 문장을 목록에 추가\n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # Attention mask\n","    attention_masks.append(encoded_dict['attention_mask'])\n","    \n","# 목록을 텐서로 변환\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# full_text 첫번째 문장을 IDs의 리스트로 출력해보기\n","print('Original: ', full_text[0])\n","print('Token IDs: ', input_ids[0])"]},{"cell_type":"markdown","id":"84e250fc","metadata":{"papermill":{"duration":0.026923,"end_time":"2022-10-17T06:20:51.585254","exception":false,"start_time":"2022-10-17T06:20:51.558331","status":"completed"},"tags":[],"id":"84e250fc"},"source":["# Training & Validation Split\n","### training 90% 사용하고 validation에 10%를 사용하도록 training set를 나눈다"]},{"cell_type":"code","execution_count":null,"id":"a4bc1128","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:20:51.641186Z","iopub.status.busy":"2022-10-17T06:20:51.640777Z","iopub.status.idle":"2022-10-17T06:20:51.647835Z","shell.execute_reply":"2022-10-17T06:20:51.646750Z"},"papermill":{"duration":0.037665,"end_time":"2022-10-17T06:20:51.650115","exception":false,"start_time":"2022-10-17T06:20:51.612450","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"a4bc1128","executionInfo":{"status":"ok","timestamp":1666692670963,"user_tz":-540,"elapsed":10,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"f5fd95fc-6b1a-4eb0-cbd4-ab35c05e6cec"},"outputs":[{"output_type":"stream","name":"stdout","text":["3,519 training samples\n","  392 validation samples\n"]}],"source":["from torch.utils.data import TensorDataset, random_split\n","# training inputs을 TensorDataset에 결합\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","#90-10 train-validation split.\n","#각 세트에 포함할 샘플 수를 계산\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","#무작위로 샘플을 선택하여 데이터 세트를 나눈다.\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"]},{"cell_type":"markdown","id":"3375a4b3","metadata":{"papermill":{"duration":0.027357,"end_time":"2022-10-17T06:20:51.706593","exception":false,"start_time":"2022-10-17T06:20:51.679236","status":"completed"},"tags":[],"id":"3375a4b3"},"source":["torch DataLoader 클래스를 사용하여 데이터 세트에 대한 iterator를 생성한다. for 루프와 달리 iterator를 사용하면 전체 데이터 세트를 메모리에 로드할 필요가 없기 때문에 이는 훈련 중에 메모리를 절약하는 데 도움이 된다."]},{"cell_type":"code","execution_count":null,"id":"4c66c29f","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:20:51.763143Z","iopub.status.busy":"2022-10-17T06:20:51.762301Z","iopub.status.idle":"2022-10-17T06:20:51.768987Z","shell.execute_reply":"2022-10-17T06:20:51.768040Z"},"papermill":{"duration":0.036924,"end_time":"2022-10-17T06:20:51.771002","exception":false,"start_time":"2022-10-17T06:20:51.734078","status":"completed"},"tags":[],"id":"4c66c29f"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","#DataLoader는 훈련을 위한 배치크기를 알아야 하므로 지정함.\n","#Bert를 미세조정하기 위해 배치 사이즈 16/32 권장\n","batch_size = 14\n","\n","#training, validation 세트에 대한 DataLoader를 생성\n","#무작위 순서로 학습 샘플을 가져옴.\n","train_dataloader = DataLoader(\n","            train_dataset, # training sample\n","            sampler = RandomSampler(train_dataset), #배치를 랜덤으로 선택\n","            batch_size = batch_size #훈련을 이 배치 사이즈로 함.\n","        )\n","\n","#validation에서 순서가 중요하지 않아서 순서대로 진행함\n","valid_dataloader = DataLoader(\n","                val_dataset,\n","                sampler = SequentialSampler(val_dataset), # 배치를 순차적으로 꺼냄.\n","                batch_size = batch_size #이 배치 사이즈로 평가하기\n","        )"]},{"cell_type":"markdown","id":"218f6e25","metadata":{"papermill":{"duration":0.027531,"end_time":"2022-10-17T06:20:51.825874","exception":false,"start_time":"2022-10-17T06:20:51.798343","status":"completed"},"tags":[],"id":"218f6e25"},"source":["# Train Model\n","## BertForSequencClassification\n","#### pre-trained된 bert 모델을 수정하여 분류를 위한 출력을 제공\n","#### 전체 모델이 우리 task에 적합할 때까지 데이터 세트에서 모델을 계속 훈련\n","\n","#### <b>BertForSequenceClassification</b>은 분류를 위해 단일 선형 레이어가 추가된 일반 Bert 모델임\n","#### 입력 데이터를 넣을 때 pre trained된 bert 모델과 추가 훈련되지 않은 분류 계층이 특정 task에 대해 훈련됨"]},{"cell_type":"code","execution_count":null,"id":"3f5cf086","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-10-17T06:20:51.882030Z","iopub.status.busy":"2022-10-17T06:20:51.881714Z","iopub.status.idle":"2022-10-17T06:21:15.740652Z","shell.execute_reply":"2022-10-17T06:21:15.739727Z"},"jupyter":{"outputs_hidden":true},"papermill":{"duration":23.889677,"end_time":"2022-10-17T06:21:15.743000","exception":false,"start_time":"2022-10-17T06:20:51.853323","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c526452691664d61a7ba77028a2492a1","d6ba7f618de7489c8c9f3fa2223eaf68","56b070ef2e484a3da315d74d1e111308","c249421b8c824a7098b592adc91be5cf","8847d3ac0118497485b222bc46ba74c6","8c86a1fcdef84ffb8fd3ca1fb3f7a32a","dc6311baa40b4c93bde57073dde4ca8c","084cc3a715174a76b17527ef22387e98","53fb3a206cca45bc98dee2aafbb9e7e7","9bf658ef6a464580882ca3e03f56e679","892b39e2a7964dbeb71d196e90120133"]},"id":"3f5cf086","executionInfo":{"status":"ok","timestamp":1666692690839,"user_tz":-540,"elapsed":19881,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"25db3cf6-5931-4a81-d370-276354f4e623"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c526452691664d61a7ba77028a2492a1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",")"]},"metadata":{},"execution_count":15}],"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","        \"bert-base-uncased\", # 12레이어 버트 모델 사용 (uncased)\n","        num_labels = 1, #출력 레이블 수 (2진 분류의 경우 2)\n","        # grammar 점수가 9개라 9개 적음\n","        # 다중 클래스 작업의 경우 이 값을 늘릴 수 있다.\n","        output_attentions = False, #모델이 attention 가중치를 리턴하는지\n","        output_hidden_states = False, #모델이 hidden states를 리턴하는지\n","        #attention_probs_dropout_prob=0.4,\n","        #hidden_dropout_prob=0.4,\n",")\n","# GPU에서 이 모델을 실행하도록 pytorch에 지시\n","model.cuda()"]},{"cell_type":"markdown","id":"1acd3402","metadata":{"papermill":{"duration":0.027013,"end_time":"2022-10-17T06:21:15.797597","exception":false,"start_time":"2022-10-17T06:21:15.770584","status":"completed"},"tags":[],"id":"1acd3402"},"source":["아래의 셀에는 가중치의 이름과 dimesions 출력\n","1. The embedding layer\n","2. The first of the twelve transformers\n","3. The Output layer"]},{"cell_type":"code","execution_count":null,"id":"ca24bf54","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:21:15.853660Z","iopub.status.busy":"2022-10-17T06:21:15.852728Z","iopub.status.idle":"2022-10-17T06:21:15.862543Z","shell.execute_reply":"2022-10-17T06:21:15.860851Z"},"papermill":{"duration":0.040021,"end_time":"2022-10-17T06:21:15.864780","exception":false,"start_time":"2022-10-17T06:21:15.824759","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"ca24bf54","executionInfo":{"status":"ok","timestamp":1666692690840,"user_tz":-540,"elapsed":25,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"ebcef4fc-bd77-4b9e-9bc4-ecd045298c47"},"outputs":[{"output_type":"stream","name":"stdout","text":["The BERT model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                 (30522, 768)\n","bert.embeddings.position_embeddings.weight             (512, 768)  \n","bert.embeddings.token_type_embeddings.weight           (2, 768)    \n","bert.embeddings.LayerNorm.weight                       (768,)      \n","bert.embeddings.LayerNorm.bias                         (768,)      \n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight       (768, 768)  \n","bert.encoder.layer.0.attention.self.query.bias         (768,)      \n","bert.encoder.layer.0.attention.self.key.weight         (768, 768)  \n","bert.encoder.layer.0.attention.self.key.bias           (768,)      \n","bert.encoder.layer.0.attention.self.value.weight       (768, 768)  \n","bert.encoder.layer.0.attention.self.value.bias         (768,)      \n","bert.encoder.layer.0.attention.output.dense.weight     (768, 768)  \n","bert.encoder.layer.0.attention.output.dense.bias       (768,)      \n","bert.encoder.layer.0.attention.output.LayerNorm.weight (768,)      \n","bert.encoder.layer.0.attention.output.LayerNorm.bias   (768,)      \n","bert.encoder.layer.0.intermediate.dense.weight         (3072, 768) \n","bert.encoder.layer.0.intermediate.dense.bias           (3072,)     \n","bert.encoder.layer.0.output.dense.weight               (768, 3072) \n","bert.encoder.layer.0.output.dense.bias                 (768,)      \n","bert.encoder.layer.0.output.LayerNorm.weight           (768,)      \n","bert.encoder.layer.0.output.LayerNorm.bias             (768,)      \n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                               (768, 768)  \n","bert.pooler.dense.bias                                 (768,)      \n","classifier.weight                                      (1, 768)    \n","classifier.bias                                        (1,)        \n"]}],"source":["# 모델의 모든 parameters를 튜플 목록으로 가져옴.\n","params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55}{:12}\".format(p[0], str(tuple(p[1].size()))))\n","    \n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:55}{:12}\".format(p[0], str(tuple(p[1].size()))))\n","    \n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:55}{:12}\".format(p[0], str(tuple(p[1].size()))))"]},{"cell_type":"markdown","id":"4529ef12","metadata":{"papermill":{"duration":0.026893,"end_time":"2022-10-17T06:21:15.918817","exception":false,"start_time":"2022-10-17T06:21:15.891924","status":"completed"},"tags":[],"id":"4529ef12"},"source":["# Optimizer & Learning Rate Scheduler\n","### 이제 모델이 로드되었으므로 저장된 모델 내에서 훈련 하이퍼파라미터를 가져와야 함\n","\n","### fine-tuning을 위해서 다음 값 중에서 선택하는 것이 좋다\n","- Batch size : 16, 32\n","- Learnin rate(Adam) : 5e-5, 3e-5, 2e-5\n","- number of epochs: 2,3,4\n","\n","여기서는:\n","- Batch size : 32 (이미 DataLoaders에서 선택함)\n","- Learning rate : 2e-5\n","- epochs : 4 (너무 많은 것 같은데..)\n","\n","Epsilon parameter <b>eps= 1e-8</b>은 구현에서 0으로 나누는 것을 방지하기 위한 매우 작은 숫자임\n","\n","run_glue.py에서 AdamW optimizer의 생성을 찾을 수 있다."]},{"cell_type":"code","execution_count":null,"id":"ecea8390","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:21:15.974424Z","iopub.status.busy":"2022-10-17T06:21:15.974133Z","iopub.status.idle":"2022-10-17T06:21:15.983110Z","shell.execute_reply":"2022-10-17T06:21:15.981767Z"},"papermill":{"duration":0.039182,"end_time":"2022-10-17T06:21:15.985274","exception":false,"start_time":"2022-10-17T06:21:15.946092","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"ecea8390","executionInfo":{"status":"ok","timestamp":1666692690840,"user_tz":-540,"elapsed":23,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"731044b2-405f-42ab-ab95-b896584e9c62"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}],"source":["# 참고 : AdamW는 hugging face 라이브러리의 클래스임(pytorch와 반대)\n","# W = Weight Dacay Fix?\n","optimizer = AdamW(model.parameters(),\n","                 lr = 2e-5, #arg.learning_rate - 기본값은 5e-5이고 노트북에는 2e-5가 있다.\n","                 eps = 1e-8) #arg.adam_epsilon - 기본값은 1e-8"]},{"cell_type":"code","execution_count":null,"id":"1c052043","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:21:16.041277Z","iopub.status.busy":"2022-10-17T06:21:16.040513Z","iopub.status.idle":"2022-10-17T06:21:16.046298Z","shell.execute_reply":"2022-10-17T06:21:16.045460Z"},"papermill":{"duration":0.035841,"end_time":"2022-10-17T06:21:16.048241","exception":false,"start_time":"2022-10-17T06:21:16.012400","status":"completed"},"tags":[],"id":"1c052043"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. 2-4사이 권장\n","# 4를 선택하지만 과적합될수있음\n","epochs = 10\n","# 총 훈련 단계 수는 [배치 수] X [에포크 수]\n","# (훈련 샘플의 수와 동일하지 않음)\n","total_steps = len(train_dataloader) * epochs\n","\n","# learning rate scheduler 만들기\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                           num_warmup_steps = 0,\n","                                           num_training_steps = total_steps)"]},{"cell_type":"markdown","id":"151d9941","metadata":{"papermill":{"duration":0.029116,"end_time":"2022-10-17T06:21:16.104600","exception":false,"start_time":"2022-10-17T06:21:16.075484","status":"completed"},"tags":[],"id":"151d9941"},"source":["# Training Model\n","### 정확도 계산을 위한 helper function 정의"]},{"cell_type":"code","execution_count":null,"id":"7364e97d","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:21:16.161250Z","iopub.status.busy":"2022-10-17T06:21:16.160297Z","iopub.status.idle":"2022-10-17T06:21:16.166441Z","shell.execute_reply":"2022-10-17T06:21:16.165540Z"},"papermill":{"duration":0.036238,"end_time":"2022-10-17T06:21:16.168404","exception":false,"start_time":"2022-10-17T06:21:16.132166","status":"completed"},"tags":[],"id":"7364e97d"},"outputs":[],"source":["import numpy as np\n","\n","# 예측 vs 레이블의 정확도를 계산하는 함수\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"markdown","id":"ba5da9a6","metadata":{"papermill":{"duration":0.028138,"end_time":"2022-10-17T06:21:16.223668","exception":false,"start_time":"2022-10-17T06:21:16.195530","status":"completed"},"tags":[],"id":"ba5da9a6"},"source":["#### 경과 시간을 hh:mm:ss 형식으로 지정하는 helper function"]},{"cell_type":"code","execution_count":null,"id":"ba740c6f","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:21:16.279879Z","iopub.status.busy":"2022-10-17T06:21:16.279044Z","iopub.status.idle":"2022-10-17T06:21:16.284638Z","shell.execute_reply":"2022-10-17T06:21:16.283803Z"},"papermill":{"duration":0.035695,"end_time":"2022-10-17T06:21:16.286573","exception":false,"start_time":"2022-10-17T06:21:16.250878","status":"completed"},"tags":[],"id":"ba740c6f"},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    #가까운 초로 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    #format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"markdown","id":"d13233ca","metadata":{"papermill":{"duration":0.027083,"end_time":"2022-10-17T06:21:16.340691","exception":false,"start_time":"2022-10-17T06:21:16.313608","status":"completed"},"tags":[],"id":"d13233ca"},"source":["#### ㄷㄱㄷㄱ 훈련 시작 준비 완료"]},{"cell_type":"code","execution_count":null,"id":"65edaac8","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:21:16.396881Z","iopub.status.busy":"2022-10-17T06:21:16.396509Z","iopub.status.idle":"2022-10-17T06:39:58.146535Z","shell.execute_reply":"2022-10-17T06:39:58.144865Z"},"papermill":{"duration":1121.781241,"end_time":"2022-10-17T06:39:58.149250","exception":false,"start_time":"2022-10-17T06:21:16.368009","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"65edaac8","executionInfo":{"status":"ok","timestamp":1666694918120,"user_tz":-540,"elapsed":2227301,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"94e5be49-c908-4bec-a263-11092eb1d2b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=======Epoch 1 / 10 =======\n","Training...\n"," Batch    40 of   252. Elapsed: 0:00:34\n"," Batch    80 of   252. Elapsed: 0:01:06\n"," Batch   120 of   252. Elapsed: 0:01:40\n"," Batch   160 of   252. Elapsed: 0:02:14\n"," Batch   200 of   252. Elapsed: 0:02:48\n"," Batch   240 of   252. Elapsed: 0:03:22\n","\n","  Average training loss: 0.71\n","  Training epoch took: 0:03:32\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 0.25\n","  Validation took: 0:00:09\n","\n","=======Epoch 2 / 10 =======\n","Training...\n"," Batch    40 of   252. Elapsed: 0:00:34\n"," Batch    80 of   252. Elapsed: 0:01:08\n"," Batch   120 of   252. Elapsed: 0:01:42\n"," Batch   160 of   252. Elapsed: 0:02:17\n"," Batch   200 of   252. Elapsed: 0:02:51\n"," Batch   240 of   252. Elapsed: 0:03:25\n","\n","  Average training loss: 0.27\n","  Training epoch took: 0:03:35\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 0.43\n","  Validation took: 0:00:09\n","\n","=======Epoch 3 / 10 =======\n","Training...\n"," Batch    40 of   252. Elapsed: 0:00:34\n"," Batch    80 of   252. Elapsed: 0:01:08\n"," Batch   120 of   252. Elapsed: 0:01:42\n"," Batch   160 of   252. Elapsed: 0:02:16\n"," Batch   200 of   252. Elapsed: 0:02:51\n"," Batch   240 of   252. Elapsed: 0:03:25\n","\n","  Average training loss: 0.21\n","  Training epoch took: 0:03:34\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 0.48\n","  Validation took: 0:00:09\n","\n","=======Epoch 4 / 10 =======\n","Training...\n"," Batch    40 of   252. Elapsed: 0:00:34\n"," Batch    80 of   252. Elapsed: 0:01:08\n"," Batch   120 of   252. Elapsed: 0:01:42\n"," Batch   160 of   252. Elapsed: 0:02:16\n"," Batch   200 of   252. Elapsed: 0:02:50\n"," Batch   240 of   252. Elapsed: 0:03:25\n","\n","  Average training loss: 0.15\n","  Training epoch took: 0:03:34\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 0.26\n","  Validation took: 0:00:09\n","\n","=======Epoch 5 / 10 =======\n","Training...\n"," Batch    40 of   252. Elapsed: 0:00:34\n"," Batch    80 of   252. Elapsed: 0:01:08\n"," Batch   120 of   252. Elapsed: 0:01:42\n"," Batch   160 of   252. Elapsed: 0:02:16\n"," Batch   200 of   252. Elapsed: 0:02:50\n"," Batch   240 of   252. Elapsed: 0:03:25\n","\n","  Average training loss: 0.09\n","  Training epoch took: 0:03:34\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 0.39\n","  Validation took: 0:00:09\n","\n","=======Epoch 6 / 10 =======\n","Training...\n"," Batch    40 of   252. Elapsed: 0:00:34\n"," Batch    80 of   252. Elapsed: 0:01:08\n"," Batch   120 of   252. Elapsed: 0:01:42\n"," Batch   160 of   252. Elapsed: 0:02:16\n"," Batch   200 of   252. Elapsed: 0:02:50\n"," Batch   240 of   252. Elapsed: 0:03:24\n","\n","  Average training loss: 0.07\n","  Training epoch took: 0:03:34\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 0.37\n","  Validation took: 0:00:09\n","\n","=======Epoch 7 / 10 =======\n","Training...\n"," Batch    40 of   252. Elapsed: 0:00:34\n"," Batch    80 of   252. Elapsed: 0:01:08\n"," Batch   120 of   252. Elapsed: 0:01:43\n"," Batch   160 of   252. Elapsed: 0:02:17\n"," Batch   200 of   252. Elapsed: 0:02:51\n"," Batch   240 of   252. Elapsed: 0:03:25\n","\n","  Average training loss: 0.05\n","  Training epoch took: 0:03:35\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 0.29\n","  Validation took: 0:00:09\n","\n","=======Epoch 8 / 10 =======\n","Training...\n"," Batch    40 of   252. Elapsed: 0:00:34\n"," Batch    80 of   252. Elapsed: 0:01:08\n"," Batch   120 of   252. Elapsed: 0:01:42\n"," Batch   160 of   252. Elapsed: 0:02:17\n"," Batch   200 of   252. Elapsed: 0:02:51\n"," Batch   240 of   252. Elapsed: 0:03:25\n","\n","  Average training loss: 0.04\n","  Training epoch took: 0:03:35\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 0.30\n","  Validation took: 0:00:09\n","\n","=======Epoch 9 / 10 =======\n","Training...\n"," Batch    40 of   252. Elapsed: 0:00:34\n"," Batch    80 of   252. Elapsed: 0:01:08\n"," Batch   120 of   252. Elapsed: 0:01:42\n"," Batch   160 of   252. Elapsed: 0:02:17\n"," Batch   200 of   252. Elapsed: 0:02:51\n"," Batch   240 of   252. Elapsed: 0:03:25\n","\n","  Average training loss: 0.03\n","  Training epoch took: 0:03:34\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 0.30\n","  Validation took: 0:00:09\n","\n","=======Epoch 10 / 10 =======\n","Training...\n"," Batch    40 of   252. Elapsed: 0:00:34\n"," Batch    80 of   252. Elapsed: 0:01:08\n"," Batch   120 of   252. Elapsed: 0:01:42\n"," Batch   160 of   252. Elapsed: 0:02:17\n"," Batch   200 of   252. Elapsed: 0:02:51\n"," Batch   240 of   252. Elapsed: 0:03:25\n","\n","  Average training loss: 0.03\n","  Training epoch took: 0:03:35\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 0.33\n","  Validation took: 0:00:09\n","\n","Training complete!\n","Total training took 0:37:07 (h:mm:ss)\n"]}],"source":["import random\n","import numpy as np\n","\n","#모든 곳에 seed 값을 설정하기\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# training , validation loss 같은 많은 수량을 저장\n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# 전체 실행에 대한 총 훈련시간을 측정함\n","total_t0 = time.time()\n","\n","#for each epoch...\n","for epoch_i in range(0, epochs):\n","    print(\"\")\n","    print('=======Epoch {:} / {:} ======='.format(epoch_i+1, epochs))\n","    print(\"Training...\")\n","    \n","    # training epoch가 걸리는 시간을 측정\n","    t0 =time.time()\n","    \n","    # epoch의 전체 loss를 재설정\n","    total_train_loss = 0\n","    \n","    # 모델을 학습 모드로 전환 \n","    # train은 *모드*를 변경할 뿐 훈련을 *수행*하지 않음.\n","    # 'dropoout' / 'batchnorm' 레이어는 훈련 중에 다르게 작동함\n","    model.train()\n","    \n","    # 훈련 데이터의 각 batch에 대해\n","    for step, batch in enumerate(train_dataloader):\n","        #40개 batch마다 진행률이 업데이트됨\n","        if step % 40 == 0 and not step == 0:\n","            #경과시간을 분 단위로 계산함\n","            elapsed = format_time(time.time()-t0)\n","            #진행상황 보고\n","            print(' Batch {:>5,} of {:>5,}. Elapsed: {:}'.format(step, len(train_dataloader), elapsed))\n","    \n","        #DataLoader에서 훈련 batch의 압축을 푼다\n","        #Batch의 압축을 풀면서 각 텐서를 사용하여 GPU에 복사함\n","        # 'to' method\n","\n","        #'batch'에는 세 개의 pytorch 텐서가 포함되어 있음.\n","        # input ids\n","        # attention masks\n","        # labels\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device, dtype=torch.float32)\n","\n","        # 모델 훈련을 수행하기 이전에 계산된 gradient는 지워야 함\n","        # backward pass : pytorch는 이 작업을 자동으로 수행하지 않는다.\n","        # gradient를 누적하는 것은 RNN을 훈련하는 동안 편리함\n","        model.zero_grad()\n","    \n","        # forward pass 수행 (정방향 전달)\n","        # 어떤 arguments에 따라 다른 수의 parameter를 return\n","        # arguments가 주어지고 flags가 설정됨\n","        # loss (label를 주었기 때문에) logits model\n","        # activation 전 출력\n","        loss, logits = model(b_input_ids,\n","                            token_type_ids=None,\n","                            attention_mask=b_input_mask,\n","                            labels = b_labels,\n","                            return_dict=False)\n","    \n","        #모든 Batch에 대한 훈련 loss를 누적하여 다음을 수행할 수 있다.\n","        # 마지막에 평균 loss를 계산함 loss는 다음을 포함하는 텐서이다.\n","        # single value; .item() function은 python 값을 return\n","        #print(loss.item(), type(loss.item()))\n","        total_train_loss += loss.item()\n","\n","        # backward pass로 gradients를 계산함\n","        loss.backward()\n","    \n","        # gradient 표준을 1.0으로 자름\n","        # \"exploding gradients\"문제에도 도움이 됨\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters and take a step using the computed gradient\n","        # Optimizer는 \"업데이트 rule\"(parameter가 어떻게 학습률과 기울기에 따라 수정되는지)지시\n","        optimizer.step()\n","        \n","        #update the learnin rate.\n","        scheduler.step()\n","    \n","    # 모든 batch에 대한 평균 손실을 계산함\n","    avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","    # 이 에포크가 얼마나 걸렸는지 측정\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","    \n","    # ======validation======\n","    #각 training epoch를 완료 후 성능을 측정함\n","    # VALIDATION SET\n","    \n","    print(\"\")\n","    print(\"Running Validation...\")\n","    \n","    t0 = time.time()\n","    \n","    # 모델을 평가 모드에. dropout layer가 다르게 작동함\n","    model.eval()\n","    \n","    #Tracking variables\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    \n","    #Evaluate data for one epoch\n","    for batch in valid_dataloader:\n","        #DataLoader에서 training batch 압축 해제\n","        #batch의 압축을 풀면서 각 텐서를 GPU에 복사\n","        #\"to\"method.\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device, dtype=torch.float32)\n","        \n","        #pytorch에게 compute 그래프를 구성하는동안 귀찮게 하지 말라고 하기\n","        #forward pass ; backprop (역전파) training에만 필요\n","        with torch.no_grad():\n","            \n","            #forward pass : logit predictions를 계산\n","            #token_type_ids는 \"segment ids\"와 동일\n","            # 2문장 작업에서 문장 1과 2를 구별\n","            \n","            #모델의 logits의 출력을 가져옴. \n","            #softmax 함수를 적용하기 전 값\n","            (loss, logits) = model(b_input_ids,\n","                                  token_type_ids=None,\n","                                  attention_mask=b_input_mask,\n","                                  labels=b_labels,\n","                                  return_dict=False)\n","        #validation loss를 누적\n","        total_eval_loss += loss.item()\n","        \n","        #logits, labels 는 CPU로 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # test 문장 batch 정확도를 계산하고 모든 batch에 누적함\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","    # validation 실행에 대한 최종 정확도를 보고함.\n","    avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","    \n","    #모든 배치에 대한 평균 loss를 계산\n","    avg_val_loss = total_eval_loss / len(valid_dataloader)\n","    \n","    #validation실행에 걸린 시간을 측정\n","    validation_time = format_time(time.time()-t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    \n","    # epoch의 모든 통계를 기록\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i+1,\n","            'Training Loss' : avg_train_loss,\n","            'Valid. Loss' : avg_val_loss,\n","            'Valid. Accur.' : avg_val_accuracy,\n","            'Training Time' : training_time,\n","            'Validation Time' : validation_time\n","        }\n","    )\n","    \n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"code","execution_count":null,"id":"a6a6a4d2","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:39:58.238241Z","iopub.status.busy":"2022-10-17T06:39:58.237535Z","iopub.status.idle":"2022-10-17T06:39:58.261316Z","shell.execute_reply":"2022-10-17T06:39:58.260408Z"},"papermill":{"duration":0.081981,"end_time":"2022-10-17T06:39:58.263589","exception":false,"start_time":"2022-10-17T06:39:58.181608","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":394},"id":"a6a6a4d2","executionInfo":{"status":"ok","timestamp":1666694918120,"user_tz":-540,"elapsed":13,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"7cef0872-7455-48fa-bec5-ad0bc0c5e6f3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1           0.708669     0.253903            0.0       0:03:32         0:00:09\n","2           0.265718     0.434804            0.0       0:03:35         0:00:09\n","3           0.210068     0.475172            0.0       0:03:34         0:00:09\n","4           0.147618     0.264418            0.0       0:03:34         0:00:09\n","5           0.094637     0.393285            0.0       0:03:34         0:00:09\n","6           0.069689     0.371532            0.0       0:03:34         0:00:09\n","7           0.049742     0.293220            0.0       0:03:35         0:00:09\n","8           0.038470     0.303034            0.0       0:03:35         0:00:09\n","9           0.032195     0.304928            0.0       0:03:34         0:00:09\n","10          0.025861     0.325605            0.0       0:03:35         0:00:09"],"text/html":["\n","  <div id=\"df-0cbc618b-7366-4558-b944-945edfd62a53\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.708669</td>\n","      <td>0.253903</td>\n","      <td>0.0</td>\n","      <td>0:03:32</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.265718</td>\n","      <td>0.434804</td>\n","      <td>0.0</td>\n","      <td>0:03:35</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.210068</td>\n","      <td>0.475172</td>\n","      <td>0.0</td>\n","      <td>0:03:34</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.147618</td>\n","      <td>0.264418</td>\n","      <td>0.0</td>\n","      <td>0:03:34</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.094637</td>\n","      <td>0.393285</td>\n","      <td>0.0</td>\n","      <td>0:03:34</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.069689</td>\n","      <td>0.371532</td>\n","      <td>0.0</td>\n","      <td>0:03:34</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.049742</td>\n","      <td>0.293220</td>\n","      <td>0.0</td>\n","      <td>0:03:35</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.038470</td>\n","      <td>0.303034</td>\n","      <td>0.0</td>\n","      <td>0:03:35</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.032195</td>\n","      <td>0.304928</td>\n","      <td>0.0</td>\n","      <td>0:03:34</td>\n","      <td>0:00:09</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.025861</td>\n","      <td>0.325605</td>\n","      <td>0.0</td>\n","      <td>0:03:35</td>\n","      <td>0:00:09</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cbc618b-7366-4558-b944-945edfd62a53')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0cbc618b-7366-4558-b944-945edfd62a53 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0cbc618b-7366-4558-b944-945edfd62a53');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":22}],"source":["import pandas as pd\n","\n","#소수점 이하 두 자리로 부동 소수점을 표시\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","#row index로 'epoch'사용\n","df_stats = df_stats.set_index('epoch')\n","\n","#column 헤더를 강제로 wrap\n","\n","#display the table.\n","df_stats"]},{"cell_type":"markdown","id":"5ca48a8c","metadata":{"papermill":{"duration":0.031903,"end_time":"2022-10-17T06:39:58.328932","exception":false,"start_time":"2022-10-17T06:39:58.297029","status":"completed"},"tags":[],"id":"5ca48a8c"},"source":["각 epoch 따라 training loss가 감소하고 valid loss 감소"]},{"cell_type":"code","execution_count":null,"id":"900d340a","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:39:58.395233Z","iopub.status.busy":"2022-10-17T06:39:58.394212Z","iopub.status.idle":"2022-10-17T06:39:59.189582Z","shell.execute_reply":"2022-10-17T06:39:59.188673Z"},"papermill":{"duration":0.83146,"end_time":"2022-10-17T06:39:59.191932","exception":false,"start_time":"2022-10-17T06:39:58.360472","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":410},"id":"900d340a","executionInfo":{"status":"ok","timestamp":1666694918892,"user_tz":-540,"elapsed":775,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"e8dfb17e-dc8c-4fd1-9423-ecdb011ffb78"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x432 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAtcAAAGJCAYAAABB4h9HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeVhU9f4H8Pfs7DAMwwCKICCbuyhQ5l6puadluWSWZtvt2nbz1r2pLddLdX+3buW1zC3NNq0w3K6WWprggikCKiCIC/u+z/r7A0VJVh3mDMP79Tw9D8x8zzmf+Trme858zveITCaTCUREREREdNvEQhdARERERGQrGK6JiIiIiMyE4ZqIiIiIyEwYromIiIiIzIThmoiIiIjITBiuiYiIiIjMhOGaiKgJCxYswPfff2/2sdbs0qVLCAkJgV6vB9Dy6/rj2PZatWoVXnvttVuulYjIWom4zjUR2YqBAwc2/FxTUwO5XA6JRAIAWL58OSZPnixUabestLQUS5YswdGjR2Fvb4958+Zh4cKFzY4fN24cFixYgBkzZjR6fMOGDYiNjcV3333X7LaXLl3CmDFjkJycDKlU2mJd7RmbkJCAl19+Gb/88kuL48zhu+++w7fffosvv/yyw49FRNSUlv+PSETUiZw4caLh59GjR+Ott97CnXfeedM4vV7faiC0FmvWrEFdXR0OHjwIrVaL9PT0FsdPmzYNsbGxN4Xr2NhYTJs2rSNLJSIisC2EiLqAhIQEDB8+HJ9++imGDh2Kv/71rygrK8OiRYsQHR2NIUOGYNGiRcjNzW3YZu7cufj2228B1J8NffjhhxETE4MhQ4Zg9OjROHDgwC2NvXjxImbPno2BAwfi0UcfxfLly/HSSy81W7tUKoW7uzvs7e3h6uqKiIiIFl/rlClTcPz4cVy+fLnhsfT0dJw7dw4TJkzA/v37MXXqVAwaNAgjRozAhx9+2Oy+bnxdBoMBMTExiIqKwpgxYxq9JgDYunUrxo8fj4EDB2LMmDH46quvAADV1dVYuHAh8vPzMXDgQAwcOBB5eXn48MMPG73un376CRMmTMDgwYMxd+5cZGRkNDw3evRorFmzBpMmTUJERAQWL16Murq6FuehKYmJiZg+fToiIiIwffp0JCYmNjz33XffYcyYMRg4cCBGjx6Nbdu2AQAuXLiAOXPmICIiAlFRUVi8eHG7j0tEXQvDNRF1CYWFhSgrK8O+ffvw5ptvwmg04v7778e+ffuwb98+KBQKvPHGG81uf+rUKfTs2RPx8fFYsGABXnvtNTTXVdfS2Jdeegn9+vVDQkICnn32WcTGxrZYd9++fbF9+/aGkNsaLy8vREVFNdpvbGwshg8f3hDSY2JicOzYMXzyySf48ssvsXfv3lb3+80332Dfvn344YcfsHXrVuzatavR8yqVCp988gkSExOxYsUKrFixAsnJyXBwcMDq1avh6emJEydO4MSJE9BoNI22zczMxIsvvohXX30Vhw8fxvDhw/Hkk09Cq9U2jNm5cyc+++wz/PTTTzh79myL7S1NKS0txaJFizB37lwkJCRg/vz5WLRoEUpKSlBdXY233noLq1evxokTJ/DVV18hLCwMAPDBBx9g6NChOHr0KH755RfMmTOnXccloq6H4ZqIugSxWIznnnsOcrkcdnZ2UCqVGDt2LOzt7eHk5ISnnnoKR48ebXZ7Hx8fPPjgg5BIJJg2bRoKCgpQWFjYrrFXrlxBUlJSQx2DBw/G6NGjmz3mhQsX8Prrr2Pjxo1YvXo1tmzZAgDQarXo06cPKioqmtxu6tSpDeHaaDTixx9/bGgJiYqKQkhICMRiMUJDQzFhwgQcOXKk1fnbuXMn5s2bB29vb7i5uWHRokWNnh85ciR69OgBkUiEyMhIDB06FMeOHWt1vwCwY8cOjBgxAkOHDoVMJsPjjz+O2traRm0+c+fOhUajgZubG0aNGoXU1NQ27fua/fv3w8/PD1OnToVUKsXEiRMREBCAffv2Aah/f6SlpaG2thaenp7o1asXgPpvDq5cuYL8/HwoFAoMHjy4Xccloq6nczQdEhHdJqVSCYVC0fB7TU0NVqxYgV9//RVlZWUAgKqqKhgMhoaLIG/k4eHR8LO9vT2A+paHpjQ3tqSkBK6urg2PAYC3tzdycnKa3M+WLVswevRoDBkyBGvWrMHs2bMBAD169EBISAicnZ2b3O7ee+/F8uXL8fvvv6OmpgY1NTUYMWIEAODkyZN47733kJaWBp1OB61Wi3HjxjW5nxvl5+fD29u74XcfH59Gzx84cAAff/wxsrKyYDQaUVtbi+Dg4Fb3e23fN+5PLBbD29sbeXl5DY+p1eqGn+3t7ZGfn9+mfTd3jGuvIS8vDw4ODvj3v/+NtWvX4rXXXsOgQYPwyiuvIDAwEC+//DI++OADzJgxA66urpg/f/5N/exERDdiuCaiLkEkEjX6fe3atcjMzMQ333wDtVqN1NRUTJ06tdlWD3NQq9UoKytDTU1NQ8BuLlgD9Rde6nQ6AICvry8+++wzPPLII3BxccELL7zQ7Hb29vYYO3YsfvjhB9TV1WHChAmQy+UAgBdffBFz5szBZ599BoVCgbfffhslJSVtqv3GWm/8WavV4rnnnkNMTAzGjBkDmUyGp59+umEu/zj3f+Tp6Ylz5841/G4ymZCTk3NT+8jt8PT0xJUrVxo9lpOTg2HDhgEAhg0bhmHDhqG2thbvv/8+/v73v2Pz5s1Qq9V46623AADHjh3D/PnzMWTIEPj5+ZmtNiKyLWwLIaIuqaqqCgqFAi4uLigtLcVHH33U4cfs1q0b+vTpgw8//BBarRYnTpxoaEtoyr333oudO3di7969MBgMcHJyQmhoKLKzsxud/W7KtGnTsHPnTuzevRtTp05teLyqqgqurq5QKBQ4deoU4uLi2lT7+PHjsXHjRuTm5qKsrAyffvppw3NarRZarRbu7u6QSqU4cOAADh061PC8SqVCaWlps20s48ePx4EDB3D48GHodDqsXbsWcrm80dKK7WEymVBXV9fovxEjRiArKws//vgj9Ho9duzYgfT0dIwcORKFhYXYu3cvqqurIZfL4eDgALG4/p/HnTt3Nlzo6urqCpFI1PAcEVFTeOaaiLqkefPm4aWXXkJ0dDQ8PT0xf/78Nl3Yd7vee+89LFmyBFFRUejXrx/uu+8+GAyGJscOHDgQ7733Hj766CO8/PLLUCqVmDFjBmbNmoUXXngBGzduRHh4eJPbDhkyBE5OTlAoFOjXr1/D40uXLkVMTAzeeOMNREZGYvz48SgvL2+17gcffBBZWVmYMmUKHB0d8fjjjyM+Ph4A4OTkhL/97W9YvHgxtFotRo0a1aiXPDAwEBMmTMDdd98Ng8GA7du3N9p3QEAA3n33Xbz55pvIy8tDWFgYVq1a1XC2vb1OnDjR6DUDQHJyMlatWoV//OMfWLZsGfz8/LBq1Sq4u7sjPz8f69evxyuvvAKRSISwsDAsW7YMAJCUlIR//OMfqKyshEqlwmuvvQZfX99bqouIugbeRIaISECLFy9GQEAAnnvuOaFLISIiM+B3W0REFnTq1ClkZ2fDaDTil19+wU8//YS7775b6LKIiMhM2BZCRGRBhYWF+NOf/oTS0lJ4eXlh2bJlzbZ2EBFR58O2ECIiIiIiM2FbCBERERGRmTBcExERERGZCcM1EREREZGZ2NwFjSUlVTAaLd9GrlI5oaio0uLHtUaci8Y4H9dxLoiIyBaIxSIolY5NPmdz4dpoNAkSrq8dm+pxLhrjfFzHuSAiIlvGthAiIiIiIjNhuCYiIiIiMhOGayIiIiIiM7G5nmsiIiIiW2Qw6FFSUgC9Xit0KV2GVCqHUqmGRNL2yMxwTURERNQJlJQUwM7OAY6OXhCJREKXY/NMJhOqqspRUlIADw/vNm/HthAiIiKiTkCv18LR0YXB2kJEIhEcHV3a/U0BwzURERFRJ8FgbVm3Mt9sCyEiIiKidlm4cB50Oh30eh0uXsxGz56BAIDg4BC8+urSVrf/4YctqKurw8yZs1scd/DgAZw8+TueeebPZqnbEkQmk8mm7uhQVFQpyE0q1GpnFBRUWPy41ohz0Rjn4zrOBRHRrcvNvQAvL792bXM4ORffHchAUXkdVC4K3D8iEHf09jJbTTk5V7BgwVxs3/5To8f1ej2kUts4h9vUvIvFIqhUTk2Ot41XLaBrb9ri8jq4d8CbloiIiOhWHE7OxYadZ6DVGwEAReV12LDzDAB0SFaZMWMSxoy5F4mJRxEQEIQnnngay5a9hqqqKmi1Wtx551A8/XT9Geg1az5BTU0Nnn12MXbs+BF79uyCs7MLzp/PgLOzE9566x2oVB7YseNH/Pbbr3jrrXeQmHgM//nP/yE8vDeSk5MAiLB8+T/g798TAPDJJx/j55/3wMXFFQMHRuD48aNYs2aj2V9naxiub4Ol37REREREAHAoKQcHT+W0OCbjShn0hsbf5mv1RqzbkYpffr/S7HZ39fPG0L5tXx3jRlVVVVi9+nMAQF1dHWJi/g0HBwfo9Xq88MKziI//DdHRd960XWpqCjZs+BIajRdiYt7Cli1fY9GiZ24al5mZgVdffR1/+ctr2LBhDTZsWIOlS9/CwYO/4LffDmL9+i+hUCjwt7+9ckv1mwMvaLwN3x3IaAjW12j1Rnx3IEOgioiIiIjq/TFYt/a4OYwbN6HhZ6PRiJUrP8C8eQ/j8cfn4Pz5DKSlnWtyu379+kOjqT8x2bt3H1y5cqnJcT16+CE4OPTquL64fLl+3IkTxzB69N2wt7eHWCzG+PETmtzeEix25jozMxNLlixBaWkp3NzcEBMTA39//0Zj/vKXv+Ds2bMNv589exYff/wxxowZY6ky26WovK5djxMRERGZw9C+rZ9dfnnloSYzicpFgVdmD+qQuhwc7Bt+/vrrL1BRUY5PP10PhUKBmJi3odU2nZHkcnnDz2KxBAaDoZlxihvGiZsdJySLnbleunQpZs2ahd27d2PWrFl4/fXXbxrzzjvvIDY2FrGxsYiJiYGrqyuGDRtmqRLbTeWiaNfjRERERJZy/4hAyKWNo55cKsb9IwItcvyKigqoVB5QKBQoKMjHwYMHOuxYAwdGYP/+n1BbWwuj0Yjdu3d02LFaY5FwXVRUhJSUFEycOBEAMHHiRKSkpKC4uLjZbbZs2YJJkyY1+iRjbYR+0xIRERE1547eXpg3PrThpJ/KRYF540Mtdl3YAw88hKSkk5g790GsWPEmIiKGdNix7rprBCIj78C8eQ9h0aJH4eGhhpNT06t5dDSLLMV3+vRpvPLKK9i+fXvDY/fddx/effdd9O7d+6bxWq0Ww4YNw/r16xEWFtauY1l6Kb7DybnYsj8DJRV1sJNLMHdsSJe/mJHLrTXG+biOc0FEdOtuZSm+rqS6ugoODo4wGo345z/fhIeHGk888fRt79cmluLbu3cvfHx82h2sATT7QjvK5JHOmDyyF95am4C0i6WYMDwIEjHvnqRWOwtdglXhfFzHuSAiujX5+WJIpVyLojlvv70MOTlXUFdXh5CQMMyb96hZ5kssFrfr3y6LhGtvb2/k5eXBYDBAIqlvUs/Pz4e3d9ON+Fu3bsX06dNv6VhC3URmxMDuSEjOxW+JFxHqp7T48a0Jz042xvm4jnNBRHTrjEYj9H9YpYyue/vtd296zBzzZTQab/q3q6Uz1xb5+KNSqRAWFoa4uDgAQFxcHMLCwuDu7n7T2NzcXBw/fhyTJk2yRGlmM6S3BgqZBPEpeUKXQkREREQCsdh3C8uWLcOmTZswduxYbNq0CcuXLwcALFy4EElJSQ3jvv/+e4waNQqurq6WKs0s7ORSDAr2wPGz+dDxUyURERFRl2SxnuvAwEB8++23Nz2+evXqRr8/9dRTlirJ7KLCvXA4OQ+nM4swsJda6HKIiIiIyMLYFW9G4f5KONnLkMDWECIiIqIuieHajKQSMYaEeeL3tELU1OmFLoeIiIiILIzh2syiwzXQ6o34Pa1Q6FKIiIiIOsSLLz6HH37Y0ugxk8mEBx6YghMnjje5zdtvL8PWrV8DAH74YQu+/vqLJsft2PEj/va3v7Rawy+/7EdKyumG38+cScHy5X9r60voMFa5znVnFtjNFSoXO8Sn5OGOPl37ZjJEREQkrCO5idiWsQsldaVQKtwwOXAcIr0G3fZ+J0yYjK++2oSpU2c0PHbixHGIxSIMGND6/m/c7lb9+ut+hIaGITy8DwAgNDQcS5e+ddv7vV0M12YmFokQFa7BroRslFdr4eJgvbdvJyIiItt1JDcRm89shc6oAwCU1JVi85mtAHDbAXvYsBH4179WICsrE/7+PQEA27dvw9ix9+GZZxaitrYGWq0WkydPw4MPzrpp+zVrPkFNTQ2efXYxdDod/v3vd5CYeAyurm7o1SukYVxGRjr+9a9/3rS/hITDOHjwFxw7dgQ//hiLmTNnQaPxwscff4A1azYCAHbujMOXX26ESCSCj093/OUvr0KpdMeOHT9iz55dcHZ2wfnzGXB2dsJbb70DlcrjtubkGobrDhAdrsGO+As4diYfowd1F7ocIiIisjEJOcdxOOdoi2Myy7KhNzW+Bkxn1OGL1C347cqRZre7w3sIorwjWty3TCbDPfeMx44d2/D0039GdXUVfv31ADZu/Bpz5jwKuVyO6upqPPHEPERG3tEQwJsSG7sVOTlXsGnTt9Dr9XjmmYUNNxr09vbG+++vvGl/UVF34K67hiM0NAzTp88EACQmHmvY5/nz6Vi16iOsWbMJHh4eWL36v/j3v9/FG2+sAACkpqZgw4YvodF4ISbmLWzZ8jUWLXqmxdfcVuy57gDdPZ3QTe3IG8oQERGRYP4YrFt7vL0mTJiM3bt3wGAw4Kef9qBv3/6QyWT45z/fxCOPzMRTTz2OwsICpKefa3E/iYnHMX78REilUtjZ2WHs2PENz9XW1rZ7f/X7PIY77hgKD4/6s9FTptyPY8euf6Do168/NJr69t3evfvgypVLtzIFTeKZ6w4SHa7B1gPnUVhaAw83e6HLISIiIhsS5R3R6tnlvx36B0rqSm96XKlww+JBT952Db16BUOlUiM+/jfs2LENDzwwC5988jHc3VVYu/YLSKVSPP/8M9Bqtbd8DHPv7xq5/HrbrlgsgcFguO19NuzPbHuiRiLDNACAhFSevSYiIiLLmxw4DjKxrNFjMrEMkwPHme0YEyZMxtq1n+LixWwMGzYClZUV8PTUQCqV4vz5dJw8+Xur+4iIGIxdu3ZAr9ejrq4We/bsaniupf05OjqisrKyyX0OGjQYhw8fQlFR/eptP/74A4YMibzNV9s2PHPdQdRu9gjq5or4lDxMuMNf6HKIiIioi7l20WJHrBZyzT33jMPHH3+AyZOnQSaTYd68x/Hmm69j+/ZY+Pr2wIABA1vdx+TJ9yM9PR1z5jwAV1c3hIb2RklJEQC0uL+xY+/D228vx759PzVc0HhNQEAQnnzyWTz//DNXL2jshpdfftVsr7slIpPJZLLIkSykqKgSRqPlX5Ja7YyCgopGj/10/BK+2HMObzwWie6eThavSShNzUVXxvm4jnNBRHTrcnMvwMvLT+gyupym5l0sFkGlajrbsS2kAw0J84RYJOKFjURERERdBMN1B3JxkKN3T3ckpOTBaFtfEBARERFRExiuO1h0uAZF5bXIuFwmdClERERE1MEYrjvYgF4ekEvFbA0hIiKi22Zjl8pZvVuZb4brDmavkGJALw8cTc2H3mAUuhwiIiLqpKRSOaqqyhmwLcRkMqGqqhxSqbz1wTfgUnwWEBWuwZHUfKRklaBfoErocoiIiKgTUirVKCkpQGXlzTeGoY4hlcqhVKrbt00H1UI36BuggqOdFAkpuQzXREREdEskEik8PLyFLoNawbYQC5BKxIgI8UTiuULU6cx3e00iIiIisi4M1xYSHa5Bnc6Ak+mFQpdCRERERB2E4dpCgn3doHRWID6Zq4YQERER2SqGawsRi0WIDPNE0vkiVNbohC6HiIiIiDoAw7UFRYd7wWA04fjZfKFLISIiIqIOwHBtQT00TvByd0ACbyhDREREZJMYri1IJBIhOlyDs9mlKC6vFbocIiIiIjIzhmsLiwrXwATgSCpbQ4iIiIhsDcO1hWncHdDT25mtIUREREQ2iOFaAFHhXriQV4GcoiqhSyEiIiIiM2K4FkBkmCdEAM9eExEREdkYhmsBuDkpEOqnRHxKHkwmk9DlEBEREZGZMFwLJDpcg/ySGmTlVghdChERERGZCcO1QCJC1JBKRLwdOhEREZENYbgWiIOdDP0CPXAkNQ9GI1tDiIiIiGwBw7WAosM1KKvS4kx2idClEBEREZEZWCxcZ2ZmYubMmRg7dixmzpyJrKysJsft2LEDkyZNwsSJEzFp0iQUFhZaqkSL6xeogp1cgniuGkJERERkEywWrpcuXYpZs2Zh9+7dmDVrFl5//fWbxiQlJeGjjz7C2rVrERcXh82bN8PZ2dlSJVqcXCZBRLAax88WQKc3CF0OEREREd0mi4TroqIipKSkYOLEiQCAiRMnIiUlBcXFxY3GrV+/Ho899hjUajUAwNnZGQqFwhIlCiaqtwY1dXqcyihufTARERERWTWLhOucnBxoNBpIJBIAgEQigaenJ3JychqNy8jIwMWLFzF79mxMmzYNK1eutPl1oMP8lHBxkCEhJVfoUoiIiIjoNkmFLuBGBoMBZ8+exbp166DVarFgwQL4+Phg6tSpbd6HSuXUgRW2TK2+tRaW4YO6Y3f8BTg628HBTmbmqoRxq3Nhqzgf13EuiIjIllkkXHt7eyMvLw8GgwESiQQGgwH5+fnw9vZuNM7Hxwfjxo2DXC6HXC7HmDFjcOrUqXaF66KiSkGWtlOrnVFQcGs3hOnf0x1xBzPxv98yMbSvd+sbWLnbmQtbxPm4jnNBRES2QCwWNXtC1yJtISqVCmFhYYiLiwMAxMXFISwsDO7u7o3GTZw4EQcPHoTJZIJOp0N8fDxCQ0MtUaKgAnxc4OFqx1VDiIiIiDo5i60WsmzZMmzatAljx47Fpk2bsHz5cgDAwoULkZSUBACYMGECVCoV7rvvPkydOhVBQUGYMWOGpUoUjEgkQnRvDVKyilFWpRW6HCIiIiK6RSKTjV0x2BnbQgDgcmEV/v5ZAmbd3Qt3D/Y1Y2WWx6/+G+N8XMe5ICIiWyB4Wwi1rpuHI3w9nZDA1hAiIiKiTovh2opEh2uQcaUc+aU1QpdCRERERLeA4dqKRIZpAIBnr4mIiIg6KYZrK6JytUNwd1fEJ+fa/M1ziIiIiGwRw7WVierthZyialzMrxS6FCIiIiJqJ4ZrKzM4RA2JWMTWECIiIqJOiOHayjg7yNG7pzsSUvNgZGsIERERUafCcG2FosM1KC6vQ/qlMqFLISIiIqJ2YLi2QgN6eUAuE/N26ERERESdDMO1FbKTSzGwlxpHU/OgNxiFLoeIiIiI2ojh2kpFhWtQVatHcmax0KUQERERURsxXFupPj3d4Wgn5aohRERERJ0Iw7WVkkrEGBLqicS0AtRpDUKXQ0RERERtwHBtxaLCNdDqjDiRXiB0KURERETUBgzXVqyXrxuUzgokJLM1hIiIiKgzYLi2YmKRCFHhGpzOLEZljU7ocoiIiIioFQzXVi46XAOD0YRjZ/KFLoWIiIiIWsFwbeV8PZ3grXLgDWWIiIiIOgGGaysnEokQHa7BuYulKC6vFbocIiIiImoBw3UnEBWuAQCueU1ERERk5RiuOwFPpQMCfFzYGkJERERk5RiuO4mocA0u5lfickGl0KUQERERUTMYrjuJyFBPiERAQirPXhMRERFZK4brTsLVSYFwPyXik/NgMpmELoeIiIiImsBw3YlEhXuhsKwW56+UC10KERERETWB4boTiQhRQyoR88JGIiIiIivFcN2J2CukGBCkwtHUPBiMRqHLISIiIqI/YLjuZKLCvVBerUPqhRKhSyEiIiKiP2C47mT6BbrDXiFFQjJbQ4iIiIisDcN1JyOTShARosbxcwXQ6gxCl0NEREREN2C47oSiwzWo1RpwKqNI6FKIiIiI6AYM151QaA8lXB3lXDWEiIiIyMowXHdCYrEIkWEanMooRHWtTuhyiIiIiOgqhutOKrq3BnqDCcfPFghdChERERFdxXDdSfl7OcNTac/WECIiIiIrYrFwnZmZiZkzZ2Ls2LGYOXMmsrKybhrz4Ycf4o477sCUKVMwZcoULF++3FLldToikQjR4RqcuVCC0so6ocshIiIiIlgwXC9duhSzZs3C7t27MWvWLLz++utNjps6dSpiY2MRGxuLpUuXWqq8TikqXAMTgCOp+UKXQkRERESwULguKipCSkoKJk6cCACYOHEiUlJSUFxcbInD2yxvlSP8NM5ISMkVuhQiIiIigoXCdU5ODjQaDSQSCQBAIpHA09MTOTk5N43dvn07Jk2ahMceewwnTpywRHmdWlS4Bpk5Fcgrrha6FCIiIqIuTyp0ATd66KGH8OSTT0Imk+HQoUN4+umnsWPHDiiVyjbvQ6Vy6sAKW6ZWO1v8mOPvCsC3+9ORdKEUfUI0Fj9+c4SYC2vG+biOc0FERLbMIuHa29sbeXl5MBgMkEgkMBgMyM/Ph7e3d6NxarW64eehQ4fC29sbaWlpiIyMbPOxiooqYTSazFZ7W6nVzigoqLD4cQEgxNcNPx/NxpgB3hCJRILUcCMh58IacT6u41wQEZEtEItFzZ7QtUhbiEqlQlhYGOLi4gAAcXFxCAsLg7u7e6NxeXnXl5VLTU3F5cuX0bNnT0uU2KlFhWuQW1yN7LxKoUshIiIi6tIs1haybNkyLFmyBCtXroSLiwtiYmIAAAsXLsRzzz2Hvn374v/+7/+QnJwMsVgMmUyGd955p9HZbGpaRIgnNv3vHOJTcuHnxa/ciYiIiIQiMplMlu+h6EBdsS0EAP6z5RSycsvx3tNDIRYL2xoi9FxYG87HdZwLIiKyBYK3hVDHi+6tQWmlFuculgpdChEREVGXxXBtI/oHeUAhk/B26EREREQCsqql+OjWKWQSDAr2wPGz+eWckUIAACAASURBVJh9TzBkUn5uEtqR3ERsy9iF0rpSuCncMDlwHCK9BgldFhEREXUgJjAbEhXuhapaPU5nFgldSpd3JDcRm89sRUldKUwASupKsfnMVhzJTRS6NCIiIupADNc2JNxfCSd7GRLYGiK4bRm7oDPqGj2mM+qwLWOXQBURERGRJbAtxIZIJWIMCfPEoVM5qKnTw17BP15LqtHXIK3kPM6VZKCkrukLS5t7nIiIiGwD05eNiQ7XYF/iZfyeVog7+ngJXY5N0xq0yCjLwrmSDJwtSUd2+SWYYIJMLINULIXeqL9pG6XCTYBKiYiIyFIYrm1MYDdXqFzsEJ+Sx3BtZgajAVnlF3G2JA3nSjKQWXYBepMBYpEYPV16YJz/GIQoA+Hv6ocT+aew+czWRq0hUpEEkwPHCfgKiIiIqKMxXNsYsUiEqHANdiVko7xaCxcHudAldVpGkxGXKq/Un5kuTkd6WSa0Bi1EEKG7sw9G+t6FYGUQAl39YSdVNNr22qog2zJ2oaSuFBKRBGKI0cstQIiXQkRERBbCcG2DosM12BF/AcfO5GP0oO5Cl9NpmEwm5FXn42xJBs6VpONcSQaq9TUAAC8HT0R7DUaIMhC9lIFwlDm0ur9Ir0GI9BoEtdoZpy+cR8zRD7A2eTMWD1wEiVjS0S+HiIiIBMBwbYO6ezqhm9oR8Sl5DNetKKopwdmS9KthOh1l2vpbc7vbKdFf3QfBykAEKwPhpnC9reNoHNR4KOR+bEj5Cjsy92AS20OIiIhsEsO1jYoO12DrgfMoLK2Bh5u90OVYjXJtRUObx7mSdBTWFgMAnGVOCFYGIsQ9CCHKIKjs3CESicx67EivQThbko7dF/ahlzIQoe69zLp/IiIiEh7DtY2KDKsP1wmpeZhwh7/Q5QimWleDtNLzOFeSjrMl6cipql8D3F5qh15ugRjpexdClEHwdtSYPUw35cHgqcgqy8b6lC/xauTzcJE7d/gxiYiIyHIYrm2U2s0eQd1ckZDStcL1teXx6s9MZyC74vryeEFuPRHpNQghyiD4OneDWGT5eygpJHI81mc23j32ITYkf4VnBjwuSB1ERETUMRiubVhUuAZf7DmHS/mV6O7pJHQ5HUJv1COr/GLDmenMsmwYblgeb7z/GAQrg+Dv2gMysXW83bs5eWNGr8n48ux32HNhP8b6jxa6JCIiIjIT60gb1CGGhHriy71pSEjNs5lwbTQZcaniytWLEDMaLY/n6+yDUVfbPAKaWB7Pmgz1icLZknTEZf4PQW4BCHTzF7okIiIiMgOGaxvm4ihHeE8l4pPzcP/wAIv0FJvbteXxzlwN02l/WB7vDu/BCFYGIdgtAA5tWB7PWohEIswKnY7s8ktYl7wZf41c3Kbl/YiIiMi6MVzbuOhwDT6LS0XG5XIEdb+95eQspaimGGdLMhruhFj+h+XxQpRBCFYGwlXhInClt8deao/H+szGv46vxBep32Jh30c65QcgIiIiuo7h2sYN7KWGTHoW8Sm5Vhuuy+oqkFaSfjVQp6Po2vJ4cqeGIB2i7AUPe3eBKzU/PxdfTAkcj+/S43Dg0m8Y6TtU6JKIiIjoNjBc2zh7hRQDgjxw9Ew+HhrTC1KJ8CtT1C+Pl9FwJ8Try+PZI9gtAKN9hyFYGWix5fGENtp3GM6VZOD79DgEuPmhhzNv/ENERNRZMVx3AdHhGhw9k4/UCyXoG6DqsOMcyU3EtoxdKK0rhZvCDZMDxyHSaxDqDFqcL83C2asrelysuNxoebworwgEKwMFWx5PaCKRCHPDHsSKo+9j7ekvsGTIn2EntRO6LCIiIroFIpPJZBK6CHMqKqqE0Wj5l6RWO6OgoMLix20Lnd6I5z88iP5BHlg4KbxDjnEkNxGbz2yFzqhreEwsEsPDzh1FtSUwmAyQiCTwd+mBEGWg1S2P15Ha+t5IL83E+4mrEKHpj0fDH7bJs/bW/PeEiIiorcRiEVSqpldis/1kQ5BJxRgcqkZCaj7qdAYoZBKzH2Nbxq5GwRqoXzavqLakoc0j0K0nFBK52Y9tK4LcemJCz3sQl/k/hCh74U6fIUKXRERERO3U9b6D76Kiwr1QpzXgZHphh+y/pK60yccNJgOmBt2HcFUIg3UbjPUfjWBlEL4590NDLzoRERF1HgzXXUSIrxvcnORISOmYwOYsa/qrEaXCrUOOZ6vEIjEeDX8ICokca05vgtagFbokIiIiageG6y5CLBYhMkyDUxlFqKrVtb5BO+RW5aFWX3fT4zKxDJMDx5n1WF2Bq8IF88IfQk5VHrakbRO6HCIiImoHhusuJLq3BgajCcfPFphtnxXaSqw8uQ52MgWmB02CUuEGEerPWM8KnY5Ir0FmO1ZXEq4KwT09RuLQlSM4nve70OUQERFRG/GCxi7ET+MMjbsD4pNzMby/z23vT2vQYdWp9SjXVmDxoEXwd+mB0T2GcUUIM5kUMBbppZnYfGYrejj7Qu3QccsoEhERkXnwzHUXIhKJEB2uwdnsUpRU3NzG0R5GkxEbUr7ChfKLeLT3w/B36WGmKukaiViC+b1nQSQSY23yJuiMeqFLIiIiolYwXHcxUeEamIDbvrBxW8Yu/F6QhGlBEzBA3cc8xdFNVPZKzAmdgeyKy4jN2CF0OURERNQKhusuxsvdAf5ezrcVrg9ejsee7P0Y1u0OjPYdZsbqqCkDPPtiRPc7se/iQSQVpghdDhEREbWA4boLig7X4EJeBXKKqtq9bUrRWXx97geEq0LwQK/JNnkXQWs0LXACujv5YGPKNyipbXpNcSIiIhIew3UXNCRMAxHa3xpyuTIHa05vgrejBo/3ng2J2Px3eqSmySQyPNZnNnQmPdYlb4bBaBC6JCIiImoCw3UXpHRWINRPifiUPJhMpjZtU1pXhv+eXAeFRIGn+s2HndSug6ukP9I4qPFwyP3IKMvCjqy9QpdDRERETbBYuM7MzMTMmTMxduxYzJw5E1lZWc2OPX/+PPr374+YmBhLldflRIVrkF9Sg6zc1pfMqzNoserUelTpq/FU//lQ2vGui0KJ9BqEaK/B2J31M84UpwldDhEREf2BxcL10qVLMWvWLOzevRuzZs3C66+/3uQ4g8GApUuX4u6777ZUaV1SRIgaUokI8cktt4YYTUasS/4Clyqu4PHes+Hr3M1CFVJzHgyZCk8HNdanfIlyLdcTJyIisiYWCddFRUVISUnBxIkTAQATJ05ESkoKiouLbxr76aefYuTIkfD397dEaV2Wo50MfQNUOJKaB6Ox+daQ79LikFSYigeCp6CPR5gFK6TmKCRyPN5nNmr1tfg85WsYTUahSyIiIqKrLBKuc3JyoNFoIJHUXwAnkUjg6emJnJycRuPOnDmDgwcP4tFHH7VEWV1edG8vlFVpcSa7pMnn9188hH2XDmKU710Y0f1OC1dHLenm5I3pvSYjtfgc9l44IHQ5REREdJXV3P5cp9Ph73//O1asWNEQwm+FSuVkxqraR612FuzYt2KMmwPW7zyDk+eLMWKIX6Pnjl0+hS3p2zC4W38sin4YYnH7Pod1trnoaB0xH9M87kZWdRZ+zNyNwT17I8Qj0OzH6Ah8bwC/XjiCL0/Foqi6GCoHdzzcbwqG+UUKXRYREZmBRcK1t7c38vLyYDAYIJFIYDAYkJ+fD29v74YxBQUFyM7OxhNPPAEAKC8vh8lkQmVlJd588802H6uoqLLFNoeOolY7o6Cg8/W/DurlgYMnr2DG8J6QSes/1GRXXML7xz+Dr5MPZgU9gKJ2rofdWeeio3TkfMzoOQVpBVn4v4Of4a+Ri+Eoc+iQ45gL3xvAkdxEbD6zFTqjDgBQWF2MVUc2oby8BpFegwSujoiI2kIsFjV7QtcibSEqlQphYWGIi4sDAMTFxSEsLAzu7u4NY3x8fJCQkICff/4ZP//8M+bNm4cHH3ywXcGa2i+qtwY1dXqcyqjvfy+pLcWqk+vgKHPEk/3mQyGRC1whtcReao/H+8xGubYCX6R+2+alFUk42zJ2NQTra3RGHbac24bMsmxUaqv450hE1IlZrC1k2bJlWLJkCVauXAkXF5eGZfYWLlyI5557Dn379rVUKXSDMD8lXBxkSEjJRXigM1aeXIs6gw4vRiyEq8JF6PKoDfxcfDElcDy+S4/DgUu/YaTvUKFLohaU1DV9h80qfTXeO/4RAMBeage1vQpqew942Kvqf3bwgNpeBRe5M++MSkRkxUSmNp4iiY+PR7du3eDr64v8/Hz861//glgsxgsvvAC1Wt3RdbYZ20La74s953Dg90voM+Y80srS8XS/xxCmCr7l/XXmuegIlpgPk8mEVafW4UxxGl4c/Ax6OHfv0OPdqq7+3rhSmYsVR96HETev8OIqd8bDodNRUFOEguoiFNQUorCmCEW1JY1WhJGLZfWB+2rYbgjf9h5Q2rlCLOK9wYiIOlpLbSFtDtfjx4/HmjVr4OPjgxdffBEAoFAoUFxcjFWrVpmv2tvEcN1+6ZdK8e7BjZB6XsSskOkY2i3qtvbXmeeiI1hqPiq1VVhx9H3IxFIsGfJnq7yLZld+byQVpmB98pcAAL3RAL1J3/CcTCzDrNDpTfZcG4wGFNeWNoTtgpr64F1QXYTC2mLojdf3IxVJoLJ3v37W2+Fa8FZBZecOifjWLxYnIqLrWgrXbW4LycvLg4+PD/R6PQ4ePIiff/4ZMpkMw4YNM1uhJIwsw0lIPS/CtTr8toM1CcdJ7ohHwx/GByc+wZdnv8Oj4Q+zfcAKmEwm7M0+gNiMneju7INFfechrfQ8tmXsQkldKZQKN0wOHNfsxYwSsQRqBxXUDqqbnjOajCirK28I2wU3hO9zpeehNWgbxopFYrgr3Bqd9b525tvDXgW5RNZhc0BE1JW0OVw7OTmhsLAQaWlpCAwMhKOjI7RaLfR6fesbk9U6kZ+EHzJ2wFMUgOxkX5SN0sLVkRcxdla9lAGY0PMexGX+DyHKXrjTZ4jQJXVpOoMOm89uxZHcRAzy7Ie5YQ9CLpEj0muQWVYGEYvEUNq5QWnnhmBlUKPnTCYTyrWVV892F15tNylEYU0xjuX9jhp9TaPxbgrXhsB9/ay3B9T27lb5LQgRkbVqc7ieM2cOZsyYAZ1Oh1dffRUAkJiYiICAgA4rjjpWZlk2NqR8CX8XX8zo8RDeSEjE0dQ83D3YV+jS6DaM9R+Nc6Xn8c25H9DTtQe8HTVCl9QlldVVYHXSBmSWZ2Niz3sxzn+MRb9JEIlEcFU4w1XhjEA3/5uer9JV17ea/OGMd1JRKiq0lY3GOsuc6s+e23vAw969PnRf/d3al38kIrK0NvdcA0BmZiYkEgl69OjR8LtWq0VISEiHFdhe7Llum8KaYrx37CPIJXK8PPhZOMudsHTtEcilYrz2yODb2ndnm4uOJsR8lNWV4x9H/g0XuTNeHvwnq/nKv6u8Ny5WXMaqU+tRpavGI+EzMcizn9AltUutvhYFNcXX+7yrr5/9Lq0razTWXmp//Yy3g0ejCyxd5E5t+kBxJDexzW0yRETWwCw91wDQs2fPhp/j4+MhFosRGcm7inU21boa/PfkWuhNBizu/xic5fVvjuhwDb7dn4H80hp4utkLXCXdDleFCx4JfwgrT67BlrRtmBU6XeiSuozE/FPYmPI1HGWOeDHiafg6dxO6pHazk9rB19kHvs4+Nz2nNehQVFuMgur6sH3tIssL5RdxoiCp8comEnmjVpOG1U0cVHBT1K9s8seb6pTUlWLzma0AwIBNRJ1Su9pCnn/+eURERODTTz/F+vXrIZFIMHv2bDz55JMdWSOZkd6ox+rTG1FQU4RnByyAl6Nnw3ORYfXhOiElD5Pu9BeuSDKL3qoQ3NNjJPZk70eIMhARmgFCl2TTTCYTdmbtxfbMPejp0gML+86Dq8L2bvUul8jg7ahpst3IYDSgqLakocXkWstJTlU+ThemQm8yNIyViqXwsHNHUW0xdMbG1+7ojDpsy9jFcE1EnVKbw3VaWhoGDKj/x/nbb7/F559/DkdHRzz88MMM152EyWTCl2e/w7mSdDwSNhPBysBGz6tc7RDc3RXxybmYeIcfV5qwAZMCxiK99Dw2n9mKHs6+Ta44QbdPa9Di89RvcCL/FKK8IvBwyP2QWUkrjiVJxBJ4OnjA08EDQON2QaPJiNK6skYtJgU1Rcitzm9yX83dbIeIyNq1+W4DRqMRIpEI2dnZMJlMCAoKgre3N8rKylrfmKzC7gv7EJ9zDOP970aUd0STY6J6eyGnqBoX8yubfJ46F4lYgvm9Z0MkEmNt8heN1kQm8yipLcX/Jf4Xv+cnYVrQBMwNe7BLBuvWiEViuNspEeIehLu6RWNa0AQ80fcRKBVuTY5XSBQoqyu3cJVERLevzeE6IiICb7zxBmJiYnDPPfcAALKzs6FUKjusODKfY7kn8OP5XRiiGYgJPe9pdtzgEDUkYhESUvIsWB11JJW9EnNCZyC74hJiM3YKXY5NySy7gHeOfYiC6kIs6jcPd/cYwW982mly4DjIxI0/jIghRp2hDksP/xNb0rahrM72L4IlItshWbZs2bK2DBw6dChSU1Ph6emJJ598EnK5HCdOnICfn19Du4g1qKnRou3rn5iPo6MC1dXa1gcKIL00E6uTPkdPV38s6Du3xbu0KWQSnL9SjtQLJbh7sO8tBQVrngshWMN8eDlqUKmtwv5Lh9DDuRs0DmpB6rCGuTCXhJzjWH16IxylDvjTwCcQ5Naz9Y3oJt2cvOFup0R2+SXUGmqhVLjhwZApmBp4H6p01Th05Qh+ufwbqnTV6O7sA4VEIXTJREQQiURwcGj6viDtWoqvM+BSfI3lVxfiveMfwVHqgBcHPwMnmWOr28Qn5+LTH1OwZPYgBPs2/ZVtS6x1LoRiLfOhM+jw3vGPUVJbir9GLobSrv1/trfLWubidhhNRmzL2IU92fvRyy0AC/rMhZO89b9XdGvyqwuwK+tnHMlNhFQsxfBud+Aev5ENqxwREQmhpaX42twWotPp8J///AdjxoxB3759MWbMGPznP/+BVmsbZ6FsUaWuCv89uRYA8FT/x9oUrAFgQC8PyGVixLM1xKbIJDI81mc2dCY91iVvhsFoaH0jaqRWX4tPkzZgT/Z+3OUThWcHLGCw7mCeDmo8Ej4Tf49+CQM9++Lni7/i9d9W4Pv07Tfd7IaIyBq0uS0kJiYGx44dwyuvvIInnngCgwcPxvfff4/09HQMGzasg8tsO7aF1NMZ9fjvyXXIrc7HM/0fb3K92uZIJWJcLqzC72kFuHeIL8Ti9rWGWNtcCM2a5sNJ5gh3Ozfsu3QQJgAhf7hldkezprlor8KaYnz4+2pklmfjgeApmNDz3hZbrMi8nGSOGKDugwjP/qjQVeHQlQT8cuk31Orr0N3JB3JJ01/PEhF1hJbaQtq8FN+uXbsQGxvbcAFjQEAAwsPDMWXKlIbboZN1MJlM2JT6DTLKMjG/96wmb33cmqhwDRJS8pCcWYz+QR7mL5IEE+k1CGeL07E762f0cgtAqHsvoUuyemklGVh9eiOMJhOe6f8450xAGkdPPNr7YYzzH4OdWXuxN/sADlz+DSO7D8UY3+H8JoGIBNfmtpDmWrNtrGXbJmzP3INjeb9jUsA4DL7FG4f06ekORzspVw2xUQ+GTIWngxobUr5CubZz90B3tEOXE/Cf31fDSeaIvwx+lsHaSng5emJ+71n4W9QL6KsKw54L+/H64RWIzdiJSl2V0OURURfW5raQnJwcrF27FhqNBnq9HqdPn8by5csxdOhQDB8+vIPLbLuu3hYSn3MMW9N/xB3eQzAtaMItLwsmFotQWFqDI2fycc9gX0glbf4cZjVzYS2scT6kYgmC3Hpi/6VDuFRxBYM1AyyyhJw1zkVzDEYDtqb9iB8zdyPUvRee6b8AbnauQpdFf+Akd8JAz34YoO6Lcm1FfbvI5d9QZ9Ciu7MP5FxznIg6QEttIW0O19HR0cjNzcXq1auxdu1aJCYmYtSoUQDql+mzFl05XJ8rScea018gWBmIx3rPhljc9kDcFDu5BL+czEE3tSO6q9t+Zb41zIU1sdb5cJE7w0nmgP2XDkEuliHQAkvJWetc/FG1rhqfJn2OY/m/Y5TvXZgb9iAUUvb0WjPnG0J2qbYcB6/E49fLh6E16uDr5MMb+xCRWXXYUnx1dXUYMGAAUlNTb7k4c+uqS/HlVuXhveMr4apwwYuDnoaDzP6292k0mfDyyt/Qw9MJf36gf5u3E3ourI01z4fJZMKa5C9wsuA0nh/0JAJc/Tv0eNY8F9fkVRdg1al1KKopwUMh03CnT6TQJdEtuFyZg52Ze3GiIAl2EjuM8h2K0b7D4CBzELo0IrIBZlmKrykikYg911agQluJlSfXQSqS4Ol+880SrAFALBIhKlyD05nFqKzRmWWfZF1EIhFmh06HUuGGtac3o0pXLXRJgkotOod3j32Eal0Nnhv4BIN1J9bNyRsL+s7Fq5HPI9S9F3Zm/YTXD/8T28//D9W6GqHLIyIbdnt9AwBv9SswrUGHT06tR7m2HE/2fxQqe3ez7j86XAOD0YRjZ/LNul+yHvZSezzWZxbKtOX4IvXbLvmB2WQyYd/Fg/j45BooFa74y+A/8Y6LNqKbkzcW9p2Lvw5ZjGBlEHZk7cXrh1dge+Ye1OgZsonI/Fpdiu/w4cPNPqfT8WymkIwmIz5P+QpZ5RexoM8c+Lv0MPsxfD2d4K1yQHxKHkYO7Gb2/ZN18HfpgSmB4/F9+nYcuPQbRvpaz3UUHU1v1OObcz/g0JUj6OfRG/PCZ8JOaid0WWRm3Z198ETfR3Cx4gp2Zu7Bjsw92HfxIMb4DsNI37tgzz9zIjKTVsP1a6+91uLz3t7eZiuG2mdbxi6cKEjCtKAJGODZt0OOIRKJEB2uwfe/ZqK4vBbuLvwHyFaN9h2GcyUZ+D49DgFufujh3F3okjpcpbYKq09/jvTSTIz1G42JAfdCLLrtL/TIivk6++CJfvNwseIytmfuQVzm//DzxV8x2nc4RvoOZcgmott2Wxc0WqOuckHjocsJ2Hx2K+7qFo2Hgqd1aHtOfkk1lnwSjwdGBWJ8lF+r4zvDRWuW1Jnmo1JbhX8c+TfkEhmWDPmz2c/gWtNcXKnMxapT61CmrcCc0AcwxGug0CWRALLLL2FH1h4kFabCUeqA0T2GY2T3O/ntBRG1qKULGtu8FF9n0RWW4kstOod1KV8izD0Y88Ifuu0l91rjaC9D0vkiXMyrbFNrSGdZbs1SOtN8yCVy9HDujn0XD6K4thT91X3M+sHNWubiVEEyVp5aC7FIjGcHLEBvVajQJZFAXBUuGKwZiD6qUBTWFOPglXgcupwAo8mI7k7ekIrbfCNjIrKgI7mJ+O/Jddia/iMOXzkKJ7kjujlZrpvCLOtcdxa2Hq4vV+bg45Nr4OmgxtP9H4NcYpm1d+t0BhxKysXgUE+4NPNmusZaApS16GzzobJXAgD2XzoEdzslfJ3N12sv9FyYTCbsyd6PzWe2wsfJC38etAhejhrB6iHr4aZwxRCv+pBdUFNUH7KvJMBkMqGbkw9DNpEVOZKbiM1ntqJKX7/CVa2hFilFZ+Fup7RYwG4pXLO5sBMpqyvHf0+ug0KiwFP95lu0NzAy1BMiEXg79C5inP8YBLsF4utzPyCnyjb+zHUGHTakfI3YjJ0Y5NkPzw96Cm4K3nGRGvNz8cXT/R/DSxHPws/FF7Hnd2Lp4X9iz4X9qDN0ng/JRLbKZDIhNmMndMbGi2rojDpsy9glUFWN8aN4J1Fn0GLVqXWo0lfj+UFPQmnnZtHjuzopEO6nREJKLqYN68klGG2cWCTGvN4PYcWR97H29Bd4efCfOvVtpMvqyvFp0ufIKs/GxJ5jMc5/NN/D1KKerj3wTP/HkVl2Adsz9+CHjB3Ym30Ad/cYgeHd74TCQt8aEnVFOoMORbUlKKwpQmFtMYpqilFYU9zwu7aZD7oldaUWrrRpDNedgNFkxLrkzbhYcQWL+s0TbBWHqHAvrN2RivM55Qj04Rk/W+emcMUj4Q9h5ck12JK2DbNCpwtd0i3JrriET05tQLWuGgv7zO2wlXXINvV09cOzAxbgfNkF7Lgasn/K/gV3+43A8G53WKw1j8iWmEwmlGsrUVRbdD00Xw3QRbXFKK0razReJpbBw94dHvbuCFEGISH3OKqbWKdeqbDsicfmMFx3At+lxyGpMAUPBE9BX49wweoYFKzG57vPIiE5j+G6i+itCsE9PUZiT/Z+hCiDEKHpL3RJ7ZKYfwqfp3wNJ5kjXoh4Br7OPkKXRJ1UwNWQnVGahR2Ze/B9+nbsvXAA9/iNxLBu0QzZRH+gNWivn32uuXr2ufZ6iP5jW4ebwhUqu/rwXB+kVfCwd4fKTgUXuVOjbxt7uHTH5jNbG+1DJpZhcuA4i72+ljBcW7n9lw5h38WDGNX9LozsLuyNPRzspOgfpMKRM/mYOSYIkg5epYSsw6SAsUgvPY/NZ7aih3N3qB1UQpfUKqPJiJ2Ze7Ejay8CXP2wsO8jcJE7C10W2YBAN3/8aeBCpJdmYkfmHnyXHoe92fUh+y6f6E7dPkXUHkaTEeXaiuvB+WrLRv3vRSjTNl52VS6RQ22vgqe9B8LcgxvCs4edO9ztlJC14+9OpNcgAPX3+yipK4VS4YbJgeMaHhca17k2k45YvzepMAWfnNqAPh5heKLvI1Zxc4vjZ/Px8fen8eLMAejds+lbrVvTWsbWwBbmo6imGCuOfgC1vQovRjx9yysnWGIu6gxafJ7yNX4vSEK012A8FHo/ZFzpgTpIemkmtmfuwbmSdLjInXGv3ygM9YliyCabUGfQNhmcr7Vv6Iz6hrEiiOCmcG101tnDzh2qqz87yRxt6lqXlta55r84VupixWWsTd6M7s4+mN97llUEW4hQ8gAAIABJREFUawDoF6iCvUKC+JTcZsM12R6VvTvmhM7A6tMbEZuxE9N7TRK6pCaV1JZi1an1uFyZg2lBEzDGd7hN/c+crE+QW0/8eeATSCvJwPbMPdiStg17LuzDvX6jMdQnsl1n44gszWgyoqyu/HrPc219kC66+nOFtrLReDuJAh72Kng5eqK3Ryg87FQNvdBKOyVPZFzFWbBCJbWl+O/JdXCUOuCpfvOt6qp0mVSCiGBPHDubj7n3GiCXSYQuiSxkgGdfDO92J36++CuClYGC9v835XzZBXyatAE6gx5P9nsUfTzChC6JupBeykAsVgbiXEkGtmf+D9+mxeJ/F/bhXv9RGOrNkE3mcSQ3sd2tELX62puD89X+5+KaEuhNhoaxIojgbucGlb0KfVXhDcHZw14Flb07HKUOPGHRBhYL15mZmViyZAlKS0vh5uaGmJgY+Pv7NxqzdetWrF+/HmKxGEajEQ888AAeeeQRS5VoFWr1tfjvqXWoM9ThhYin4apwEbqkm0T11uBgUg5OZRRhcKin0OWQBd0fNAHny7KwMeUb/DVyscWXhGxOQs5xbD6zBW52bvjzwEXw5o1hSCDBykD0cnsSaaUZiDu/B9+ei8WeC/sx1m8U7vCJ5Jm9drqVMGmrrt045dpFfCV1pdh8ZitMJhOC3AJuWHmjuNESdpW6qkb7sZfawcNehW6O3ujv0QeqawHaTvX/7d17XNP3vT/w1/ebhJAAAQIkXLwgqBiZ1itCO4tVq/bM6qrrsfP0srrZX7d2du12zvG3/X5d127nHPvb8bFus+t6tnXznK5rbW1taWvtTacWlVovlZuK4gVCgAAiEMjt+/sjISFy8ULINwmv5+PRB+Gbb/h++DTqizfvz+cLfWwSFCKLZsMVsp7r+++/H6tXr8bKlSuxY8cOvPHGG9i6dWvAOR0dHYiL8/TkdHR04M4778Tvfvc7TJly7bcmjuSea5fbhReO/xlVrafwvenrYEqZHKTRBZfbLeGJLfsxKSsRj6zqv61ZNPQYB1O0zYelqwn/UfYcxsZn4bGZD13XX8TBngu35MbbNTvx4fndmJyUi29PuxfxqrigfX2i4ZAkCdWtp/Hu2Q9x5lItktSJWDp+IYoy5zJkX4MrwyTg2RFi7ZTVIQnYkiRBggS35O7znwQ33JAkKfCY5PYev/Jzz2OX1Oc5eD5Kva+/4hpSn9f7z5NQcnYXbANsP3clURChj0329jv7K8+psZ7HWpV2xOduNJC959pqtaKiogIvvfQSAGD58uV45pln0NLSAr3e37cbH+8fZHd3NxwOx6j59YMkSXjt1A5UtFRjbd7qsA3WgOcNVWAyYPeRenR1O6CN5a87RxOjNg33TL4LWytfxXu1H+HOnKWyjMPm7Mafy/+KE9YqzM8qwt2TVrDiQmFFEARM0U9CXvJEb8jehVdPvold5z7F0uyFKMqYM6pvqy5JEuxuB7ocXbA5u9HltMHmtPkel9R8MOBd+P5a9Qa+aDzmD7F9Qq4Ef7iVvMHVH2z7PNcn0EqS55wrw7GEyNnvYe2U1b7+5yR1Iv8ulFlI/lSbzWYYjUYoFJ7/2QqFAgaDAWazOSBcA8DHH3+MzZs34/z58/jhD3+IvLy8UAxRdh9f+Dv21R3A7eMW4JaseXIP56oKp6bjo88v4vDJJsyfzr2DR5t5GbNR3XoaH9R+gklJOZiinxTS6zfbrPjd8T+jsasJayZ/HbeOuTmk1ye6Hn1DdlXrKbx75kP8rXo7Pqj9BMuyF6IwQkO2JElwuJ2wOW2+YNzl8IRjz7Fub1j2H78yQLsl93Vf1+F2oLX7EkRBgCCIECFCFESIggBRUEEURAiCcMVxz2MBfT/3v17R+xrveSK8z/UegwhR7Hvc+xzEwPOGen3faw7wes8Yes8V/N+H99i/l/2q381VAM+NU27JDP/cMJqE3Z/mRYsWYdGiRaivr8cjjzyCW2+9FTk5Odf8+sFK9KGQlnZj++gevHgEb51+D4VjZuHbhXeHzc4gQ0lNjUdGShy+ONWMVYv6/wB0o3MRraJxPh65+V5c/LAO/131Kp5d+hMkxV7b+oDhzkV540lsPvwi3JDwk+LvY5rx2tvGiORmMMzG/MmzcKyhEttOvINXqrfjowu7sWrqHSieUITSC4fxyvEdsHa1IEWrxzenr8T88QUjNh6Hy4FOhw2d9i502rvQ5bCh09GFTrsnEHfau3zPd3mPdzq60GW3odNhg7PPVmwDiVGoEKfSQhujQZxKi5T4RGhV6QHH4mK0iPM+1qo0ns9VGmz88N/R3NXa72umavXY/LX/O1JTErbuc67C78teDrj1d4wiBvfOvCsq/42JZCHpubZarVi6dCkOHjwIhUIBl8uFefPmYdeuXf0q1309+eSTyM7Oxrp1667jWpHVc13bfh6/+uL3yIrPwGMz/1dE7Y365t/PoOSzWvzno7cgKV7tOx5tPcbDFc3zUddhxrOf/waTknLwvZvWXfUHw+HOxb66A3j15FtI06Ti4enfgkGbesNfi0hukiShouUk3j27C+faLyBOqUW3qweuPrs3XK3H2Ol29qkU22Bz9K8O+5532PpVlB1XCcdKQQGNSgOtUgONUgONMtb7OBYapQZalee41vu577FKA40idli7pMjdcx2OuMAzfMjec52SkgKTyYSSkhKsXLkSJSUlMJlM/YJ1TU0NcnNzAQAtLS04ePAglixZEoohysJqa8ELx/4MXUwCHp7+rYgK1gAwb6oR73xWi0OVjVgyd6zcwyEZZMVn4BuT7sTfqt/ER+f3YMn420bkOi63C2+cLsGei/sxVZ+HdV9ZC41SMyLXIgoVQRCQn5KHqfrJKLdW4cUvtwYEa8DTAvG36jdRYa32BeguZzds3qBsv6In+UqiIEKrDAzHSbFJvjCsvTIQXxGU5fx3KdzvwieHgvRZo/r7jxQhawt56qmnsHHjRjz//PPQ6XTYtGkTAGD9+vXYsGEDpk2bhldffRX79++HUqmEJEm499578dWvfjVUQwypLocNzx9/CU7JhR/c9CASYuRrZ7lRmalxGGeMx8GKBobrUeyrmYWobjmNd858gIlJE5CTmB3Ur9/l6MIfT7yMqtZTWDh2Pu6a+LWIaJ0iulaCIOArqaZ+wbpXj6sHZ9vP+0JvulbnC8P9K8rex97nVKIqojcGYJikSMTbnwfJ9fy62+l24vljf8LptrN4dMa3MTl54giPbuTsPHger316Gv/+UCGMes/2PtHcBnEjRsN8dDls+I+yX8EtSfjfBT9A3CBbPV3vXDR0NuL3x/8Ma3crvpm3CkWZc4M1ZKKw83/2/xtae9r6HU9WJ+Hnt/xYhhER0WCGagth+SfEJEnC36rfRHXraaydsjqigzUAFJgMEAAcrLDIPRSSkValwbqv/BMu2dvxcuU2BONn9gprNX55+LfoctqwYeZDDNYU9VbkLoNKDGzDUIkqrMhdJtOIiOhGMFyH2AfnPkWpuQx3ZC9CYcYcuYczbHpdLCaPTcKBCktQAhVFrmzdOKzMvQPHmsuxp+6zG/46kiThkwt78fyxP0Efm4x/mbMBE5MmBHGkROGpIH0W1k5ZjWS1586nyeqkUb14jyhShd1WfNHsc8tRvHNmJ+YYZ+BrE6Jnoea8fCO27qzGeUsHxqdzO6DRbOHY+TjZWoM3T5UgNzEbYxOyruv1TrcTr1a/hc/Mh3BTaj7un3oPYpXqq7+QKEqwx5go8rFyHSI1bbX478rXkJuYjXtN/xjRC0yuNCfPAIUo4EBFg9xDIZmJgoj7TWsQp4rDH0/8D7qd3df82sv2Dvz6yH/hM/MhLMtehO9Mu4/BmoiIIg7DdQg0djXj91/+GXp1Eh6a/gBUEXgnrqHEa1SYlpOCgxUWWRaTUniJj4nDg/nfRLOtBX+rfvOa2oV698s+f/kCHpz6TdyZs5Q7ghARUUTiv14jrMPRid8d+xMA4Ls3rUO8Kk7mEY2MeVONaOuw4+SF/ivdafSZlJyLOyYsRpnlCA6YPx/y3GNN5fjPw1vgcjvx+KzvYk76zBCNkoiIKPgYrkeQw+3Ei8e3oqW7FQ9NeyCq7yY3Y2Iq1CoFDnDXEPK6I3sRJiXl4LWTb6Ghs//7QpIkfFD7Cf7ry60wag34l7kbMF7H/dKJiCiyMVyPEEmS8HLlNtRcOov7TP8Y9bsdqGMUGGOIw95j9Vjxwx345+f3o7ScPdijmSiI+Fb+NxGjiMEfT7wMu8t/Jzm7y4E/V7yCt8/sxCzDdDw+67tIUifKOFoiIqLgiK7m3zDy3tkPUWY5gjtzlo6KX3OXljfgXMNl9HbXWtt78Jf3qwAARfnp8g2MZJWkTsT9U+/B88f+iI37fga7y45EdSIUggLW7hbcmbMMS8ffFlULfImIaHRj5XoEHDQfxnu1H6EwfQ6Wjl8o93BCYvueGjhdgQvX7E43tu+pkWlEFC46HZ0QBRE9LjskAG09l2DtbsFtY76KZdkLGayJiCiqMFwH2cnWGrxc9TomJ0/EN6esGjXBwdreM+hxl9sd4tFQOHm7ZifcUv/3wNGmEzKMhoiIaGQxXAdRQ2cjXvxyK9I0KVj/lfugjLIt94aSoht8P+InfrsfL394EjX1l3gXx1GotWfgHWQGO05ERBTJRk/6GyGHGr7A2zU70drTBhEiVKIK371pHbQqjdxDC6lVxbn4y/tVsDv9FcoYpYjiGZlovdyDPUfr8fHhizAka1A41Yii/HQY9VoZR0yhkqxOGjBI997imYiIKJowXA/DoYYv8NeqN+Bwe3ZBcMMNl+TCmUu1SNXoZR5daPUuWty+pwYt7T3Q69RYVZzrO97V7cTh6kaUljfgnf21eHt/LSZk6FCUb0SByQhdXIycw6cRtCJ3WcCfEwBQiSqsyF0m46iIiIhGhiBF2e/prdaOkN0l8P/s/7dBK3I/v+XHIRlDOEpLS0BT0+VBn29p78bBSgsOlFtwobEDoiAgf4IehflGzJqUBnWMIoSjHXlXm4/RoPc3PG09bUhSJ2FF7jIUpM+Se1hEREQ3RBQFpKTED/gcK9fDwF7SG6PXxeKOeeNxx7zxuNjUgQPlFhysaMB/vWOFWqXAzMmpKJyajvwJyVCIXBYQDQrSZ6EgfRZ/0CAioqjHcD0M7CUdvjFp8fjGgnisKs7BqQttOFBhQVllIw6UW6DTqjDX5OnPnpCRMGp2XiEiIqLIxXA9DOwlDR5REJA3Lhl545KxdvFkfHnGitLyBt9CSGOyBoX56SjMN8KYzIWQREREFJ7Ycz1M7CXtL5i/+u/qduBwdRNKyxtQfb4NEoCcTB0Kp0bOQki2QvhxLoiIKBoM1XPNcB0kDA1+IzUXgy2ELMo3YmYYL4Tke8OPc0FERNGACxopKgy0EPJARQNe9C6EnDU5FYX56ZiazYWQREREJA+Ga4pIVy6ELC234POqRpR6F0IWmIwo+ko6stO5EJKIiIhCh+GaIlrfhZD/dPtkHK+x4kBFA3YfrcNH3oWQRd6FkAYuhCQiIqIRxnBNUUOlFDE7Lw2z89LQ1e3A59VNOFDegB37zuKtfWeRk6lDUX465poM0GnDfyEkERERRR4uaAwSLtTyC7e56F0IWXrCgotNnoWQX8nRo3BqaBZChtt8yIlzQURE0YALGmlUC1gI2diB0ooGHKywBCyELMpPh4kLIYmIiGiYGK5pVBljiMfdholYXZzbfyFkXAwKTAYU5XMhJBEREd0YhmsalQZdCHmkDh99fhFGvRZFU41cCElERETXheGaRr2rLYTMzdShkAshiYiI6BpwQWOQcKGWX7TMRUt7Nw5WWFBafsVCyN47QqqubSFktMxHMHAuiIgoGnBBI9EN0OticUfheNxR6F8IeaDcguM1vQsh01CUb+RCSCIiIvJh5TpIWJHzi+a5cEuSdyFkA8qqmmDrcQ66ELK0vAHb99Sgpb0Hep0aq4pzUZSfLvN3IK9ofm8QEdHoMVTlmuE6SBga/EbLXDicLs9CyHILjtU0w+mSPAsh841Qq0S8+fezsDvdvvNjlCIeuGPKqA7Yo+W9QURE0Y1tIUQjQKVUYHaeAbPzDOjsduCwdyHkW3vPDni+3enG9j01ozpcExERRTuGa6IgiItV4dabMnHrTZloae/Gj57/bMDzrO09IR4ZERERhVLIVmGdPXsWa9aswdKlS7FmzRrU1tb2O2fLli342te+hjvvvBOrVq3C3r17QzU8oqDR62KRolMP+JwoCthVdgEdNkeIR0VEREShELKe6/vvvx+rV6/GypUrsWPHDrzxxhvYunVrwDl79+7FnDlzoNFoUFVVhXvvvRf79u1DbGzsNV+HPdfy41x4FjP+5f2qgJ5rhShAr1Ojqa0bSoWIuVPSUDwjC5PGJI6au0HyvUFERNFA9p5rq9WKiooKvPTSSwCA5cuX45lnnkFLSwv0er3vvPnz5/se5+XlQZIktLW1IT2dPaoUWXr7qgfaLeS85TL2HKvHgfIGlJZbkJkah+KbMnHztHTExapkHjkRERENR0jCtdlshtFohELhuemGQqGAwWCA2WwOCNd9vfXWWxg3bhyDNUWsovx0FOWn96vWjjMm4L4lefjHBRNxqNKC3Ufr8crHp/D6nhrMnWJA8YxMTMwaPdVsIiKiaBKWCxoPHTqE5557Dn/605+u+7WDlehDIS0tQbZrhxvORaDB5mNMVhJWLc7DmbpL2HmgFrsPX8RnJxowPj0BSwuzcdvsMYiPsluu871BRETRLCQ911arFUuXLsXBgwehUCjgcrkwb9487Nq1q1/l+siRI/jBD36A559/Hvn5+TdwLfZcy41zEeh65qPb7sShykbsOVqHs+bLUClFFEwxoHhmFnIzdRFfzeZ7g4iIooHsPdcpKSkwmUwoKSnBypUrUVJSApPJ1C9YHz9+HI8//jh+/etf31CwJop0sTFK35Z+5xr8vdn7TzRgTFocimdkoSjfCC17s4mIiMJSyHYLqampwcaNG9He3g6dTodNmzYhJycH69evx4YNGzBt2jSsXr0adXV1MBqNvtc9++yzyMvLu+brsHItP85FoOHOR7fdiYMVFuw5Wo/ahsuIUYqYazJgwYws5ERYNZvvDSIiiga8/XkIMDT4cS4CBXM+zjVcxp6jdSitsKDH7sKYtHgUz8hEUX46tLFhuYQiAN8bREQUDRiuQ4ChwY9zEWgk5sPW48TBSgv2HKnHOYunml1gMqJ4ZiZyMsK3ms33BhERRQPZe66JKLg0aiUWzMjCghlZqG1ox+4j9ThYYcG+L80Ya/BUswunRkY1m4iIKJqwch0krMj5cS4ChWo+bD2e3uzdR+tw3tKBGJWnmr1gRhYmZCSERTWb7w0iIooGrFwTjQIatRILZmaheEYmar292QcrGrHvuBnjDPEonpmFwqlGaNT8Y09ERDRSWLkOElbk/DgXgeScD1uPEwcqLNhzpA7nGzugVikwb6oBxTOykJ0e+mo23xtERBQNWLkmGqU0aiVum5mFBTMycdbsqWYfqLDg78fMGGeMx4IZWZjHajYREVHQsHIdJKzI+XEuAoXbfHR1O3GgogG7j9TjYlNvNduIBTMzkZ2uG9Frh9tcEBER3QhWronIRxurxMJZY3DbzCycMbdjzxHPXSD/fqwe440JKJ6ZiXkmVrOJiIhuBCvXQcKKnB/nIlAkzEdXtxOl5Q3Yc7QOF5s6oY5RoHCqZ6eR8ekJQbtOJMwFERHR1bByTURD0sYqsWj2GCyclYUz9e3YfbQOpScasOdoPbLTE1A8IxPzphoRG8O/MoiIiIbCynWQsCLnx7kIFKnz0dXtQGm5Z9/suqZOxMYoUJifjuKbMm+4mh2pc0FERNQXK9dEdN20sSpfNbumvh17jtRh/5dm7D5ShwkZCSiekYUCk4HVbCIioj5YuQ4SVuT8OBeBomk+OrsdvnaRumZPNbsoPx3FMzIxznj1anY0zQUREY1erFwTUVDExaqweM5YLJo9BjV1nt7sfV+a8emROkzI0GHBjEwUmIxQxyjkHioREZEsWLkOElbk/DgXgaJ9PjpsDu9OI/Wob+6ERu3pzV4wIwtjDYE/1Uf7XBAR0egwVOWa4TpIGBr8OBeBRst8SJKEUxcvYc/RepRVNcLpciM3U4dbZ2QCAN7edxYt7T3Q69RYVZyLovx0mUdMRER0YxiuQ2C0BKhrwbkINBrno8PmwGcnPPtmm61d/Z6PUYp44I4pDNhERBSRhgrXYojHQkSjQLxGhSVzx+Ln35kHnVbV73m7041tn56WYWREREQji+GaiEaMIAho73IM+Fxbhx1PvXQI7x04h6Y2W4hHRkRENDK4WwgRjagUnRrW9p5+x7VqJZQKEa/vrsHru2swIUOHApMBc6cYoNfFyjBSIiKi4WO4JqIRtao4F395vwp2p9t3LEYp4p+WTEZRfjqa22woq2rEocpGvPrJabz6yWlMHJOIeSYj5uSlITFeLePoiYiIrg8XNAbJaFy0NhjORSDOB1Ba3oDte2quuluIpaULhyotOFTViLqmTggCkDc2CQUmI2bnpSFBGyPD6ImIiAJxt5AQYIDy41wE4nz4Xc9c1DV3oqzSgkOVjWho6YIoCDBlJ6NgigGz8tIQF9t/oSQREVEoMFyHAAOUH+ciEOfD70bmQpIkXGjs8LaOWNDU1g2FKCB/gh4FJgNmTkqDRs0ONyIiCh3e/pyIIpYgCBhnTMA4YwJW3ZqD2obLKKtsRFmVBX+osUKpqMa0HD0KTEbMmJjKW68TEZGsGK6JKGIIgoAJGTpMyNDhG7fl4kx9Ow5VWFBW3Ygjp5oRoxRx08RUFJgMmJaTghgVgzYREYUWwzURRSRREDAxKxETsxJxz6JJOHWxDYcqG/F5dSPKqhqhjlFg5qRUFEwxIn+CHiolt/UnIqKRx3BNRBFPFAXkjUtG3rhkrL19EqrOt6Gs0oLD1U04UG6BRq3ErMmpKDAZYRqfDKWCQZuIiEYGwzURRRWFKCI/W4/8bD3uXZKHitpWlFVa8MXJJuz/sgHxGhVmTU5DgcmAKeOSIYqC3EMmIqIownBNRFFLqRAxPTcF03NTcL/ThRNnWnCoqhEHKyz4+7F66LQqzJ5iwDyTERPHJEIUGLSJiGh4GK6JaFRQKRWYOTkNMyenocfhwpc1VhyqtGD/cTM+/aIOyQlqzMkzoMBkQE6mDgKDNhER3QCGayIaddQqBeZMMWDOFAO67U4cPd2MsspGfHrkIj78/AJSdLGYa/IE7fHGBAZtIiK6ZgzXRDSqxcYoUTg1HYVT09HV7cSRU00oq2rEh2UXsPPgeRiSNZg7xYACkxFj0uIYtImIaEgM10REXtpYJW6ZloFbpmWgw+bAFyebcKjSgvcOnMO7peeQkaJFgcmIApMBGSlxcg+XiIjCUMhuf3727Fls3LgRbW1tSEpKwqZNm5CdnR1wzr59+7B582acPHkS9913H/71X//1uq/D25/Lj3MRiPPhF6lz0d5px+HqRhyqbMTJC22QAIxJi0eBt3XEkKyVe4hERBRCQ93+PGTh+v7778fq1auxcuVK7NixA2+88Qa2bt0acM65c+fQ1dWFnTt3wm63M1xHKM5FIM6HXzTMRevlHs+NaiobcbruEgBgfHoCCkwGzJ1iQGqiRuYREhHRSBsqXIekLcRqtaKiogIvvfQSAGD58uV45pln0NLSAr1e7ztv/PjxAICPPvoIdrs9FEMjIrouyQlq3D5nLG6fMxbWS90oq2pEWZUF2z6twbZPa5CbqcNckxFzpxiQnKCWe7hERBRiIQnXZrMZRqMRCoUCAKBQKGAwGGA2mwPCNRFRJElJjMWyeeOwbN44NLZ2oazK0zryt49P4dWPT2HSmEQUTDVidp4BiXExcg+XiIhCIOoWNA5Wog+FtLQE2a4dbjgXgTgfftE6F2lpCcifbMS3VgAXGy9j79F67D1ah//ZdRJ//fAkpk1MxfwZWSialokvqizY+n4lmlttSE3W4P47TFgwe6zc3wIREQVBSMJ1RkYGLBYLXC4XFAoFXC4XGhsbkZGREfRrsedafpyLQJwPv9EyF2oBWDwzE4tnZuJiUwcOVTairNKC3247hi3bjgEC0LvapanVht+8dhTtl7tRlJ8u78CJiOiayN5znZKSApPJhJKSEqxcuRIlJSUwmUxsCSGiqDcmLR5j0uJx1/wJOG/pwKa/foFuuyvgHLvTja07q3G5046M1DhkpGih18XyduxERBEoZLuF1NTUYOPGjWhvb4dOp8OmTZuQk5OD9evXY8OGDZg2bRo+//xzPPHEE+jo6IAkSUhISMAvfvELzJ8//5qvw8q1/DgXgTgffpwLYN1/fHJN58WoRGTo45CRqkVGShwyUzwfDckaKBXiCI+SiIiGEhZb8YUKw7X8OBeBOB9+nAvgn5/fD2t7T7/jKTo1nvzWXJitXai3dsLc3AWztRNma2fA+QpRgCFZg4wUT4U7M8UbwPVxUMcoQvmtEBGNWrK3hRARkceq4lz85f0q2J1u37EYpYhVxblI0MYgQRuDyWOTAl7TbXeioaUL5mZv8LZ6gvex081w9SkmpOjU3tDtCdyZ3gCeoOVOJUREocJwTUQUQr2LFrfvqYG1vQcpOjVWFecOuZgxNkaJ7HQdstN1AcedLjcaW20wWztR7w3c5uYunLxYB7vDH97jNSpPW0lqXECLiV6nhsC+biKioGK4JiIKsaL89KDsDKJUiMhMjUNmahxm9znuliS0tHd7KtzN/uB9uLoJHbZ633lqlQLpKVpf2M5IiUNmqhZpSezrJiK6UQzXRERRRhQEpCZqkJqowbSclIDn2rvsMDd3+nu7rV2ovtCG0nKL75zevm5fP3dKHDJT4pCu17Kvm4joKhiuiYhGEZ02BrpxMcgblxxw3Nbj7ev2Bu765k7UNXfiyKkWv7I+AAAPDUlEQVRmuKW+fd2xAf3cnmp3HOI1qlB/K0REYYnhmoiIoFErMSFDhwkZ/fu6La02b7XbX/E+eb4tYFFmglYV0M/dG8CTE67e111a3nBdPehEROGM4ZqIiAalVIjISo1DVmpcwHG3JKHlUrd/IaV3UWVZVSM6u52+89QxCmTotb5+7t4tBA3JGihEEaXlDQG7p1jbe/CX96sAgAGbiCISwzUREV03URCQmqRBapIG03P9fd2SJOFyl8O/g4m34l11vhWl5Q2+8xSiAKNei+Y2W0AFHPDcsXL7nhqGayKKSAzXREQUNIIgQBcXA13cwH3dvXt0994op765c8CvY23vwa+2HUNaogapSbHeBZqxSEuKhTaW/d1EFL4YromIKCQ0aiVyMnXIyfT3dQ92x0qVUkTr5R6cutgGW48r4DmtWonUpFikJWqQkhiLtCRP8E71flSruKMJEcmH4ZqIiGQz2B0rH7hjCory0yFJEjq7nbBe6kZTmw3Nl7rRdMmG5rZu1Fs7cfyMFY4r2kp0WpUvaPuCt7cCnqKL5R7eRDSiGK6JiEg2V7tjpSAIiNeoEK9RYXx6Qr/XS5KE9k47mi51o9kbvpsv2dDU1o2z5nYcrm4KuEW8IADJCWpfm0nfAJ6WpEFSvBqiyLtWEtGNY7gmIiJZDeeOlYIgIDFejcR4NSZmJfZ73uV2o+2y3Re4ez9aL9lQea4VbZd7IPU5XyEKSNHFBvR597agpCZpoNOqeMt4IhoSwzUREUUthSgiJTEWKYmxyBvX/3mH042W9u6AdpPeAH70VBPauxwB58coxcA+70QN0pL8bSdxXGxJNOoxXBMR0ailUoow6rUw6rUDPt9jd3nCdkDbiefxqYuXYOtxBpyvUSuR1mdxZe9CyzRvEL/a7eN5Qx2iyMdwTURENAh1jAJZafHISosf8PnObkdAtbv5kieAN7R04cQZa789vBO0qsBqd5+2k9N1l/DfH1TzhjpEEY7hmoiI6AbFxaoQlz7EYssuB5rbbGi6ZPPueOIJ4LXmy/0WWw7E7nTj5Q9PQhCAeI0KcbEq30eNWsH+b6IwxHBNREQ0AgRBQGJcDBLjYpA7wGJLt1tCW0ePb4vBP75bOeDX6ep24sW3K/odFwUBcRrlFaFbiTiNCnEa/+cBz2uUUKsYyolGEsM1ERGRDERRgF4XC70uFnkA3tp7ZsAb6iQnqPGje2agw+ZAp83p+djt8H7uQEe3E502B1rau3G+0XNOj8PV/4JeSoXQJ2z3CeCDBnIV4jVKqJS8OQ/RtWC4JiIiCgOD3VDnGwtykZESd11fy+F0BwZwmxOd3b1h3POxN6g3tdlw1tyODpsTTpd70K8ZoxS9YdwTtv1h/IpQfkVQH85Ne7jAkyIRwzUREVEYuNoNda6HSikiKV6NpHj1db2ux+HyhnEHOr0V8SvDeG9or2/u9J0zVO94bIwioC2lN5DHaVSIH6SNRRurxKHKxoAfNrjAkyIFwzUREVGYGM4NdYJBrVJArVJAr4u95tdIkoRuuyeUd3YP0LbirZr3Hre293jPdUAaYj2nAODKp+1ON7burMKZ+nbEqESolQrEqBRQq0TvRwViVCJilAqoYxSIUfY/zjtw0khjuCYiIqIbJggCNGolNGolUq/jdW5JQneP01cl94dxz+c79p0d8HU9DjcOlDegx+Eeso1lMCql6AvdAcH8yiDe+1gpeo95jvseK0VvgA88rlKKEEdowSjbZCIDwzURERGFnCgI0MaqoB3krpb7jtcPuMAzRafG//veLQA8t7e3O9ywO93ocbhgd7g8nztcns+dfR47Ah/3OP3n9zhc6La7cKnTAbsz8PyrbZc4EF/1XNUnxCtFxMQovNX2ASrtV4b6K46fONuC13fXwME2mbDHcE1ERERhZ7AFnquKc32fK0QRGrUIzfW1ll8Xp6s3wLu84dztDeXex86hw3vf8zttTrQ6ewKO9zhcQ7bHDMXudOOPJRV4r/RcYFV9gEr8QBX3fu00fR4rxBtfiBoK4VzFZ7gmIiKisBPMBZ7DoVSIUCpEaEcoMkmSBJdbGjyg213ocboG3OscANwSkJ6i9VXg2zvt/q/VJ/hfL6VC6NfyMlBP+0AtM+oh+t77hvwb3W+9tLwhrBe7MlwTERFRWJJ7gWcoCIIApUKAUiEiboh1pG/srhm0TeaRu6YNeQ1Jkvq3yDhd6LF7Wmc8H/0tMgEVd2dgld3ucKOjy+Fru+l9rdM1nPaZ/pVzT7/7Fa013sdv7zsb8BsNwFPF376nJizeLwzXRERERGHuWtpkBiMIgm8nmIQRGp+v/71v5b03qF+t993uP8fXPtPe4w/wDs85V4vvA/3wIQeGayIiIqIwFy5tMoPx97+PXPuM0+VGj8ONJ/94CG0dA1fxwwHDNREREVEEGA1tMoMRBAEqpQIqpQJ333bjVfxQYLgmIiIioogR7lV8hmsiIiIiiijhXMUP700MiYiIiIgiCMM1EREREVGQMFwTEREREQVJyML12bNnsWbNGixduhRr1qxBbW1tv3NcLhd+9rOfYfHixbj99tuxbdu2UA2PiIiIiGjYQhauf/rTn2Lt2rX44IMPsHbtWjz55JP9znnnnXdw/vx57Nq1C6+++ip+85vf4OLFi6EaIhERERHRsIQkXFutVlRUVGD58uUAgOXLl6OiogItLS0B57333nu4++67IYoi9Ho9Fi9ejJ07d4ZiiEREREREwxaScG02m2E0GqFQKAAACoUCBoMBZrO533mZmZm+zzMyMtDQ0BCKIRIRERERDVvU7XOdkhIv27XT0hJku3a44VwE4nz4cS6IiCiahSRcZ2RkwGKxwOVyQaFQwOVyobGxERkZGf3Oq6+vx/Tp0wH0r2RfC6u1A263FLSxX6u0tAQ0NV0O+XXDEeciEOfDj3NBRETRQBSFQQu6IQnXKSkpMJlMKCkpwcqVK1FSUgKTyQS9Xh9w3rJly7Bt2zYsWbIEbW1t+Oijj/Dyyy9f17VEUQjm0CPm2uGGcxGI8+HHuSAiokg31L9lgiRJISnz1tTUYOPGjWhvb4dOp8OmTZuQk5OD9evXY8OGDZg2bRpcLheefvpp7N+/HwCwfv16rFmzJhTDIyIiIiIatpCFayIiIiKiaMc7NBIRERERBQnDNRERERFRkDBcExEREREFCcM1EREREVGQMFwTEREREQUJwzURERERUZAwXBMRERERBQnDNRERERFRkDBcD8OmTZuwcOFC5OXl4eTJk3IPR3atra1Yv349li5dijvvvBOPPvooWlpa5B6WbL73ve9hxYoV+PrXv461a9eisrJS7iHJ7re//S3/vBARUVRjuB6GRYsW4eWXX0ZWVpbcQwkLgiDgO9/5Dj744AO88847GDt2LH75y1/KPSzZbNq0CW+//TbeeustrFu3Dj/+8Y/lHpKsysvLcfToUf55ISKiqMZwPQxz5sxBRkaG3MMIG0lJSZg3b57v8xkzZqC+vl7GEckrISHB97ijowOCIMg4GnnZ7XY8/fTTeOqpp+QeChER0YhSyj0Aik5utxuvvPIKFi5cKPdQZPWTn/wE+/fvhyRJ+MMf/iD3cGTz3HPPYcWKFRgzZozcQyEiIhpRrFzTiHjmmWeg1Wpx7733yj0UWf3iF7/A7t278fjjj+PZZ5+VeziyOHLkCE6cOIG1a9fKPRQiIqIRx3BNQbdp0yacO3cOv/rVryCKfIsBwNe//nUcPHgQra2tcg8l5MrKylBTU4NFixZh4cKFaGhowLe//W3s27dP7qEREREFHdtCKKg2b96MEydO4MUXX0RMTIzcw5FNZ2cn2tvbfT35n3zyCRITE5GUlCTzyELvoYcewkMPPeT7fOHChXjhhRcwefJkGUdFREQ0Mhiuh+HnP/85du3ahebmZjz44INISkrCu+++K/ewZHPq1Cn8/ve/R3Z2Nu655x4AwJgxY7BlyxaZRxZ6NpsNjz32GGw2G0RRRGJiIl544YVRvaiRiIhoNBAkSZLkHgQRERERUTRgQywRERERUZAwXBMRERERBQnDNRERERFRkDBcExEREREFCcM1EREREVGQMFwTEdE1y8vLw7lz5+QeBhFR2OI+10REEWzhwoVobm6GQqHwHbvrrrvw5JNPyjgqIqLRi+GaiCjCvfDCC7j55pvlHgYREYFtIUREUWn79u2455578PTTT2P27NlYtmwZSktLfc9bLBY8/PDDKCgowO23347XXnvN95zL5cILL7yAxYsXY+bMmVi1ahXMZrPv+c8++wxLlizBnDlz8LOf/Qy8FxkRkR8r10REUer48eNYtmwZDhw4gA8//BCPPvooPv74YyQlJeGJJ57ApEmTsHfvXpw5cwYPPvggxo4di6KiIrz00kt499138eKLL2LChAmorq5GbGys7+vu3r0br7/+Ojo6OrBq1SrcdtttuPXWW2X8TomIwgcr10REEe6RRx7BnDlzfP/1VqH1ej0eeOABqFQq/MM//AMmTJiA3bt3w2w244svvsCPfvQjqNVqmEwm3H333dixYwcAYNu2bXjssceQk5MDQRAwZcoUJCcn+663fv166HQ6ZGZmYt68eaiqqpLl+yYiCkesXBMRRbgtW7b067nevn07jEYjBEHwHcvMzERjYyMaGxuRmJiI+Pj4gOdOnDgBAGhoaMC4ceMGvV5aWprvsUajQWdnZ7C+FSKiiMfKNRFRlLJYLAH90GazGQaDAQaDAZcuXUJHR0fAc0ajEQCQnp6O8+fPh3y8RETRgOGaiChKtbS0YOvWrXA4HHj//fdRU1OD4uJiZGRkYObMmdi8eTN6enpQVVWF119/HStWrAAA3H333XjuuedQW1sLSZJQVVWF1tZWmb8bIqLIwLYQIqII9/DDDwfsc33zzTdj0aJFmD59Os6dO4fCwkKkpqbi17/+ta93evPmzfjpT3+K+fPnQ6fT4fvf/76vteTBBx+E3W7HunXr0NraipycHGzZskWW742IKNIIEvdQIiKKOtu3b8e2bdvwyiuvyD0UIqJRhW0hRERERERBwnBNRERERBQkbAshIiIiIgoSVq6JiIiIiIKE4ZqIiIiIKEgYromIiIiIgoThmoiIiIgoSBiuiYiIiIiChOGaiIiIiChI/j9Hxv356Ds1EAAAAABJRU5ErkJggg==\n"},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","\n","sns.set(style='darkgrid')\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"]},{"cell_type":"markdown","id":"3c2dfa30","metadata":{"papermill":{"duration":0.032699,"end_time":"2022-10-17T06:39:59.257750","exception":false,"start_time":"2022-10-17T06:39:59.225051","status":"completed"},"tags":[],"id":"3c2dfa30"},"source":["# Performance of Test Set"]},{"cell_type":"markdown","id":"61cb9d5d","metadata":{"papermill":{"duration":0.031879,"end_time":"2022-10-17T06:39:59.321252","exception":false,"start_time":"2022-10-17T06:39:59.289373","status":"completed"},"tags":[],"id":"61cb9d5d"},"source":["## Data Preparation\n","#### text data set를 준비하기 위해 train data에 대해 했던 것과 동일한 단계를 모두 적용해야 한다."]},{"cell_type":"code","execution_count":null,"id":"44f9028e","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:39:59.401031Z","iopub.status.busy":"2022-10-17T06:39:59.399813Z","iopub.status.idle":"2022-10-17T06:39:59.430431Z","shell.execute_reply":"2022-10-17T06:39:59.429411Z"},"papermill":{"duration":0.079749,"end_time":"2022-10-17T06:39:59.433185","exception":false,"start_time":"2022-10-17T06:39:59.353436","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"44f9028e","executionInfo":{"status":"ok","timestamp":1666694918892,"user_tz":-540,"elapsed":5,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"0937caf6-b2e3-4d3e-ed25-6ab9ffb8305b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["        text_id                                          full_text  grammar\n","0  0000C359D63E  when a person has no experience on a job their...      3.0\n","1  000BAD50D026  Do you think students would benefit from being...      3.0\n","2  00367BB2546B  Thomas Jefferson once states that \"it is wonde...      3.0"],"text/html":["\n","  <div id=\"df-29b694e7-8495-415b-b8b3-4090586ab6bb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_id</th>\n","      <th>full_text</th>\n","      <th>grammar</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000C359D63E</td>\n","      <td>when a person has no experience on a job their...</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000BAD50D026</td>\n","      <td>Do you think students would benefit from being...</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00367BB2546B</td>\n","      <td>Thomas Jefferson once states that \"it is wonde...</td>\n","      <td>3.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29b694e7-8495-415b-b8b3-4090586ab6bb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-29b694e7-8495-415b-b8b3-4090586ab6bb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-29b694e7-8495-415b-b8b3-4090586ab6bb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":24}],"source":["df_INNER_JOIN = pd.merge(test, submission, left_on='text_id', right_on='text_id', how='inner')\n","df_INNER_JOIN.drop(['cohesion','syntax','vocabulary','phraseology','conventions'], inplace=True, axis=1)\n","labels = df_INNER_JOIN\n","labels"]},{"cell_type":"code","execution_count":null,"id":"e9226dc8","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:39:59.538171Z","iopub.status.busy":"2022-10-17T06:39:59.537737Z","iopub.status.idle":"2022-10-17T06:39:59.618009Z","shell.execute_reply":"2022-10-17T06:39:59.617084Z"},"papermill":{"duration":0.132707,"end_time":"2022-10-17T06:39:59.620669","exception":false,"start_time":"2022-10-17T06:39:59.487962","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"e9226dc8","executionInfo":{"status":"ok","timestamp":1666694918893,"user_tz":-540,"elapsed":5,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"ec479052-1b70-486c-83de-d44b1c7cf2fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of test sentences: 3\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["import pandas as pd\n","#test = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')\n","\n","#문장 수\n","print('Number of test sentences: {:,}\\n'.format(labels.shape[0]))\n","\n","#리스트 만들기\n","full_text = labels.full_text.values\n","labels = labels.grammar.values\n","\n","#모든 문장을 토큰화하고 토큰을 해당 단어 IDs에 매핑한다.\n","input_ids=[]\n","attention_masks = []\n","\n","for text in full_text:\n","    encoded_dict = tokenizer.encode_plus(\n","                    text,#문장을 encode\n","                    add_special_tokens = True, #[CLS][SEP]토큰 추가\n","                    max_length = 256, #모든 문장 자르고 채우기 \n","                    truncation=True,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,#Attention mask 만들기\n","                    return_tensors = 'pt' #파이토치 텐서로 리턴\n","                    )\n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","batch_size = 32\n","\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","id":"11fb41e5","metadata":{"papermill":{"duration":0.047884,"end_time":"2022-10-17T06:39:59.718882","exception":false,"start_time":"2022-10-17T06:39:59.670998","status":"completed"},"tags":[],"id":"11fb41e5"},"source":["# Evaluate on Test Set\n","\n","### 테스트 세트가 준비되면 fine-tuning된 모델을 적용하여 테스트 세트에 대한 예측을 생성할 수 있음"]},{"cell_type":"code","execution_count":null,"id":"815afe1c","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:39:59.816926Z","iopub.status.busy":"2022-10-17T06:39:59.816453Z","iopub.status.idle":"2022-10-17T06:39:59.874091Z","shell.execute_reply":"2022-10-17T06:39:59.872609Z"},"papermill":{"duration":0.110139,"end_time":"2022-10-17T06:39:59.876901","exception":false,"start_time":"2022-10-17T06:39:59.766762","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"815afe1c","executionInfo":{"status":"ok","timestamp":1666694918893,"user_tz":-540,"elapsed":4,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"315b088b-cf50-49cf-c235-347518070552"},"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction labels for 3 test sentences...\n","    완료. \n","[array([[2.6528852],\n","       [2.2039738],\n","       [3.833632 ]], dtype=float32)]\n","[array([3., 3., 3.])]\n"]}],"source":["# 테스트 셋 예측\n","print('Prediction labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# put model in evaluation mode\n","model.eval()\n","\n","#Tracking Variables\n","predictions, true_labels = [], []\n","\n","#predict\n","for batch in prediction_dataloader:\n","    #GPU에 Batch 넣기\n","    batch= tuple(t.to(device) for t in batch)\n","    \n","    #DataLoader에서 input 압축풀기\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    #모델에 기울기를 계산하거나 저장하지 않도록 지시하여 메모리를 절약하고\n","    #예측속도향상\n","    with torch.no_grad():\n","        #Forward pass, calculate logit predictions\n","        outputs = model(b_input_ids, token_type_ids=None,\n","                       attention_mask=b_input_mask)\n","    logits = outputs[0]\n","    \n","    #Logits, labels를 cpu로 옮기기\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    #Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","    \n","print('    완료. ')\n","print(predictions)\n","print(true_labels)"]},{"cell_type":"code","execution_count":null,"id":"7b7017c1","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:39:59.944581Z","iopub.status.busy":"2022-10-17T06:39:59.944237Z","iopub.status.idle":"2022-10-17T06:39:59.949864Z","shell.execute_reply":"2022-10-17T06:39:59.948985Z"},"papermill":{"duration":0.041281,"end_time":"2022-10-17T06:39:59.951930","exception":false,"start_time":"2022-10-17T06:39:59.910649","status":"completed"},"tags":[],"id":"7b7017c1"},"outputs":[],"source":["grammar_submission = predictions[0]"]},{"cell_type":"code","execution_count":null,"id":"ed3a7579","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:00.018879Z","iopub.status.busy":"2022-10-17T06:40:00.018081Z","iopub.status.idle":"2022-10-17T06:40:00.024325Z","shell.execute_reply":"2022-10-17T06:40:00.023386Z"},"papermill":{"duration":0.042169,"end_time":"2022-10-17T06:40:00.026396","exception":false,"start_time":"2022-10-17T06:39:59.984227","status":"completed"},"tags":[],"id":"ed3a7579"},"outputs":[],"source":["def MCRMSE(y_trues, y_preds):\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = y_trues[:,i]\n","        y_pred = y_preds[:,i]\n","        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores"]},{"cell_type":"markdown","id":"89695fe2","metadata":{"papermill":{"duration":0.031923,"end_time":"2022-10-17T06:40:00.091106","exception":false,"start_time":"2022-10-17T06:40:00.059183","status":"completed"},"tags":[],"id":"89695fe2"},"source":["# 2. Cohesion"]},{"cell_type":"code","execution_count":null,"id":"62213ffc","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:00.156602Z","iopub.status.busy":"2022-10-17T06:40:00.156240Z","iopub.status.idle":"2022-10-17T06:40:00.240043Z","shell.execute_reply":"2022-10-17T06:40:00.239085Z"},"papermill":{"duration":0.119382,"end_time":"2022-10-17T06:40:00.242472","exception":false,"start_time":"2022-10-17T06:40:00.123090","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":353},"id":"62213ffc","executionInfo":{"status":"error","timestamp":1666694919447,"user_tz":-540,"elapsed":557,"user":{"displayName":"Bini Kim","userId":"16545463548428762859"}},"outputId":"292f8cf3-93f0-4cf4-f566-4c13c893bf60"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-247bd8a855c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/feedback-prize-english-language-learning/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/feedback-prize-english-language-learning/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/feedback-prize-english-language-learning/train.csv'"]}],"source":["train = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/train.csv')\n","test = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')"]},{"cell_type":"code","execution_count":null,"id":"2007e206","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:00.309860Z","iopub.status.busy":"2022-10-17T06:40:00.308259Z","iopub.status.idle":"2022-10-17T06:40:00.315457Z","shell.execute_reply":"2022-10-17T06:40:00.314619Z"},"papermill":{"duration":0.042545,"end_time":"2022-10-17T06:40:00.317680","exception":false,"start_time":"2022-10-17T06:40:00.275135","status":"completed"},"tags":[],"id":"2007e206"},"outputs":[],"source":["#cohesion data만 사용하기 위해서 나머지 평가항목 컬럼은 드롭\n","train.drop(['grammar','syntax','vocabulary','phraseology','conventions'], inplace=True, axis=1)"]},{"cell_type":"code","execution_count":null,"id":"f6c91873","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:00.384486Z","iopub.status.busy":"2022-10-17T06:40:00.384128Z","iopub.status.idle":"2022-10-17T06:40:00.396846Z","shell.execute_reply":"2022-10-17T06:40:00.395767Z"},"papermill":{"duration":0.05154,"end_time":"2022-10-17T06:40:00.402264","exception":false,"start_time":"2022-10-17T06:40:00.350724","status":"completed"},"tags":[],"id":"f6c91873"},"outputs":[],"source":["print('Number of trainig sentences: {:,}\\n'.format(train.shape[0]))\n","train.sample(10)"]},{"cell_type":"code","execution_count":null,"id":"61293ee4","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:00.469689Z","iopub.status.busy":"2022-10-17T06:40:00.469358Z","iopub.status.idle":"2022-10-17T06:40:00.474312Z","shell.execute_reply":"2022-10-17T06:40:00.473337Z"},"papermill":{"duration":0.040249,"end_time":"2022-10-17T06:40:00.476294","exception":false,"start_time":"2022-10-17T06:40:00.436045","status":"completed"},"tags":[],"id":"61293ee4"},"outputs":[],"source":["# Get the lists of sentences and their labels.\n","full_text = train.full_text.values\n","labels = train.cohesion.values"]},{"cell_type":"code","execution_count":null,"id":"4795b8d5","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:00.545003Z","iopub.status.busy":"2022-10-17T06:40:00.543266Z","iopub.status.idle":"2022-10-17T06:40:02.247077Z","shell.execute_reply":"2022-10-17T06:40:02.246077Z"},"papermill":{"duration":1.740521,"end_time":"2022-10-17T06:40:02.249547","exception":false,"start_time":"2022-10-17T06:40:00.509026","status":"completed"},"tags":[],"id":"4795b8d5"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","print(\"Loading Bert Tokenizer...\")\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"id":"e7a3f2d7","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:02.317037Z","iopub.status.busy":"2022-10-17T06:40:02.316144Z","iopub.status.idle":"2022-10-17T06:40:46.689656Z","shell.execute_reply":"2022-10-17T06:40:46.688586Z"},"papermill":{"duration":44.443127,"end_time":"2022-10-17T06:40:46.725905","exception":false,"start_time":"2022-10-17T06:40:02.282778","status":"completed"},"tags":[],"id":"e7a3f2d7"},"outputs":[],"source":["# 모든 문장을 토큰화하고 토큰을 해당 단어 ID에 매핑\n","input_ids = []\n","attention_masks = []\n","\n","for text in full_text:\n","    encoded_dict = tokenizer.encode_plus(\n","                    text,#문장을 encode\n","                    add_special_tokens = True, #[CLS][SEP]토큰 추가\n","                    max_length = 256, #모든 문장 자르고 채우기 \n","                    # 'max_length'의 숫자는 어떻게 정하는 거지?\n","                    truncation=True,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,#Attention mask 만들기\n","                    return_tensors = 'pt' #파이토치 텐서로 리턴\n","                    )\n","    # 인코딩된 문장을 목록에 추가\n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # Attention mask\n","    attention_masks.append(encoded_dict['attention_mask'])\n","    \n","# 목록을 텐서로 변환\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# full_text 첫번째 문장을 IDs의 리스트로 출력해보기\n","print('Original: ', full_text[0])\n","print('Token IDs: ', input_ids[0])"]},{"cell_type":"code","execution_count":null,"id":"c3f4c9ee","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:46.800689Z","iopub.status.busy":"2022-10-17T06:40:46.799934Z","iopub.status.idle":"2022-10-17T06:40:46.808219Z","shell.execute_reply":"2022-10-17T06:40:46.806743Z"},"papermill":{"duration":0.052454,"end_time":"2022-10-17T06:40:46.810766","exception":false,"start_time":"2022-10-17T06:40:46.758312","status":"completed"},"tags":[],"id":"c3f4c9ee"},"outputs":[],"source":["from torch.utils.data import TensorDataset, random_split\n","# training inputs을 TensorDataset에 결합\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","#90-10 train-validation split.\n","#각 세트에 포함할 샘플 수를 계산\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","#무작위로 샘플을 선택하여 데이터 세트를 나눈다.\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"]},{"cell_type":"code","execution_count":null,"id":"55a9659e","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:46.878504Z","iopub.status.busy":"2022-10-17T06:40:46.877519Z","iopub.status.idle":"2022-10-17T06:40:46.885576Z","shell.execute_reply":"2022-10-17T06:40:46.884701Z"},"papermill":{"duration":0.044086,"end_time":"2022-10-17T06:40:46.887598","exception":false,"start_time":"2022-10-17T06:40:46.843512","status":"completed"},"tags":[],"id":"55a9659e"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","#DataLoader는 훈련을 위한 배치크기를 알아야 하므로 지정함.\n","#Bert를 미세조정하기 위해 배치 사이즈 16/32 권장\n","batch_size = 16\n","\n","#training, validation 세트에 대한 DataLoader를 생성\n","#무작위 순서로 학습 샘플을 가져옴.\n","train_dataloader = DataLoader(\n","            train_dataset, # training sample\n","            sampler = RandomSampler(train_dataset), #배치를 랜덤으로 선택\n","            batch_size = batch_size #훈련을 이 배치 사이즈로 함.\n","        )\n","\n","#validation에서 순서가 중요하지 않아서 순서대로 진행함\n","valid_dataloader = DataLoader(\n","                val_dataset,\n","                sampler = SequentialSampler(val_dataset), # 배치를 순차적으로 꺼냄.\n","                batch_size = batch_size #이 배치 사이즈로 평가하기\n","        )"]},{"cell_type":"code","execution_count":null,"id":"6dceab87","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-10-17T06:40:46.955224Z","iopub.status.busy":"2022-10-17T06:40:46.954204Z","iopub.status.idle":"2022-10-17T06:40:49.252840Z","shell.execute_reply":"2022-10-17T06:40:49.251859Z"},"jupyter":{"outputs_hidden":true},"papermill":{"duration":2.334554,"end_time":"2022-10-17T06:40:49.254892","exception":false,"start_time":"2022-10-17T06:40:46.920338","status":"completed"},"tags":[],"id":"6dceab87"},"outputs":[],"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","        \"bert-base-uncased\", # 12레이어 버트 모델 사용 (uncased)\n","        num_labels = 1, #출력 레이블 수 (2진 분류의 경우 2)\n","        # grammar 점수가 9개라 9개 적음\n","        # 다중 클래스 작업의 경우 이 값을 늘릴 수 있다.\n","        output_attentions = False, #모델이 attention 가중치를 리턴하는지\n","        output_hidden_states = False, #모델이 hidden states를 리턴하는지\n","        #attention_probs_dropout_prob=0.4,\n","        #hidden_dropout_prob=0.4,\n",")\n","# GPU에서 이 모델을 실행하도록 pytorch에 지시\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"id":"8aee714a","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-10-17T06:40:49.322776Z","iopub.status.busy":"2022-10-17T06:40:49.322470Z","iopub.status.idle":"2022-10-17T06:40:49.330941Z","shell.execute_reply":"2022-10-17T06:40:49.329978Z"},"jupyter":{"outputs_hidden":true},"papermill":{"duration":0.045359,"end_time":"2022-10-17T06:40:49.333814","exception":false,"start_time":"2022-10-17T06:40:49.288455","status":"completed"},"tags":[],"id":"8aee714a"},"outputs":[],"source":["# 모델의 모든 parameters를 튜플 목록으로 가져옴.\n","params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55}{:12}\".format(p[0], str(tuple(p[1].size()))))\n","    \n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:55}{:12}\".format(p[0], str(tuple(p[1].size()))))\n","    \n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:55}{:12}\".format(p[0], str(tuple(p[1].size()))))"]},{"cell_type":"code","execution_count":null,"id":"4a31ca5c","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:49.403599Z","iopub.status.busy":"2022-10-17T06:40:49.402950Z","iopub.status.idle":"2022-10-17T06:40:49.411684Z","shell.execute_reply":"2022-10-17T06:40:49.410458Z"},"papermill":{"duration":0.04526,"end_time":"2022-10-17T06:40:49.413743","exception":false,"start_time":"2022-10-17T06:40:49.368483","status":"completed"},"tags":[],"id":"4a31ca5c"},"outputs":[],"source":["# 참고 : AdamW는 hugging face 라이브러리의 클래스임(pytorch와 반대)\n","# W = Weight Dacay Fix?\n","optimizer = AdamW(model.parameters(),\n","                 lr = 2e-5, #arg.learning_rate - 기본값은 5e-5이고 노트북에는 2e-5가 있다.\n","                 eps = 1e-8) #arg.adam_epsilon - 기본값은 1e-8"]},{"cell_type":"code","execution_count":null,"id":"01f8e8c4","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:49.482128Z","iopub.status.busy":"2022-10-17T06:40:49.481224Z","iopub.status.idle":"2022-10-17T06:40:49.487589Z","shell.execute_reply":"2022-10-17T06:40:49.486729Z"},"papermill":{"duration":0.042797,"end_time":"2022-10-17T06:40:49.489692","exception":false,"start_time":"2022-10-17T06:40:49.446895","status":"completed"},"tags":[],"id":"01f8e8c4"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 10\n","# 총 훈련 단계 수는 [배치 수] X [에포크 수]\n","# (훈련 샘플의 수와 동일하지 않음)\n","total_steps = len(train_dataloader) * epochs\n","\n","# learning rate scheduler 만들기\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                           num_warmup_steps = 0,\n","                                           num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"id":"d3f9aa2c","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:49.559002Z","iopub.status.busy":"2022-10-17T06:40:49.558647Z","iopub.status.idle":"2022-10-17T06:40:49.564423Z","shell.execute_reply":"2022-10-17T06:40:49.563482Z"},"papermill":{"duration":0.042664,"end_time":"2022-10-17T06:40:49.566422","exception":false,"start_time":"2022-10-17T06:40:49.523758","status":"completed"},"tags":[],"id":"d3f9aa2c"},"outputs":[],"source":["import numpy as np\n","\n","# 예측 vs 레이블의 정확도를 계산하는 함수\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":null,"id":"04ed9f37","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:49.636341Z","iopub.status.busy":"2022-10-17T06:40:49.634601Z","iopub.status.idle":"2022-10-17T06:40:49.640485Z","shell.execute_reply":"2022-10-17T06:40:49.639577Z"},"papermill":{"duration":0.042567,"end_time":"2022-10-17T06:40:49.642459","exception":false,"start_time":"2022-10-17T06:40:49.599892","status":"completed"},"tags":[],"id":"04ed9f37"},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    #가까운 초로 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    #format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"id":"6d2d6fed","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:40:49.710377Z","iopub.status.busy":"2022-10-17T06:40:49.710096Z","iopub.status.idle":"2022-10-17T06:55:43.929288Z","shell.execute_reply":"2022-10-17T06:55:43.928176Z"},"papermill":{"duration":894.256453,"end_time":"2022-10-17T06:55:43.932028","exception":false,"start_time":"2022-10-17T06:40:49.675575","status":"completed"},"tags":[],"id":"6d2d6fed"},"outputs":[],"source":["import random\n","import numpy as np\n","\n","#모든 곳에 seed 값을 설정하기\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# training , validation loss 같은 많은 수량을 저장\n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# 전체 실행에 대한 총 훈련시간을 측정함\n","total_t0 = time.time()\n","\n","#for each epoch...\n","for epoch_i in range(0, epochs):\n","    print(\"\")\n","    print('=======Epoch {:} / {:} ======='.format(epoch_i+1, epochs))\n","    print(\"Training...\")\n","    \n","    # training epoch가 걸리는 시간을 측정\n","    t0 =time.time()\n","    \n","    # epoch의 전체 loss를 재설정\n","    total_train_loss = 0\n","    \n","    # 모델을 학습 모드로 전환 \n","    # train은 *모드*를 변경할 뿐 훈련을 *수행*하지 않음.\n","    # 'dropoout' / 'batchnorm' 레이어는 훈련 중에 다르게 작동함\n","    model.train()\n","    \n","    # 훈련 데이터의 각 batch에 대해\n","    for step, batch in enumerate(train_dataloader):\n","        #40개 batch마다 진행률이 업데이트됨\n","        if step % 40 == 0 and not step == 0:\n","            #경과시간을 분 단위로 계산함\n","            elapsed = format_time(time.time()-t0)\n","            #진행상황 보고\n","            print(' Batch {:>5,} of {:>5,}. Elapsed: {:}'.format(step, len(train_dataloader), elapsed))\n","    \n","        #DataLoader에서 훈련 batch의 압축을 푼다\n","        #Batch의 압축을 풀면서 각 텐서를 사용하여 GPU에 복사함\n","        # 'to' method\n","\n","        #'batch'에는 세 개의 pytorch 텐서가 포함되어 있음.\n","        # input ids\n","        # attention masks\n","        # labels\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device, dtype=torch.float32)\n","\n","        # 모델 훈련을 수행하기 이전에 계산된 gradient는 지워야 함\n","        # backward pass : pytorch는 이 작업을 자동으로 수행하지 않는다.\n","        # gradient를 누적하는 것은 RNN을 훈련하는 동안 편리함\n","        model.zero_grad()\n","    \n","        # forward pass 수행 (정방향 전달)\n","        # 어떤 arguments에 따라 다른 수의 parameter를 return\n","        # arguments가 주어지고 flags가 설정됨\n","        # loss (label를 주었기 때문에) logits model\n","        # activation 전 출력\n","        loss, logits = model(b_input_ids,\n","                            token_type_ids=None,\n","                            attention_mask=b_input_mask,\n","                            labels = b_labels,\n","                            return_dict=False)\n","    \n","        #모든 Batch에 대한 훈련 loss를 누적하여 다음을 수행할 수 있다.\n","        # 마지막에 평균 loss를 계산함 loss는 다음을 포함하는 텐서이다.\n","        # single value; .item() function은 python 값을 return\n","        #print(loss.item(), type(loss.item()))\n","        total_train_loss += loss.item()\n","\n","        # backward pass로 gradients를 계산함\n","        loss.backward()\n","    \n","        # gradient 표준을 1.0으로 자름\n","        # \"exploding gradients\"문제에도 도움이 됨\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters and take a step using the computed gradient\n","        # Optimizer는 \"업데이트 rule\"(parameter가 어떻게 학습률과 기울기에 따라 수정되는지)지시\n","        optimizer.step()\n","        \n","        #update the learnin rate.\n","        scheduler.step()\n","    \n","    # 모든 batch에 대한 평균 손실을 계산함\n","    avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","    # 이 에포크가 얼마나 걸렸는지 측정\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","    \n","    # ======validation======\n","    #각 training epoch를 완료 후 성능을 측정함\n","    # VALIDATION SET\n","    \n","    print(\"\")\n","    print(\"Running Validation...\")\n","    \n","    t0 = time.time()\n","    \n","    # 모델을 평가 모드에. dropout layer가 다르게 작동함\n","    model.eval()\n","    \n","    #Tracking variables\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    \n","    #Evaluate data for one epoch\n","    for batch in valid_dataloader:\n","        #DataLoader에서 training batch 압축 해제\n","        #batch의 압축을 풀면서 각 텐서를 GPU에 복사\n","        #\"to\"method.\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device, dtype=torch.float32)\n","        \n","        #pytorch에게 compute 그래프를 구성하는동안 귀찮게 하지 말라고 하기\n","        #forward pass ; backprop (역전파) training에만 필요\n","        with torch.no_grad():\n","            \n","            #forward pass : logit predictions를 계산\n","            #token_type_ids는 \"segment ids\"와 동일\n","            # 2문장 작업에서 문장 1과 2를 구별\n","            \n","            #모델의 logits의 출력을 가져옴. \n","            #softmax 함수를 적용하기 전 값\n","            (loss, logits) = model(b_input_ids,\n","                                  token_type_ids=None,\n","                                  attention_mask=b_input_mask,\n","                                  labels=b_labels,\n","                                  return_dict=False)\n","        #validation loss를 누적\n","        total_eval_loss += loss.item()\n","        \n","        #logits, labels 는 CPU로 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # test 문장 batch 정확도를 계산하고 모든 batch에 누적함\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","    # validation 실행에 대한 최종 정확도를 보고함.\n","    avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","    \n","    #모든 배치에 대한 평균 loss를 계산\n","    avg_val_loss = total_eval_loss / len(valid_dataloader)\n","    \n","    #validation실행에 걸린 시간을 측정\n","    validation_time = format_time(time.time()-t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    \n","    # epoch의 모든 통계를 기록\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i+1,\n","            'Training Loss' : avg_train_loss,\n","            'Valid. Loss' : avg_val_loss,\n","            'Valid. Accur.' : avg_val_accuracy,\n","            'Training Time' : training_time,\n","            'Validation Time' : validation_time\n","        }\n","    )\n","    \n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"code","execution_count":null,"id":"036ff91b","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:55:44.008939Z","iopub.status.busy":"2022-10-17T06:55:44.007997Z","iopub.status.idle":"2022-10-17T06:55:44.022458Z","shell.execute_reply":"2022-10-17T06:55:44.021417Z"},"papermill":{"duration":0.054722,"end_time":"2022-10-17T06:55:44.024547","exception":false,"start_time":"2022-10-17T06:55:43.969825","status":"completed"},"tags":[],"id":"036ff91b"},"outputs":[],"source":["import pandas as pd\n","df_stats = pd.DataFrame(data=training_stats)\n","df_stats = df_stats.set_index('epoch')\n","df_stats"]},{"cell_type":"code","execution_count":null,"id":"483770fb","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:55:44.099816Z","iopub.status.busy":"2022-10-17T06:55:44.099142Z","iopub.status.idle":"2022-10-17T06:55:44.349330Z","shell.execute_reply":"2022-10-17T06:55:44.348422Z"},"papermill":{"duration":0.289962,"end_time":"2022-10-17T06:55:44.351434","exception":false,"start_time":"2022-10-17T06:55:44.061472","status":"completed"},"tags":[],"id":"483770fb"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","\n","sns.set(style='darkgrid')\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"a0564e1e","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:55:44.429721Z","iopub.status.busy":"2022-10-17T06:55:44.428182Z","iopub.status.idle":"2022-10-17T06:55:44.444968Z","shell.execute_reply":"2022-10-17T06:55:44.444100Z"},"papermill":{"duration":0.057814,"end_time":"2022-10-17T06:55:44.447167","exception":false,"start_time":"2022-10-17T06:55:44.389353","status":"completed"},"tags":[],"id":"a0564e1e"},"outputs":[],"source":["df_INNER_JOIN = pd.merge(test, submission, left_on='text_id', right_on='text_id', how='inner')\n","df_INNER_JOIN.drop(['grammar','syntax','vocabulary','phraseology','conventions'], inplace=True, axis=1)\n","labels = df_INNER_JOIN\n","labels"]},{"cell_type":"code","execution_count":null,"id":"876d1dc3","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:55:44.524648Z","iopub.status.busy":"2022-10-17T06:55:44.523127Z","iopub.status.idle":"2022-10-17T06:55:44.577593Z","shell.execute_reply":"2022-10-17T06:55:44.576462Z"},"papermill":{"duration":0.095441,"end_time":"2022-10-17T06:55:44.580162","exception":false,"start_time":"2022-10-17T06:55:44.484721","status":"completed"},"tags":[],"id":"876d1dc3"},"outputs":[],"source":["import pandas as pd\n","#test = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')\n","\n","#문장 수\n","print('Number of test sentences: {:,}\\n'.format(labels.shape[0]))\n","\n","#리스트 만들기\n","full_text = labels.full_text.values\n","labels = labels.cohesion.values\n","\n","#모든 문장을 토큰화하고 토큰을 해당 단어 IDs에 매핑한다.\n","input_ids=[]\n","attention_masks = []\n","\n","for text in full_text:\n","    encoded_dict = tokenizer.encode_plus(\n","                    text,#문장을 encode\n","                    add_special_tokens = True, #[CLS][SEP]토큰 추가\n","                    max_length = 256, #모든 문장 자르고 채우기 \n","                    truncation=True,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,#Attention mask 만들기\n","                    return_tensors = 'pt' #파이토치 텐서로 리턴\n","                    )\n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","batch_size = 32\n","\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"id":"f0dc93f4","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:55:44.657598Z","iopub.status.busy":"2022-10-17T06:55:44.657294Z","iopub.status.idle":"2022-10-17T06:55:44.696584Z","shell.execute_reply":"2022-10-17T06:55:44.695211Z"},"papermill":{"duration":0.080161,"end_time":"2022-10-17T06:55:44.698563","exception":false,"start_time":"2022-10-17T06:55:44.618402","status":"completed"},"tags":[],"id":"f0dc93f4"},"outputs":[],"source":["# 테스트 셋 예측\n","print('Prediction labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# put model in evaluation mode\n","model.eval()\n","\n","#Tracking Variables\n","predictions, true_labels = [], []\n","\n","#predict\n","for batch in prediction_dataloader:\n","    #GPU에 Batch 넣기\n","    batch= tuple(t.to(device) for t in batch)\n","    \n","    #DataLoader에서 input 압축풀기\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    #모델에 기울기를 계산하거나 저장하지 않도록 지시하여 메모리를 절약하고\n","    #예측속도향상\n","    with torch.no_grad():\n","        #Forward pass, calculate logit predictions\n","        outputs = model(b_input_ids, token_type_ids=None,\n","                       attention_mask=b_input_mask)\n","    logits = outputs[0]\n","    \n","    #Logits, labels를 cpu로 옮기기\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    #Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","    \n","print('    완료. ')\n","print(predictions)\n","print(true_labels)"]},{"cell_type":"code","execution_count":null,"id":"6f70a939","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:55:44.776094Z","iopub.status.busy":"2022-10-17T06:55:44.775164Z","iopub.status.idle":"2022-10-17T06:55:44.779993Z","shell.execute_reply":"2022-10-17T06:55:44.779156Z"},"papermill":{"duration":0.045198,"end_time":"2022-10-17T06:55:44.781762","exception":false,"start_time":"2022-10-17T06:55:44.736564","status":"completed"},"tags":[],"id":"6f70a939"},"outputs":[],"source":["cohesion_submission = predictions[0]"]},{"cell_type":"markdown","id":"ae613093","metadata":{"papermill":{"duration":0.064316,"end_time":"2022-10-17T06:55:44.885176","exception":false,"start_time":"2022-10-17T06:55:44.820860","status":"completed"},"tags":[],"id":"ae613093"},"source":["# 3. Syntax"]},{"cell_type":"code","execution_count":null,"id":"9efc6e51","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:55:44.961514Z","iopub.status.busy":"2022-10-17T06:55:44.961148Z","iopub.status.idle":"2022-10-17T06:55:45.060034Z","shell.execute_reply":"2022-10-17T06:55:45.058809Z"},"papermill":{"duration":0.139842,"end_time":"2022-10-17T06:55:45.062621","exception":false,"start_time":"2022-10-17T06:55:44.922779","status":"completed"},"tags":[],"id":"9efc6e51"},"outputs":[],"source":["train = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/train.csv')\n","test = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')\n","#cohesion data만 사용하기 위해서 나머지 평가항목 컬럼은 드롭\n","train.drop(['cohesion','grammar','vocabulary','phraseology','conventions'], inplace=True, axis=1)\n","print('Number of trainig sentences: {:,}\\n'.format(train.shape[0]))\n","train.sample(10)"]},{"cell_type":"code","execution_count":null,"id":"e8a1ad04","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:55:45.187163Z","iopub.status.busy":"2022-10-17T06:55:45.186220Z","iopub.status.idle":"2022-10-17T06:55:46.883916Z","shell.execute_reply":"2022-10-17T06:55:46.882957Z"},"papermill":{"duration":1.78324,"end_time":"2022-10-17T06:55:46.886318","exception":false,"start_time":"2022-10-17T06:55:45.103078","status":"completed"},"tags":[],"id":"e8a1ad04"},"outputs":[],"source":["# Get the lists of sentences and their labels.\n","full_text = train.full_text.values\n","labels = train.syntax.values\n","\n","from transformers import BertTokenizer\n","\n","print(\"Loading Bert Tokenizer...\")\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"id":"ea050f51","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:55:46.966084Z","iopub.status.busy":"2022-10-17T06:55:46.965162Z","iopub.status.idle":"2022-10-17T06:56:31.327798Z","shell.execute_reply":"2022-10-17T06:56:31.326828Z"},"papermill":{"duration":44.44418,"end_time":"2022-10-17T06:56:31.369775","exception":false,"start_time":"2022-10-17T06:55:46.925595","status":"completed"},"tags":[],"id":"ea050f51"},"outputs":[],"source":["# 모든 문장을 토큰화하고 토큰을 해당 단어 ID에 매핑\n","input_ids = []\n","attention_masks = []\n","\n","for text in full_text:\n","    encoded_dict = tokenizer.encode_plus(\n","                    text,#문장을 encode\n","                    add_special_tokens = True, #[CLS][SEP]토큰 추가\n","                    max_length = 256, #모든 문장 자르고 채우기 \n","                    # 'max_length'의 숫자는 어떻게 정하는 거지?\n","                    truncation=True,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,#Attention mask 만들기\n","                    return_tensors = 'pt' #파이토치 텐서로 리턴\n","                    )\n","    # 인코딩된 문장을 목록에 추가\n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # Attention mask\n","    attention_masks.append(encoded_dict['attention_mask'])\n","    \n","# 목록을 텐서로 변환\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# full_text 첫번째 문장을 IDs의 리스트로 출력해보기\n","print('Original: ', full_text[0])\n","print('Token IDs: ', input_ids[0])"]},{"cell_type":"code","execution_count":null,"id":"5abff4bd","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:56:31.449170Z","iopub.status.busy":"2022-10-17T06:56:31.448249Z","iopub.status.idle":"2022-10-17T06:56:31.455641Z","shell.execute_reply":"2022-10-17T06:56:31.454647Z"},"papermill":{"duration":0.049038,"end_time":"2022-10-17T06:56:31.458218","exception":false,"start_time":"2022-10-17T06:56:31.409180","status":"completed"},"tags":[],"id":"5abff4bd"},"outputs":[],"source":["from torch.utils.data import TensorDataset, random_split\n","# training inputs을 TensorDataset에 결합\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","#90-10 train-validation split.\n","#각 세트에 포함할 샘플 수를 계산\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","#무작위로 샘플을 선택하여 데이터 세트를 나눈다.\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"]},{"cell_type":"code","execution_count":null,"id":"f4c6c193","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-10-17T06:56:31.536708Z","iopub.status.busy":"2022-10-17T06:56:31.536386Z","iopub.status.idle":"2022-10-17T06:56:33.624697Z","shell.execute_reply":"2022-10-17T06:56:33.623817Z"},"jupyter":{"outputs_hidden":true},"papermill":{"duration":2.129933,"end_time":"2022-10-17T06:56:33.626800","exception":false,"start_time":"2022-10-17T06:56:31.496867","status":"completed"},"tags":[],"id":"f4c6c193"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 16\n","\n","train_dataloader = DataLoader(\n","            train_dataset, # training sample\n","            sampler = RandomSampler(train_dataset), #배치를 랜덤으로 선택\n","            batch_size = batch_size #훈련을 이 배치 사이즈로 함.\n","        )\n","\n","valid_dataloader = DataLoader(\n","                val_dataset,\n","                sampler = SequentialSampler(val_dataset), # 배치를 순차적으로 꺼냄.\n","                batch_size = batch_size #이 배치 사이즈로 평가하기\n","        )\n","\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","        \"bert-base-uncased\", # 12레이어 버트 모델 사용 (uncased)\n","        num_labels = 1, \n","        output_attentions = False, #모델이 attention 가중치를 리턴하는지\n","        output_hidden_states = False, #모델이 hidden states를 리턴하는지\n","        #attention_probs_dropout_prob=0.4,\n","        #hidden_dropout_prob=0.4,\n",")\n","\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"id":"5fa21cd7","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:56:33.708020Z","iopub.status.busy":"2022-10-17T06:56:33.706986Z","iopub.status.idle":"2022-10-17T06:56:33.716315Z","shell.execute_reply":"2022-10-17T06:56:33.714756Z"},"papermill":{"duration":0.051939,"end_time":"2022-10-17T06:56:33.718849","exception":false,"start_time":"2022-10-17T06:56:33.666910","status":"completed"},"tags":[],"id":"5fa21cd7"},"outputs":[],"source":["optimizer = AdamW(model.parameters(),\n","                 lr = 2e-5, #arg.learning_rate - 기본값은 5e-5이고 노트북에는 2e-5가 있다.\n","                 eps = 1e-8) #arg.adam_epsilon - 기본값은 1e-8"]},{"cell_type":"code","execution_count":null,"id":"b03b5d9f","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:56:33.799737Z","iopub.status.busy":"2022-10-17T06:56:33.798859Z","iopub.status.idle":"2022-10-17T06:56:33.804886Z","shell.execute_reply":"2022-10-17T06:56:33.803835Z"},"papermill":{"duration":0.048475,"end_time":"2022-10-17T06:56:33.807091","exception":false,"start_time":"2022-10-17T06:56:33.758616","status":"completed"},"tags":[],"id":"b03b5d9f"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 10\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                           num_warmup_steps = 0,\n","                                           num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"id":"7444b93e","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:56:33.887258Z","iopub.status.busy":"2022-10-17T06:56:33.886406Z","iopub.status.idle":"2022-10-17T06:56:33.892541Z","shell.execute_reply":"2022-10-17T06:56:33.891710Z"},"papermill":{"duration":0.048836,"end_time":"2022-10-17T06:56:33.894471","exception":false,"start_time":"2022-10-17T06:56:33.845635","status":"completed"},"tags":[],"id":"7444b93e"},"outputs":[],"source":["import numpy as np\n","\n","# 예측 vs 레이블의 정확도를 계산하는 함수\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":null,"id":"bdcf6cb8","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:56:33.974969Z","iopub.status.busy":"2022-10-17T06:56:33.974187Z","iopub.status.idle":"2022-10-17T06:56:33.979498Z","shell.execute_reply":"2022-10-17T06:56:33.978658Z"},"papermill":{"duration":0.04812,"end_time":"2022-10-17T06:56:33.981480","exception":false,"start_time":"2022-10-17T06:56:33.933360","status":"completed"},"tags":[],"id":"bdcf6cb8"},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    #가까운 초로 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    #format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"id":"588b8635","metadata":{"execution":{"iopub.execute_input":"2022-10-17T06:56:34.060710Z","iopub.status.busy":"2022-10-17T06:56:34.060403Z","iopub.status.idle":"2022-10-17T07:11:28.356091Z","shell.execute_reply":"2022-10-17T07:11:28.354999Z"},"papermill":{"duration":894.339018,"end_time":"2022-10-17T07:11:28.358860","exception":false,"start_time":"2022-10-17T06:56:34.019842","status":"completed"},"tags":[],"id":"588b8635"},"outputs":[],"source":["import random\n","import numpy as np\n","\n","#모든 곳에 seed 값을 설정하기\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# training , validation loss 같은 많은 수량을 저장\n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# 전체 실행에 대한 총 훈련시간을 측정함\n","total_t0 = time.time()\n","\n","#for each epoch...\n","for epoch_i in range(0, epochs):\n","    print(\"\")\n","    print('=======Epoch {:} / {:} ======='.format(epoch_i+1, epochs))\n","    print(\"Training...\")\n","    \n","    # training epoch가 걸리는 시간을 측정\n","    t0 =time.time()\n","    \n","    # epoch의 전체 loss를 재설정\n","    total_train_loss = 0\n","    \n","    # 모델을 학습 모드로 전환 \n","    # train은 *모드*를 변경할 뿐 훈련을 *수행*하지 않음.\n","    # 'dropoout' / 'batchnorm' 레이어는 훈련 중에 다르게 작동함\n","    model.train()\n","    \n","    # 훈련 데이터의 각 batch에 대해\n","    for step, batch in enumerate(train_dataloader):\n","        #40개 batch마다 진행률이 업데이트됨\n","        if step % 40 == 0 and not step == 0:\n","            #경과시간을 분 단위로 계산함\n","            elapsed = format_time(time.time()-t0)\n","            #진행상황 보고\n","            print(' Batch {:>5,} of {:>5,}. Elapsed: {:}'.format(step, len(train_dataloader), elapsed))\n","    \n","        #DataLoader에서 훈련 batch의 압축을 푼다\n","        #Batch의 압축을 풀면서 각 텐서를 사용하여 GPU에 복사함\n","        # 'to' method\n","\n","        #'batch'에는 세 개의 pytorch 텐서가 포함되어 있음.\n","        # input ids\n","        # attention masks\n","        # labels\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device, dtype=torch.float32)\n","\n","        # 모델 훈련을 수행하기 이전에 계산된 gradient는 지워야 함\n","        # backward pass : pytorch는 이 작업을 자동으로 수행하지 않는다.\n","        # gradient를 누적하는 것은 RNN을 훈련하는 동안 편리함\n","        model.zero_grad()\n","    \n","        # forward pass 수행 (정방향 전달)\n","        # 어떤 arguments에 따라 다른 수의 parameter를 return\n","        # arguments가 주어지고 flags가 설정됨\n","        # loss (label를 주었기 때문에) logits model\n","        # activation 전 출력\n","        loss, logits = model(b_input_ids,\n","                            token_type_ids=None,\n","                            attention_mask=b_input_mask,\n","                            labels = b_labels,\n","                            return_dict=False)\n","    \n","        #모든 Batch에 대한 훈련 loss를 누적하여 다음을 수행할 수 있다.\n","        # 마지막에 평균 loss를 계산함 loss는 다음을 포함하는 텐서이다.\n","        # single value; .item() function은 python 값을 return\n","        #print(loss.item(), type(loss.item()))\n","        total_train_loss += loss.item()\n","\n","        # backward pass로 gradients를 계산함\n","        loss.backward()\n","    \n","        # gradient 표준을 1.0으로 자름\n","        # \"exploding gradients\"문제에도 도움이 됨\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters and take a step using the computed gradient\n","        # Optimizer는 \"업데이트 rule\"(parameter가 어떻게 학습률과 기울기에 따라 수정되는지)지시\n","        optimizer.step()\n","        \n","        #update the learnin rate.\n","        scheduler.step()\n","    \n","    # 모든 batch에 대한 평균 손실을 계산함\n","    avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","    # 이 에포크가 얼마나 걸렸는지 측정\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","    \n","    # ======validation======\n","    #각 training epoch를 완료 후 성능을 측정함\n","    # VALIDATION SET\n","    \n","    print(\"\")\n","    print(\"Running Validation...\")\n","    \n","    t0 = time.time()\n","    \n","    # 모델을 평가 모드에. dropout layer가 다르게 작동함\n","    model.eval()\n","    \n","    #Tracking variables\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    \n","    #Evaluate data for one epoch\n","    for batch in valid_dataloader:\n","        #DataLoader에서 training batch 압축 해제\n","        #batch의 압축을 풀면서 각 텐서를 GPU에 복사\n","        #\"to\"method.\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device, dtype=torch.float32)\n","        \n","        #pytorch에게 compute 그래프를 구성하는동안 귀찮게 하지 말라고 하기\n","        #forward pass ; backprop (역전파) training에만 필요\n","        with torch.no_grad():\n","            \n","            #forward pass : logit predictions를 계산\n","            #token_type_ids는 \"segment ids\"와 동일\n","            # 2문장 작업에서 문장 1과 2를 구별\n","            \n","            #모델의 logits의 출력을 가져옴. \n","            #softmax 함수를 적용하기 전 값\n","            (loss, logits) = model(b_input_ids,\n","                                  token_type_ids=None,\n","                                  attention_mask=b_input_mask,\n","                                  labels=b_labels,\n","                                  return_dict=False)\n","        #validation loss를 누적\n","        total_eval_loss += loss.item()\n","        \n","        #logits, labels 는 CPU로 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # test 문장 batch 정확도를 계산하고 모든 batch에 누적함\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","    # validation 실행에 대한 최종 정확도를 보고함.\n","    avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","    \n","    #모든 배치에 대한 평균 loss를 계산\n","    avg_val_loss = total_eval_loss / len(valid_dataloader)\n","    \n","    #validation실행에 걸린 시간을 측정\n","    validation_time = format_time(time.time()-t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    \n","    # epoch의 모든 통계를 기록\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i+1,\n","            'Training Loss' : avg_train_loss,\n","            'Valid. Loss' : avg_val_loss,\n","            'Valid. Accur.' : avg_val_accuracy,\n","            'Training Time' : training_time,\n","            'Validation Time' : validation_time\n","        }\n","    )\n","    \n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"code","execution_count":null,"id":"5c76b0c7","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:11:28.446622Z","iopub.status.busy":"2022-10-17T07:11:28.446280Z","iopub.status.idle":"2022-10-17T07:11:28.462081Z","shell.execute_reply":"2022-10-17T07:11:28.461030Z"},"papermill":{"duration":0.061269,"end_time":"2022-10-17T07:11:28.464165","exception":false,"start_time":"2022-10-17T07:11:28.402896","status":"completed"},"tags":[],"id":"5c76b0c7"},"outputs":[],"source":["import pandas as pd\n","df_stats = pd.DataFrame(data=training_stats)\n","df_stats = df_stats.set_index('epoch')\n","df_stats"]},{"cell_type":"code","execution_count":null,"id":"89597bf5","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:11:28.550695Z","iopub.status.busy":"2022-10-17T07:11:28.550367Z","iopub.status.idle":"2022-10-17T07:11:28.779855Z","shell.execute_reply":"2022-10-17T07:11:28.778836Z"},"papermill":{"duration":0.275215,"end_time":"2022-10-17T07:11:28.782173","exception":false,"start_time":"2022-10-17T07:11:28.506958","status":"completed"},"tags":[],"id":"89597bf5"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","\n","sns.set(style='darkgrid')\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"9f54c047","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:11:28.875610Z","iopub.status.busy":"2022-10-17T07:11:28.875258Z","iopub.status.idle":"2022-10-17T07:11:28.889850Z","shell.execute_reply":"2022-10-17T07:11:28.888846Z"},"papermill":{"duration":0.061819,"end_time":"2022-10-17T07:11:28.891922","exception":false,"start_time":"2022-10-17T07:11:28.830103","status":"completed"},"tags":[],"id":"9f54c047"},"outputs":[],"source":["df_INNER_JOIN = pd.merge(test, submission, left_on='text_id', right_on='text_id', how='inner')\n","df_INNER_JOIN.drop(['grammar','cohesion','vocabulary','phraseology','conventions'], inplace=True, axis=1)\n","labels = df_INNER_JOIN\n","labels"]},{"cell_type":"code","execution_count":null,"id":"2c397eda","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:11:28.981457Z","iopub.status.busy":"2022-10-17T07:11:28.980456Z","iopub.status.idle":"2022-10-17T07:11:29.031391Z","shell.execute_reply":"2022-10-17T07:11:29.030324Z"},"papermill":{"duration":0.099434,"end_time":"2022-10-17T07:11:29.035844","exception":false,"start_time":"2022-10-17T07:11:28.936410","status":"completed"},"tags":[],"id":"2c397eda"},"outputs":[],"source":["import pandas as pd\n","#test = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')\n","\n","#문장 수\n","print('Number of test sentences: {:,}\\n'.format(labels.shape[0]))\n","\n","#리스트 만들기\n","full_text = labels.full_text.values\n","labels = labels.syntax.values\n","\n","#모든 문장을 토큰화하고 토큰을 해당 단어 IDs에 매핑한다.\n","input_ids=[]\n","attention_masks = []\n","\n","for text in full_text:\n","    encoded_dict = tokenizer.encode_plus(\n","                    text,#문장을 encode\n","                    add_special_tokens = True, #[CLS][SEP]토큰 추가\n","                    max_length = 256, #모든 문장 자르고 채우기 \n","                    truncation=True,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,#Attention mask 만들기\n","                    return_tensors = 'pt' #파이토치 텐서로 리턴\n","                    )\n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","batch_size = 32\n","\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"id":"8ab88ce7","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:11:29.125694Z","iopub.status.busy":"2022-10-17T07:11:29.124730Z","iopub.status.idle":"2022-10-17T07:11:29.163579Z","shell.execute_reply":"2022-10-17T07:11:29.162314Z"},"papermill":{"duration":0.085414,"end_time":"2022-10-17T07:11:29.165695","exception":false,"start_time":"2022-10-17T07:11:29.080281","status":"completed"},"tags":[],"id":"8ab88ce7"},"outputs":[],"source":["# 테스트 셋 예측\n","print('Prediction labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# put model in evaluation mode\n","model.eval()\n","\n","#Tracking Variables\n","predictions, true_labels = [], []\n","\n","#predict\n","for batch in prediction_dataloader:\n","    #GPU에 Batch 넣기\n","    batch= tuple(t.to(device) for t in batch)\n","    \n","    #DataLoader에서 input 압축풀기\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    #모델에 기울기를 계산하거나 저장하지 않도록 지시하여 메모리를 절약하고\n","    #예측속도향상\n","    with torch.no_grad():\n","        #Forward pass, calculate logit predictions\n","        outputs = model(b_input_ids, token_type_ids=None,\n","                       attention_mask=b_input_mask)\n","    logits = outputs[0]\n","    \n","    #Logits, labels를 cpu로 옮기기\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    #Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","    \n","print('    완료. ')\n","print(predictions)\n","print(true_labels)"]},{"cell_type":"code","execution_count":null,"id":"28441337","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:11:29.253430Z","iopub.status.busy":"2022-10-17T07:11:29.252838Z","iopub.status.idle":"2022-10-17T07:11:29.257117Z","shell.execute_reply":"2022-10-17T07:11:29.256164Z"},"papermill":{"duration":0.050256,"end_time":"2022-10-17T07:11:29.259133","exception":false,"start_time":"2022-10-17T07:11:29.208877","status":"completed"},"tags":[],"id":"28441337"},"outputs":[],"source":["syntax_submission = predictions[0]"]},{"cell_type":"markdown","id":"fade1d09","metadata":{"papermill":{"duration":0.043769,"end_time":"2022-10-17T07:11:29.346277","exception":false,"start_time":"2022-10-17T07:11:29.302508","status":"completed"},"tags":[],"id":"fade1d09"},"source":["# 4. Vocabulary"]},{"cell_type":"code","execution_count":null,"id":"4e5f5e32","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:11:29.436531Z","iopub.status.busy":"2022-10-17T07:11:29.435631Z","iopub.status.idle":"2022-10-17T07:11:29.535021Z","shell.execute_reply":"2022-10-17T07:11:29.534035Z"},"papermill":{"duration":0.146168,"end_time":"2022-10-17T07:11:29.537101","exception":false,"start_time":"2022-10-17T07:11:29.390933","status":"completed"},"tags":[],"id":"4e5f5e32"},"outputs":[],"source":["train = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/train.csv')\n","test = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')\n","#cohesion data만 사용하기 위해서 나머지 평가항목 컬럼은 드롭\n","train.drop(['cohesion','grammar','syntax','phraseology','conventions'], inplace=True, axis=1)\n","print('Number of trainig sentences: {:,}\\n'.format(train.shape[0]))\n","train.sample(10)"]},{"cell_type":"code","execution_count":null,"id":"0b2b208d","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:11:29.628442Z","iopub.status.busy":"2022-10-17T07:11:29.627583Z","iopub.status.idle":"2022-10-17T07:11:31.346155Z","shell.execute_reply":"2022-10-17T07:11:31.345197Z"},"papermill":{"duration":1.766088,"end_time":"2022-10-17T07:11:31.349175","exception":false,"start_time":"2022-10-17T07:11:29.583087","status":"completed"},"tags":[],"id":"0b2b208d"},"outputs":[],"source":["# Get the lists of sentences and their labels.\n","full_text = train.full_text.values\n","labels = train.vocabulary.values\n","\n","from transformers import BertTokenizer\n","\n","print(\"Loading Bert Tokenizer...\")\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"id":"707bf75c","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-10-17T07:11:31.630472Z","iopub.status.busy":"2022-10-17T07:11:31.629978Z","iopub.status.idle":"2022-10-17T07:12:15.661754Z","shell.execute_reply":"2022-10-17T07:12:15.660838Z"},"jupyter":{"outputs_hidden":true},"papermill":{"duration":44.253536,"end_time":"2022-10-17T07:12:15.720515","exception":false,"start_time":"2022-10-17T07:11:31.466979","status":"completed"},"tags":[],"id":"707bf75c"},"outputs":[],"source":["# 모든 문장을 토큰화하고 토큰을 해당 단어 ID에 매핑\n","input_ids = []\n","attention_masks = []\n","\n","for text in full_text:\n","    encoded_dict = tokenizer.encode_plus(\n","                    text,#문장을 encode\n","                    add_special_tokens = True, #[CLS][SEP]토큰 추가\n","                    max_length = 256, #모든 문장 자르고 채우기 \n","                    # 'max_length'의 숫자는 어떻게 정하는 거지?\n","                    truncation=True,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,#Attention mask 만들기\n","                    return_tensors = 'pt' #파이토치 텐서로 리턴\n","                    )\n","    # 인코딩된 문장을 목록에 추가\n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # Attention mask\n","    attention_masks.append(encoded_dict['attention_mask'])\n","    \n","# 목록을 텐서로 변환\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# full_text 첫번째 문장을 IDs의 리스트로 출력해보기\n","print('Original: ', full_text[0])\n","print('Token IDs: ', input_ids[0])"]},{"cell_type":"code","execution_count":null,"id":"4b1de5d9","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:12:15.850137Z","iopub.status.busy":"2022-10-17T07:12:15.849770Z","iopub.status.idle":"2022-10-17T07:12:15.858122Z","shell.execute_reply":"2022-10-17T07:12:15.856821Z"},"papermill":{"duration":0.073292,"end_time":"2022-10-17T07:12:15.860556","exception":false,"start_time":"2022-10-17T07:12:15.787264","status":"completed"},"tags":[],"id":"4b1de5d9"},"outputs":[],"source":["from torch.utils.data import TensorDataset, random_split\n","# training inputs을 TensorDataset에 결합\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","#90-10 train-validation split.\n","#각 세트에 포함할 샘플 수를 계산\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","#무작위로 샘플을 선택하여 데이터 세트를 나눈다.\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"]},{"cell_type":"code","execution_count":null,"id":"0116adf4","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-10-17T07:12:15.952208Z","iopub.status.busy":"2022-10-17T07:12:15.951858Z","iopub.status.idle":"2022-10-17T07:12:18.089423Z","shell.execute_reply":"2022-10-17T07:12:18.088264Z"},"jupyter":{"outputs_hidden":true},"papermill":{"duration":2.186258,"end_time":"2022-10-17T07:12:18.092577","exception":false,"start_time":"2022-10-17T07:12:15.906319","status":"completed"},"tags":[],"id":"0116adf4"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 16\n","\n","train_dataloader = DataLoader(\n","            train_dataset, # training sample\n","            sampler = RandomSampler(train_dataset), #배치를 랜덤으로 선택\n","            batch_size = batch_size #훈련을 이 배치 사이즈로 함.\n","        )\n","\n","valid_dataloader = DataLoader(\n","                val_dataset,\n","                sampler = SequentialSampler(val_dataset), # 배치를 순차적으로 꺼냄.\n","                batch_size = batch_size #이 배치 사이즈로 평가하기\n","        )\n","\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","        \"bert-base-uncased\", # 12레이어 버트 모델 사용 (uncased)\n","        num_labels = 1, \n","        output_attentions = False, #모델이 attention 가중치를 리턴하는지\n","        output_hidden_states = False, #모델이 hidden states를 리턴하는지\n","        #attention_probs_dropout_prob=0.4,\n","        #hidden_dropout_prob=0.4,\n",")\n","\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"id":"095b72eb","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:12:18.187854Z","iopub.status.busy":"2022-10-17T07:12:18.186901Z","iopub.status.idle":"2022-10-17T07:12:18.196116Z","shell.execute_reply":"2022-10-17T07:12:18.194841Z"},"papermill":{"duration":0.059902,"end_time":"2022-10-17T07:12:18.198034","exception":false,"start_time":"2022-10-17T07:12:18.138132","status":"completed"},"tags":[],"id":"095b72eb"},"outputs":[],"source":["optimizer = AdamW(model.parameters(),\n","                 lr = 2e-5, #arg.learning_rate - 기본값은 5e-5이고 노트북에는 2e-5가 있다.\n","                 eps = 1e-8) #arg.adam_epsilon - 기본값은 1e-8"]},{"cell_type":"code","execution_count":null,"id":"99e48701","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:12:18.298324Z","iopub.status.busy":"2022-10-17T07:12:18.297924Z","iopub.status.idle":"2022-10-17T07:12:18.304723Z","shell.execute_reply":"2022-10-17T07:12:18.303624Z"},"papermill":{"duration":0.063393,"end_time":"2022-10-17T07:12:18.306854","exception":false,"start_time":"2022-10-17T07:12:18.243461","status":"completed"},"tags":[],"id":"99e48701"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 10\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                           num_warmup_steps = 0,\n","                                           num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"id":"e5f14b4a","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:12:18.401992Z","iopub.status.busy":"2022-10-17T07:12:18.400939Z","iopub.status.idle":"2022-10-17T07:12:18.407645Z","shell.execute_reply":"2022-10-17T07:12:18.406828Z"},"papermill":{"duration":0.054422,"end_time":"2022-10-17T07:12:18.409639","exception":false,"start_time":"2022-10-17T07:12:18.355217","status":"completed"},"tags":[],"id":"e5f14b4a"},"outputs":[],"source":["import numpy as np\n","\n","# 예측 vs 레이블의 정확도를 계산하는 함수\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    #가까운 초로 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    #format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"id":"55f2f1fb","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:12:18.500584Z","iopub.status.busy":"2022-10-17T07:12:18.500240Z","iopub.status.idle":"2022-10-17T07:27:13.202583Z","shell.execute_reply":"2022-10-17T07:27:13.201124Z"},"papermill":{"duration":894.750692,"end_time":"2022-10-17T07:27:13.205114","exception":false,"start_time":"2022-10-17T07:12:18.454422","status":"completed"},"tags":[],"id":"55f2f1fb"},"outputs":[],"source":["import random\n","import numpy as np\n","\n","#모든 곳에 seed 값을 설정하기\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# training , validation loss 같은 많은 수량을 저장\n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# 전체 실행에 대한 총 훈련시간을 측정함\n","total_t0 = time.time()\n","\n","#for each epoch...\n","for epoch_i in range(0, epochs):\n","    print(\"\")\n","    print('=======Epoch {:} / {:} ======='.format(epoch_i+1, epochs))\n","    print(\"Training...\")\n","    \n","    # training epoch가 걸리는 시간을 측정\n","    t0 =time.time()\n","    \n","    # epoch의 전체 loss를 재설정\n","    total_train_loss = 0\n","    \n","    # 모델을 학습 모드로 전환 \n","    # train은 *모드*를 변경할 뿐 훈련을 *수행*하지 않음.\n","    # 'dropoout' / 'batchnorm' 레이어는 훈련 중에 다르게 작동함\n","    model.train()\n","    \n","    # 훈련 데이터의 각 batch에 대해\n","    for step, batch in enumerate(train_dataloader):\n","        #40개 batch마다 진행률이 업데이트됨\n","        if step % 40 == 0 and not step == 0:\n","            #경과시간을 분 단위로 계산함\n","            elapsed = format_time(time.time()-t0)\n","            #진행상황 보고\n","            print(' Batch {:>5,} of {:>5,}. Elapsed: {:}'.format(step, len(train_dataloader), elapsed))\n","    \n","        #DataLoader에서 훈련 batch의 압축을 푼다\n","        #Batch의 압축을 풀면서 각 텐서를 사용하여 GPU에 복사함\n","        # 'to' method\n","\n","        #'batch'에는 세 개의 pytorch 텐서가 포함되어 있음.\n","        # input ids\n","        # attention masks\n","        # labels\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device, dtype=torch.float32)\n","\n","        # 모델 훈련을 수행하기 이전에 계산된 gradient는 지워야 함\n","        # backward pass : pytorch는 이 작업을 자동으로 수행하지 않는다.\n","        # gradient를 누적하는 것은 RNN을 훈련하는 동안 편리함\n","        model.zero_grad()\n","    \n","        # forward pass 수행 (정방향 전달)\n","        # 어떤 arguments에 따라 다른 수의 parameter를 return\n","        # arguments가 주어지고 flags가 설정됨\n","        # loss (label를 주었기 때문에) logits model\n","        # activation 전 출력\n","        loss, logits = model(b_input_ids,\n","                            token_type_ids=None,\n","                            attention_mask=b_input_mask,\n","                            labels = b_labels,\n","                            return_dict=False)\n","    \n","        #모든 Batch에 대한 훈련 loss를 누적하여 다음을 수행할 수 있다.\n","        # 마지막에 평균 loss를 계산함 loss는 다음을 포함하는 텐서이다.\n","        # single value; .item() function은 python 값을 return\n","        #print(loss.item(), type(loss.item()))\n","        total_train_loss += loss.item()\n","\n","        # backward pass로 gradients를 계산함\n","        loss.backward()\n","    \n","        # gradient 표준을 1.0으로 자름\n","        # \"exploding gradients\"문제에도 도움이 됨\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters and take a step using the computed gradient\n","        # Optimizer는 \"업데이트 rule\"(parameter가 어떻게 학습률과 기울기에 따라 수정되는지)지시\n","        optimizer.step()\n","        \n","        #update the learnin rate.\n","        scheduler.step()\n","    \n","    # 모든 batch에 대한 평균 손실을 계산함\n","    avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","    # 이 에포크가 얼마나 걸렸는지 측정\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","    \n","    # ======validation======\n","    #각 training epoch를 완료 후 성능을 측정함\n","    # VALIDATION SET\n","    \n","    print(\"\")\n","    print(\"Running Validation...\")\n","    \n","    t0 = time.time()\n","    \n","    # 모델을 평가 모드에. dropout layer가 다르게 작동함\n","    model.eval()\n","    \n","    #Tracking variables\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    \n","    #Evaluate data for one epoch\n","    for batch in valid_dataloader:\n","        #DataLoader에서 training batch 압축 해제\n","        #batch의 압축을 풀면서 각 텐서를 GPU에 복사\n","        #\"to\"method.\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device, dtype=torch.float32)\n","        \n","        #pytorch에게 compute 그래프를 구성하는동안 귀찮게 하지 말라고 하기\n","        #forward pass ; backprop (역전파) training에만 필요\n","        with torch.no_grad():\n","            \n","            #forward pass : logit predictions를 계산\n","            #token_type_ids는 \"segment ids\"와 동일\n","            # 2문장 작업에서 문장 1과 2를 구별\n","            \n","            #모델의 logits의 출력을 가져옴. \n","            #softmax 함수를 적용하기 전 값\n","            (loss, logits) = model(b_input_ids,\n","                                  token_type_ids=None,\n","                                  attention_mask=b_input_mask,\n","                                  labels=b_labels,\n","                                  return_dict=False)\n","        #validation loss를 누적\n","        total_eval_loss += loss.item()\n","        \n","        #logits, labels 는 CPU로 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # test 문장 batch 정확도를 계산하고 모든 batch에 누적함\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","    # validation 실행에 대한 최종 정확도를 보고함.\n","    avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","    \n","    #모든 배치에 대한 평균 loss를 계산\n","    avg_val_loss = total_eval_loss / len(valid_dataloader)\n","    \n","    #validation실행에 걸린 시간을 측정\n","    validation_time = format_time(time.time()-t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    \n","    # epoch의 모든 통계를 기록\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i+1,\n","            'Training Loss' : avg_train_loss,\n","            'Valid. Loss' : avg_val_loss,\n","            'Valid. Accur.' : avg_val_accuracy,\n","            'Training Time' : training_time,\n","            'Validation Time' : validation_time\n","        }\n","    )\n","    \n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"code","execution_count":null,"id":"37906360","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:27:13.305371Z","iopub.status.busy":"2022-10-17T07:27:13.305013Z","iopub.status.idle":"2022-10-17T07:27:13.319768Z","shell.execute_reply":"2022-10-17T07:27:13.318853Z"},"papermill":{"duration":0.066883,"end_time":"2022-10-17T07:27:13.321964","exception":false,"start_time":"2022-10-17T07:27:13.255081","status":"completed"},"tags":[],"id":"37906360"},"outputs":[],"source":["import pandas as pd\n","df_stats = pd.DataFrame(data=training_stats)\n","df_stats = df_stats.set_index('epoch')\n","df_stats"]},{"cell_type":"code","execution_count":null,"id":"b7fe879b","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:27:13.422051Z","iopub.status.busy":"2022-10-17T07:27:13.420983Z","iopub.status.idle":"2022-10-17T07:27:13.651701Z","shell.execute_reply":"2022-10-17T07:27:13.650712Z"},"papermill":{"duration":0.282526,"end_time":"2022-10-17T07:27:13.653876","exception":false,"start_time":"2022-10-17T07:27:13.371350","status":"completed"},"tags":[],"id":"b7fe879b"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","\n","sns.set(style='darkgrid')\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"a999526c","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:27:13.754553Z","iopub.status.busy":"2022-10-17T07:27:13.753004Z","iopub.status.idle":"2022-10-17T07:27:13.768692Z","shell.execute_reply":"2022-10-17T07:27:13.767763Z"},"papermill":{"duration":0.067483,"end_time":"2022-10-17T07:27:13.770781","exception":false,"start_time":"2022-10-17T07:27:13.703298","status":"completed"},"tags":[],"id":"a999526c"},"outputs":[],"source":["df_INNER_JOIN = pd.merge(test, submission, left_on='text_id', right_on='text_id', how='inner')\n","df_INNER_JOIN.drop(['grammar','cohesion','syntax','phraseology','conventions'], inplace=True, axis=1)\n","labels = df_INNER_JOIN\n","labels"]},{"cell_type":"code","execution_count":null,"id":"9b2ff3d1","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:27:13.870512Z","iopub.status.busy":"2022-10-17T07:27:13.869635Z","iopub.status.idle":"2022-10-17T07:27:13.922046Z","shell.execute_reply":"2022-10-17T07:27:13.920696Z"},"papermill":{"duration":0.104901,"end_time":"2022-10-17T07:27:13.924936","exception":false,"start_time":"2022-10-17T07:27:13.820035","status":"completed"},"tags":[],"id":"9b2ff3d1"},"outputs":[],"source":["import pandas as pd\n","#test = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')\n","\n","#문장 수\n","print('Number of test sentences: {:,}\\n'.format(labels.shape[0]))\n","\n","#리스트 만들기\n","full_text = labels.full_text.values\n","labels = labels.vocabulary.values\n","\n","#모든 문장을 토큰화하고 토큰을 해당 단어 IDs에 매핑한다.\n","input_ids=[]\n","attention_masks = []\n","\n","for text in full_text:\n","    encoded_dict = tokenizer.encode_plus(\n","                    text,#문장을 encode\n","                    add_special_tokens = True, #[CLS][SEP]토큰 추가\n","                    max_length = 256, #모든 문장 자르고 채우기 \n","                    truncation=True,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,#Attention mask 만들기\n","                    return_tensors = 'pt' #파이토치 텐서로 리턴\n","                    )\n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","batch_size = 32\n","\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"id":"ac6ac024","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:27:14.035614Z","iopub.status.busy":"2022-10-17T07:27:14.035249Z","iopub.status.idle":"2022-10-17T07:27:14.074262Z","shell.execute_reply":"2022-10-17T07:27:14.073058Z"},"papermill":{"duration":0.092262,"end_time":"2022-10-17T07:27:14.076668","exception":false,"start_time":"2022-10-17T07:27:13.984406","status":"completed"},"tags":[],"id":"ac6ac024"},"outputs":[],"source":["# 테스트 셋 예측\n","print('Prediction labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# put model in evaluation mode\n","model.eval()\n","\n","#Tracking Variables\n","predictions, true_labels = [], []\n","\n","#predict\n","for batch in prediction_dataloader:\n","    #GPU에 Batch 넣기\n","    batch= tuple(t.to(device) for t in batch)\n","    \n","    #DataLoader에서 input 압축풀기\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    #모델에 기울기를 계산하거나 저장하지 않도록 지시하여 메모리를 절약하고\n","    #예측속도향상\n","    with torch.no_grad():\n","        #Forward pass, calculate logit predictions\n","        outputs = model(b_input_ids, token_type_ids=None,\n","                       attention_mask=b_input_mask)\n","    logits = outputs[0]\n","    \n","    #Logits, labels를 cpu로 옮기기\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    #Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","    \n","print('    완료. ')\n","print(predictions)\n","print(true_labels)"]},{"cell_type":"code","execution_count":null,"id":"ff5c6454","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:27:14.245657Z","iopub.status.busy":"2022-10-17T07:27:14.245238Z","iopub.status.idle":"2022-10-17T07:27:14.249824Z","shell.execute_reply":"2022-10-17T07:27:14.248967Z"},"papermill":{"duration":0.089407,"end_time":"2022-10-17T07:27:14.254130","exception":false,"start_time":"2022-10-17T07:27:14.164723","status":"completed"},"tags":[],"id":"ff5c6454"},"outputs":[],"source":["vocab_submission = predictions[0]"]},{"cell_type":"markdown","id":"5086a3ba","metadata":{"papermill":{"duration":0.07181,"end_time":"2022-10-17T07:27:14.399202","exception":false,"start_time":"2022-10-17T07:27:14.327392","status":"completed"},"tags":[],"id":"5086a3ba"},"source":["# 5. phraseology"]},{"cell_type":"code","execution_count":null,"id":"1b9300e3","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:27:14.544121Z","iopub.status.busy":"2022-10-17T07:27:14.543666Z","iopub.status.idle":"2022-10-17T07:27:14.673656Z","shell.execute_reply":"2022-10-17T07:27:14.672665Z"},"papermill":{"duration":0.204783,"end_time":"2022-10-17T07:27:14.676301","exception":false,"start_time":"2022-10-17T07:27:14.471518","status":"completed"},"tags":[],"id":"1b9300e3"},"outputs":[],"source":["train = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/train.csv')\n","test = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')\n","#cohesion data만 사용하기 위해서 나머지 평가항목 컬럼은 드롭\n","train.drop(['cohesion','grammar','syntax','vocabulary','conventions'], inplace=True, axis=1)\n","print('Number of trainig sentences: {:,}\\n'.format(train.shape[0]))\n","train.sample(10)"]},{"cell_type":"code","execution_count":null,"id":"30f8b25f","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:27:14.846168Z","iopub.status.busy":"2022-10-17T07:27:14.845595Z","iopub.status.idle":"2022-10-17T07:27:16.553360Z","shell.execute_reply":"2022-10-17T07:27:16.552339Z"},"papermill":{"duration":1.795647,"end_time":"2022-10-17T07:27:16.555870","exception":false,"start_time":"2022-10-17T07:27:14.760223","status":"completed"},"tags":[],"id":"30f8b25f"},"outputs":[],"source":["# Get the lists of sentences and their labels.\n","full_text = train.full_text.values\n","labels = train.phraseology.values\n","\n","from transformers import BertTokenizer\n","\n","print(\"Loading Bert Tokenizer...\")\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"id":"39dd222f","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:27:16.657832Z","iopub.status.busy":"2022-10-17T07:27:16.656769Z","iopub.status.idle":"2022-10-17T07:28:00.666136Z","shell.execute_reply":"2022-10-17T07:28:00.665137Z"},"papermill":{"duration":44.113938,"end_time":"2022-10-17T07:28:00.719854","exception":false,"start_time":"2022-10-17T07:27:16.605916","status":"completed"},"tags":[],"id":"39dd222f"},"outputs":[],"source":["# 모든 문장을 토큰화하고 토큰을 해당 단어 ID에 매핑\n","input_ids = []\n","attention_masks = []\n","\n","for text in full_text:\n","    encoded_dict = tokenizer.encode_plus(\n","                    text,#문장을 encode\n","                    add_special_tokens = True, #[CLS][SEP]토큰 추가\n","                    max_length = 256, #모든 문장 자르고 채우기 \n","                    # 'max_length'의 숫자는 어떻게 정하는 거지?\n","                    truncation=True,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,#Attention mask 만들기\n","                    return_tensors = 'pt' #파이토치 텐서로 리턴\n","                    )\n","    # 인코딩된 문장을 목록에 추가\n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # Attention mask\n","    attention_masks.append(encoded_dict['attention_mask'])\n","    \n","# 목록을 텐서로 변환\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# full_text 첫번째 문장을 IDs의 리스트로 출력해보기\n","print('Original: ', full_text[0])\n","print('Token IDs: ', input_ids[0])"]},{"cell_type":"code","execution_count":null,"id":"d9d9e83b","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:28:00.848907Z","iopub.status.busy":"2022-10-17T07:28:00.848506Z","iopub.status.idle":"2022-10-17T07:28:00.855652Z","shell.execute_reply":"2022-10-17T07:28:00.854733Z"},"papermill":{"duration":0.088955,"end_time":"2022-10-17T07:28:00.858313","exception":false,"start_time":"2022-10-17T07:28:00.769358","status":"completed"},"tags":[],"id":"d9d9e83b"},"outputs":[],"source":["from torch.utils.data import TensorDataset, random_split\n","# training inputs을 TensorDataset에 결합\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","#90-10 train-validation split.\n","#각 세트에 포함할 샘플 수를 계산\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","#무작위로 샘플을 선택하여 데이터 세트를 나눈다.\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"]},{"cell_type":"code","execution_count":null,"id":"4477409f","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-10-17T07:28:00.962322Z","iopub.status.busy":"2022-10-17T07:28:00.961950Z","iopub.status.idle":"2022-10-17T07:28:03.049217Z","shell.execute_reply":"2022-10-17T07:28:03.048285Z"},"jupyter":{"outputs_hidden":true},"papermill":{"duration":2.141353,"end_time":"2022-10-17T07:28:03.051426","exception":false,"start_time":"2022-10-17T07:28:00.910073","status":"completed"},"tags":[],"id":"4477409f"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 16\n","\n","train_dataloader = DataLoader(\n","            train_dataset, # training sample\n","            sampler = RandomSampler(train_dataset), #배치를 랜덤으로 선택\n","            batch_size = batch_size #훈련을 이 배치 사이즈로 함.\n","        )\n","\n","valid_dataloader = DataLoader(\n","                val_dataset,\n","                sampler = SequentialSampler(val_dataset), # 배치를 순차적으로 꺼냄.\n","                batch_size = batch_size #이 배치 사이즈로 평가하기\n","        )\n","\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","        \"bert-base-uncased\", # 12레이어 버트 모델 사용 (uncased)\n","        num_labels = 1, \n","        output_attentions = False, #모델이 attention 가중치를 리턴하는지\n","        output_hidden_states = False, #모델이 hidden states를 리턴하는지\n","        #attention_probs_dropout_prob=0.4,\n","        #hidden_dropout_prob=0.4,\n",")\n","\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"id":"72e38bad","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:28:03.155770Z","iopub.status.busy":"2022-10-17T07:28:03.154776Z","iopub.status.idle":"2022-10-17T07:28:03.163940Z","shell.execute_reply":"2022-10-17T07:28:03.162381Z"},"papermill":{"duration":0.063976,"end_time":"2022-10-17T07:28:03.166481","exception":false,"start_time":"2022-10-17T07:28:03.102505","status":"completed"},"tags":[],"id":"72e38bad"},"outputs":[],"source":["optimizer = AdamW(model.parameters(),\n","                 lr = 2e-5, #arg.learning_rate - 기본값은 5e-5이고 노트북에는 2e-5가 있다.\n","                 eps = 1e-8) #arg.adam_epsilon - 기본값은 1e-8"]},{"cell_type":"code","execution_count":null,"id":"66c5f2ec","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:28:03.271446Z","iopub.status.busy":"2022-10-17T07:28:03.270502Z","iopub.status.idle":"2022-10-17T07:28:03.277038Z","shell.execute_reply":"2022-10-17T07:28:03.276181Z"},"papermill":{"duration":0.06109,"end_time":"2022-10-17T07:28:03.279126","exception":false,"start_time":"2022-10-17T07:28:03.218036","status":"completed"},"tags":[],"id":"66c5f2ec"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 10\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                           num_warmup_steps = 0,\n","                                           num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"id":"72f6b124","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:28:03.383491Z","iopub.status.busy":"2022-10-17T07:28:03.383132Z","iopub.status.idle":"2022-10-17T07:28:03.389411Z","shell.execute_reply":"2022-10-17T07:28:03.388493Z"},"papermill":{"duration":0.060906,"end_time":"2022-10-17T07:28:03.391416","exception":false,"start_time":"2022-10-17T07:28:03.330510","status":"completed"},"tags":[],"id":"72f6b124"},"outputs":[],"source":["import numpy as np\n","\n","# 예측 vs 레이블의 정확도를 계산하는 함수\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    #가까운 초로 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    #format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"id":"798a1286","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:28:03.494241Z","iopub.status.busy":"2022-10-17T07:28:03.493886Z","iopub.status.idle":"2022-10-17T07:42:58.786371Z","shell.execute_reply":"2022-10-17T07:42:58.785463Z"},"papermill":{"duration":895.347451,"end_time":"2022-10-17T07:42:58.789030","exception":false,"start_time":"2022-10-17T07:28:03.441579","status":"completed"},"tags":[],"id":"798a1286"},"outputs":[],"source":["import random\n","import numpy as np\n","\n","#모든 곳에 seed 값을 설정하기\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# training , validation loss 같은 많은 수량을 저장\n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# 전체 실행에 대한 총 훈련시간을 측정함\n","total_t0 = time.time()\n","\n","#for each epoch...\n","for epoch_i in range(0, epochs):\n","    print(\"\")\n","    print('=======Epoch {:} / {:} ======='.format(epoch_i+1, epochs))\n","    print(\"Training...\")\n","    \n","    # training epoch가 걸리는 시간을 측정\n","    t0 =time.time()\n","    \n","    # epoch의 전체 loss를 재설정\n","    total_train_loss = 0\n","    \n","    # 모델을 학습 모드로 전환 \n","    # train은 *모드*를 변경할 뿐 훈련을 *수행*하지 않음.\n","    # 'dropoout' / 'batchnorm' 레이어는 훈련 중에 다르게 작동함\n","    model.train()\n","    \n","    # 훈련 데이터의 각 batch에 대해\n","    for step, batch in enumerate(train_dataloader):\n","        #40개 batch마다 진행률이 업데이트됨\n","        if step % 40 == 0 and not step == 0:\n","            #경과시간을 분 단위로 계산함\n","            elapsed = format_time(time.time()-t0)\n","            #진행상황 보고\n","            print(' Batch {:>5,} of {:>5,}. Elapsed: {:}'.format(step, len(train_dataloader), elapsed))\n","    \n","        #DataLoader에서 훈련 batch의 압축을 푼다\n","        #Batch의 압축을 풀면서 각 텐서를 사용하여 GPU에 복사함\n","        # 'to' method\n","\n","        #'batch'에는 세 개의 pytorch 텐서가 포함되어 있음.\n","        # input ids\n","        # attention masks\n","        # labels\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device, dtype=torch.float32)\n","\n","        # 모델 훈련을 수행하기 이전에 계산된 gradient는 지워야 함\n","        # backward pass : pytorch는 이 작업을 자동으로 수행하지 않는다.\n","        # gradient를 누적하는 것은 RNN을 훈련하는 동안 편리함\n","        model.zero_grad()\n","    \n","        # forward pass 수행 (정방향 전달)\n","        # 어떤 arguments에 따라 다른 수의 parameter를 return\n","        # arguments가 주어지고 flags가 설정됨\n","        # loss (label를 주었기 때문에) logits model\n","        # activation 전 출력\n","        loss, logits = model(b_input_ids,\n","                            token_type_ids=None,\n","                            attention_mask=b_input_mask,\n","                            labels = b_labels,\n","                            return_dict=False)\n","    \n","        #모든 Batch에 대한 훈련 loss를 누적하여 다음을 수행할 수 있다.\n","        # 마지막에 평균 loss를 계산함 loss는 다음을 포함하는 텐서이다.\n","        # single value; .item() function은 python 값을 return\n","        #print(loss.item(), type(loss.item()))\n","        total_train_loss += loss.item()\n","\n","        # backward pass로 gradients를 계산함\n","        loss.backward()\n","    \n","        # gradient 표준을 1.0으로 자름\n","        # \"exploding gradients\"문제에도 도움이 됨\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters and take a step using the computed gradient\n","        # Optimizer는 \"업데이트 rule\"(parameter가 어떻게 학습률과 기울기에 따라 수정되는지)지시\n","        optimizer.step()\n","        \n","        #update the learnin rate.\n","        scheduler.step()\n","    \n","    # 모든 batch에 대한 평균 손실을 계산함\n","    avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","    # 이 에포크가 얼마나 걸렸는지 측정\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","    \n","    # ======validation======\n","    #각 training epoch를 완료 후 성능을 측정함\n","    # VALIDATION SET\n","    \n","    print(\"\")\n","    print(\"Running Validation...\")\n","    \n","    t0 = time.time()\n","    \n","    # 모델을 평가 모드에. dropout layer가 다르게 작동함\n","    model.eval()\n","    \n","    #Tracking variables\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    \n","    #Evaluate data for one epoch\n","    for batch in valid_dataloader:\n","        #DataLoader에서 training batch 압축 해제\n","        #batch의 압축을 풀면서 각 텐서를 GPU에 복사\n","        #\"to\"method.\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device, dtype=torch.float32)\n","        \n","        #pytorch에게 compute 그래프를 구성하는동안 귀찮게 하지 말라고 하기\n","        #forward pass ; backprop (역전파) training에만 필요\n","        with torch.no_grad():\n","            \n","            #forward pass : logit predictions를 계산\n","            #token_type_ids는 \"segment ids\"와 동일\n","            # 2문장 작업에서 문장 1과 2를 구별\n","            \n","            #모델의 logits의 출력을 가져옴. \n","            #softmax 함수를 적용하기 전 값\n","            (loss, logits) = model(b_input_ids,\n","                                  token_type_ids=None,\n","                                  attention_mask=b_input_mask,\n","                                  labels=b_labels,\n","                                  return_dict=False)\n","        #validation loss를 누적\n","        total_eval_loss += loss.item()\n","        \n","        #logits, labels 는 CPU로 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # test 문장 batch 정확도를 계산하고 모든 batch에 누적함\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","    # validation 실행에 대한 최종 정확도를 보고함.\n","    avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","    \n","    #모든 배치에 대한 평균 loss를 계산\n","    avg_val_loss = total_eval_loss / len(valid_dataloader)\n","    \n","    #validation실행에 걸린 시간을 측정\n","    validation_time = format_time(time.time()-t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    \n","    # epoch의 모든 통계를 기록\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i+1,\n","            'Training Loss' : avg_train_loss,\n","            'Valid. Loss' : avg_val_loss,\n","            'Valid. Accur.' : avg_val_accuracy,\n","            'Training Time' : training_time,\n","            'Validation Time' : validation_time\n","        }\n","    )\n","    \n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"code","execution_count":null,"id":"1365c7b4","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:42:58.961847Z","iopub.status.busy":"2022-10-17T07:42:58.961374Z","iopub.status.idle":"2022-10-17T07:42:58.984158Z","shell.execute_reply":"2022-10-17T07:42:58.983237Z"},"papermill":{"duration":0.112225,"end_time":"2022-10-17T07:42:58.986741","exception":false,"start_time":"2022-10-17T07:42:58.874516","status":"completed"},"tags":[],"id":"1365c7b4"},"outputs":[],"source":["import pandas as pd\n","df_stats = pd.DataFrame(data=training_stats)\n","df_stats = df_stats.set_index('epoch')\n","df_stats"]},{"cell_type":"code","execution_count":null,"id":"bcab05e9","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:42:59.107267Z","iopub.status.busy":"2022-10-17T07:42:59.106904Z","iopub.status.idle":"2022-10-17T07:42:59.347665Z","shell.execute_reply":"2022-10-17T07:42:59.346681Z"},"papermill":{"duration":0.298798,"end_time":"2022-10-17T07:42:59.349927","exception":false,"start_time":"2022-10-17T07:42:59.051129","status":"completed"},"tags":[],"id":"bcab05e9"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","\n","sns.set(style='darkgrid')\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"c1f71c51","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:42:59.466431Z","iopub.status.busy":"2022-10-17T07:42:59.466075Z","iopub.status.idle":"2022-10-17T07:42:59.483927Z","shell.execute_reply":"2022-10-17T07:42:59.482828Z"},"papermill":{"duration":0.078948,"end_time":"2022-10-17T07:42:59.486209","exception":false,"start_time":"2022-10-17T07:42:59.407261","status":"completed"},"tags":[],"id":"c1f71c51"},"outputs":[],"source":["df_INNER_JOIN = pd.merge(test, submission, left_on='text_id', right_on='text_id', how='inner')\n","df_INNER_JOIN.drop(['grammar','cohesion','syntax','vocabulary','conventions'], inplace=True, axis=1)\n","labels = df_INNER_JOIN\n","labels"]},{"cell_type":"code","execution_count":null,"id":"b65fd277","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:42:59.599461Z","iopub.status.busy":"2022-10-17T07:42:59.599105Z","iopub.status.idle":"2022-10-17T07:42:59.651628Z","shell.execute_reply":"2022-10-17T07:42:59.650580Z"},"papermill":{"duration":0.11204,"end_time":"2022-10-17T07:42:59.654256","exception":false,"start_time":"2022-10-17T07:42:59.542216","status":"completed"},"tags":[],"id":"b65fd277"},"outputs":[],"source":["import pandas as pd\n","#test = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')\n","\n","#문장 수\n","print('Number of test sentences: {:,}\\n'.format(labels.shape[0]))\n","\n","#리스트 만들기\n","full_text = labels.full_text.values\n","labels = labels.phraseology.values\n","\n","#모든 문장을 토큰화하고 토큰을 해당 단어 IDs에 매핑한다.\n","input_ids=[]\n","attention_masks = []\n","\n","for text in full_text:\n","    encoded_dict = tokenizer.encode_plus(\n","                    text,#문장을 encode\n","                    add_special_tokens = True, #[CLS][SEP]토큰 추가\n","                    max_length = 256, #모든 문장 자르고 채우기 \n","                    truncation=True,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,#Attention mask 만들기\n","                    return_tensors = 'pt' #파이토치 텐서로 리턴\n","                    )\n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","batch_size = 32\n","\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"id":"31f9432d","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:42:59.768926Z","iopub.status.busy":"2022-10-17T07:42:59.768545Z","iopub.status.idle":"2022-10-17T07:42:59.809415Z","shell.execute_reply":"2022-10-17T07:42:59.808436Z"},"papermill":{"duration":0.101437,"end_time":"2022-10-17T07:42:59.811668","exception":false,"start_time":"2022-10-17T07:42:59.710231","status":"completed"},"tags":[],"id":"31f9432d"},"outputs":[],"source":["# 테스트 셋 예측\n","print('Prediction labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# put model in evaluation mode\n","model.eval()\n","\n","#Tracking Variables\n","predictions, true_labels = [], []\n","\n","#predict\n","for batch in prediction_dataloader:\n","    #GPU에 Batch 넣기\n","    batch= tuple(t.to(device) for t in batch)\n","    \n","    #DataLoader에서 input 압축풀기\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    #모델에 기울기를 계산하거나 저장하지 않도록 지시하여 메모리를 절약하고\n","    #예측속도향상\n","    with torch.no_grad():\n","        #Forward pass, calculate logit predictions\n","        outputs = model(b_input_ids, token_type_ids=None,\n","                       attention_mask=b_input_mask)\n","    logits = outputs[0]\n","    \n","    #Logits, labels를 cpu로 옮기기\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    #Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","    \n","print('    완료. ')\n","print(predictions)\n","print(true_labels)"]},{"cell_type":"code","execution_count":null,"id":"cb4cc0a0","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:42:59.925276Z","iopub.status.busy":"2022-10-17T07:42:59.924928Z","iopub.status.idle":"2022-10-17T07:42:59.929411Z","shell.execute_reply":"2022-10-17T07:42:59.928443Z"},"papermill":{"duration":0.063301,"end_time":"2022-10-17T07:42:59.931725","exception":false,"start_time":"2022-10-17T07:42:59.868424","status":"completed"},"tags":[],"id":"cb4cc0a0"},"outputs":[],"source":["phrase_submission = predictions[0]"]},{"cell_type":"markdown","id":"e80b6748","metadata":{"papermill":{"duration":0.056876,"end_time":"2022-10-17T07:43:00.045139","exception":false,"start_time":"2022-10-17T07:42:59.988263","status":"completed"},"tags":[],"id":"e80b6748"},"source":["# 6. Conventions"]},{"cell_type":"code","execution_count":null,"id":"1938b0f5","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:43:00.159347Z","iopub.status.busy":"2022-10-17T07:43:00.157595Z","iopub.status.idle":"2022-10-17T07:43:00.250772Z","shell.execute_reply":"2022-10-17T07:43:00.249641Z"},"papermill":{"duration":0.153033,"end_time":"2022-10-17T07:43:00.253784","exception":false,"start_time":"2022-10-17T07:43:00.100751","status":"completed"},"tags":[],"id":"1938b0f5"},"outputs":[],"source":["train = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/train.csv')\n","test = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')\n","#cohesion data만 사용하기 위해서 나머지 평가항목 컬럼은 드롭\n","train.drop(['cohesion','grammar','syntax','vocabulary','phraseology'], inplace=True, axis=1)\n","print('Number of trainig sentences: {:,}\\n'.format(train.shape[0]))\n","train.sample(10)"]},{"cell_type":"code","execution_count":null,"id":"d9d65364","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:43:00.368010Z","iopub.status.busy":"2022-10-17T07:43:00.366340Z","iopub.status.idle":"2022-10-17T07:43:02.067828Z","shell.execute_reply":"2022-10-17T07:43:02.066833Z"},"papermill":{"duration":1.760114,"end_time":"2022-10-17T07:43:02.070385","exception":false,"start_time":"2022-10-17T07:43:00.310271","status":"completed"},"tags":[],"id":"d9d65364"},"outputs":[],"source":["# Get the lists of sentences and their labels.\n","full_text = train.full_text.values\n","labels = train.conventions.values\n","\n","from transformers import BertTokenizer\n","\n","print(\"Loading Bert Tokenizer...\")\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"id":"566a614f","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-10-17T07:43:02.185522Z","iopub.status.busy":"2022-10-17T07:43:02.184610Z","iopub.status.idle":"2022-10-17T07:43:46.317465Z","shell.execute_reply":"2022-10-17T07:43:46.313119Z"},"jupyter":{"outputs_hidden":true},"papermill":{"duration":44.247548,"end_time":"2022-10-17T07:43:46.374969","exception":false,"start_time":"2022-10-17T07:43:02.127421","status":"completed"},"tags":[],"id":"566a614f"},"outputs":[],"source":["# 모든 문장을 토큰화하고 토큰을 해당 단어 ID에 매핑\n","input_ids = []\n","attention_masks = []\n","\n","for text in full_text:\n","    encoded_dict = tokenizer.encode_plus(\n","                    text,#문장을 encode\n","                    add_special_tokens = True, #[CLS][SEP]토큰 추가\n","                    max_length = 256, #모든 문장 자르고 채우기 \n","                    # 'max_length'의 숫자는 어떻게 정하는 거지?\n","                    truncation=True,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,#Attention mask 만들기\n","                    return_tensors = 'pt' #파이토치 텐서로 리턴\n","                    )\n","    # 인코딩된 문장을 목록에 추가\n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # Attention mask\n","    attention_masks.append(encoded_dict['attention_mask'])\n","    \n","# 목록을 텐서로 변환\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# full_text 첫번째 문장을 IDs의 리스트로 출력해보기\n","print('Original: ', full_text[0])\n","print('Token IDs: ', input_ids[0])"]},{"cell_type":"code","execution_count":null,"id":"a3e1bbbc","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:43:46.490635Z","iopub.status.busy":"2022-10-17T07:43:46.489622Z","iopub.status.idle":"2022-10-17T07:43:46.497989Z","shell.execute_reply":"2022-10-17T07:43:46.496568Z"},"papermill":{"duration":0.068685,"end_time":"2022-10-17T07:43:46.500518","exception":false,"start_time":"2022-10-17T07:43:46.431833","status":"completed"},"tags":[],"id":"a3e1bbbc"},"outputs":[],"source":["from torch.utils.data import TensorDataset, random_split\n","# training inputs을 TensorDataset에 결합\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","#90-10 train-validation split.\n","#각 세트에 포함할 샘플 수를 계산\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","#무작위로 샘플을 선택하여 데이터 세트를 나눈다.\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"]},{"cell_type":"code","execution_count":null,"id":"11b68169","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-10-17T07:43:46.617550Z","iopub.status.busy":"2022-10-17T07:43:46.616612Z","iopub.status.idle":"2022-10-17T07:43:48.887738Z","shell.execute_reply":"2022-10-17T07:43:48.886752Z"},"jupyter":{"outputs_hidden":true},"papermill":{"duration":2.331509,"end_time":"2022-10-17T07:43:48.889968","exception":false,"start_time":"2022-10-17T07:43:46.558459","status":"completed"},"tags":[],"id":"11b68169"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 16\n","\n","train_dataloader = DataLoader(\n","            train_dataset, # training sample\n","            sampler = RandomSampler(train_dataset), #배치를 랜덤으로 선택\n","            batch_size = batch_size #훈련을 이 배치 사이즈로 함.\n","        )\n","\n","valid_dataloader = DataLoader(\n","                val_dataset,\n","                sampler = SequentialSampler(val_dataset), # 배치를 순차적으로 꺼냄.\n","                batch_size = batch_size #이 배치 사이즈로 평가하기\n","        )\n","\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","        \"bert-base-uncased\", # 12레이어 버트 모델 사용 (uncased)\n","        num_labels = 1, \n","        output_attentions = False, #모델이 attention 가중치를 리턴하는지\n","        output_hidden_states = False, #모델이 hidden states를 리턴하는지\n","        #attention_probs_dropout_prob=0.4,\n","        #hidden_dropout_prob=0.4,\n",")\n","\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"id":"21427b30","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:43:49.008358Z","iopub.status.busy":"2022-10-17T07:43:49.006519Z","iopub.status.idle":"2022-10-17T07:43:49.016320Z","shell.execute_reply":"2022-10-17T07:43:49.015240Z"},"papermill":{"duration":0.071504,"end_time":"2022-10-17T07:43:49.018924","exception":false,"start_time":"2022-10-17T07:43:48.947420","status":"completed"},"tags":[],"id":"21427b30"},"outputs":[],"source":["optimizer = AdamW(model.parameters(),\n","                 lr = 2e-5, #arg.learning_rate - 기본값은 5e-5이고 노트북에는 2e-5가 있다.\n","                 eps = 1e-8) #arg.adam_epsilon - 기본값은 1e-8"]},{"cell_type":"code","execution_count":null,"id":"6224564f","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:43:49.135536Z","iopub.status.busy":"2022-10-17T07:43:49.135168Z","iopub.status.idle":"2022-10-17T07:43:49.141756Z","shell.execute_reply":"2022-10-17T07:43:49.140828Z"},"papermill":{"duration":0.067473,"end_time":"2022-10-17T07:43:49.143966","exception":false,"start_time":"2022-10-17T07:43:49.076493","status":"completed"},"tags":[],"id":"6224564f"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 10\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                           num_warmup_steps = 0,\n","                                           num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"id":"722f1a36","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:43:49.261104Z","iopub.status.busy":"2022-10-17T07:43:49.260183Z","iopub.status.idle":"2022-10-17T07:43:49.266950Z","shell.execute_reply":"2022-10-17T07:43:49.266065Z"},"papermill":{"duration":0.067071,"end_time":"2022-10-17T07:43:49.268951","exception":false,"start_time":"2022-10-17T07:43:49.201880","status":"completed"},"tags":[],"id":"722f1a36"},"outputs":[],"source":["import numpy as np\n","\n","# 예측 vs 레이블의 정확도를 계산하는 함수\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    #가까운 초로 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    #format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"id":"2fe21386","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:43:49.384963Z","iopub.status.busy":"2022-10-17T07:43:49.384598Z","iopub.status.idle":"2022-10-17T07:58:44.843075Z","shell.execute_reply":"2022-10-17T07:58:44.841779Z"},"papermill":{"duration":895.519668,"end_time":"2022-10-17T07:58:44.845265","exception":false,"start_time":"2022-10-17T07:43:49.325597","status":"completed"},"tags":[],"id":"2fe21386"},"outputs":[],"source":["import random\n","import numpy as np\n","\n","#모든 곳에 seed 값을 설정하기\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# training , validation loss 같은 많은 수량을 저장\n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# 전체 실행에 대한 총 훈련시간을 측정함\n","total_t0 = time.time()\n","\n","#for each epoch...\n","for epoch_i in range(0, epochs):\n","    print(\"\")\n","    print('=======Epoch {:} / {:} ======='.format(epoch_i+1, epochs))\n","    print(\"Training...\")\n","    \n","    # training epoch가 걸리는 시간을 측정\n","    t0 =time.time()\n","    \n","    # epoch의 전체 loss를 재설정\n","    total_train_loss = 0\n","    \n","    # 모델을 학습 모드로 전환 \n","    # train은 *모드*를 변경할 뿐 훈련을 *수행*하지 않음.\n","    # 'dropoout' / 'batchnorm' 레이어는 훈련 중에 다르게 작동함\n","    model.train()\n","    \n","    # 훈련 데이터의 각 batch에 대해\n","    for step, batch in enumerate(train_dataloader):\n","        #40개 batch마다 진행률이 업데이트됨\n","        if step % 40 == 0 and not step == 0:\n","            #경과시간을 분 단위로 계산함\n","            elapsed = format_time(time.time()-t0)\n","            #진행상황 보고\n","            print(' Batch {:>5,} of {:>5,}. Elapsed: {:}'.format(step, len(train_dataloader), elapsed))\n","    \n","        #DataLoader에서 훈련 batch의 압축을 푼다\n","        #Batch의 압축을 풀면서 각 텐서를 사용하여 GPU에 복사함\n","        # 'to' method\n","\n","        #'batch'에는 세 개의 pytorch 텐서가 포함되어 있음.\n","        # input ids\n","        # attention masks\n","        # labels\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device, dtype=torch.float32)\n","\n","        # 모델 훈련을 수행하기 이전에 계산된 gradient는 지워야 함\n","        # backward pass : pytorch는 이 작업을 자동으로 수행하지 않는다.\n","        # gradient를 누적하는 것은 RNN을 훈련하는 동안 편리함\n","        model.zero_grad()\n","    \n","        # forward pass 수행 (정방향 전달)\n","        # 어떤 arguments에 따라 다른 수의 parameter를 return\n","        # arguments가 주어지고 flags가 설정됨\n","        # loss (label를 주었기 때문에) logits model\n","        # activation 전 출력\n","        loss, logits = model(b_input_ids,\n","                            token_type_ids=None,\n","                            attention_mask=b_input_mask,\n","                            labels = b_labels,\n","                            return_dict=False)\n","    \n","        #모든 Batch에 대한 훈련 loss를 누적하여 다음을 수행할 수 있다.\n","        # 마지막에 평균 loss를 계산함 loss는 다음을 포함하는 텐서이다.\n","        # single value; .item() function은 python 값을 return\n","        #print(loss.item(), type(loss.item()))\n","        total_train_loss += loss.item()\n","\n","        # backward pass로 gradients를 계산함\n","        loss.backward()\n","    \n","        # gradient 표준을 1.0으로 자름\n","        # \"exploding gradients\"문제에도 도움이 됨\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters and take a step using the computed gradient\n","        # Optimizer는 \"업데이트 rule\"(parameter가 어떻게 학습률과 기울기에 따라 수정되는지)지시\n","        optimizer.step()\n","        \n","        #update the learnin rate.\n","        scheduler.step()\n","    \n","    # 모든 batch에 대한 평균 손실을 계산함\n","    avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","    # 이 에포크가 얼마나 걸렸는지 측정\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","    \n","    # ======validation======\n","    #각 training epoch를 완료 후 성능을 측정함\n","    # VALIDATION SET\n","    \n","    print(\"\")\n","    print(\"Running Validation...\")\n","    \n","    t0 = time.time()\n","    \n","    # 모델을 평가 모드에. dropout layer가 다르게 작동함\n","    model.eval()\n","    \n","    #Tracking variables\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    \n","    #Evaluate data for one epoch\n","    for batch in valid_dataloader:\n","        #DataLoader에서 training batch 압축 해제\n","        #batch의 압축을 풀면서 각 텐서를 GPU에 복사\n","        #\"to\"method.\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device, dtype=torch.float32)\n","        \n","        #pytorch에게 compute 그래프를 구성하는동안 귀찮게 하지 말라고 하기\n","        #forward pass ; backprop (역전파) training에만 필요\n","        with torch.no_grad():\n","            \n","            #forward pass : logit predictions를 계산\n","            #token_type_ids는 \"segment ids\"와 동일\n","            # 2문장 작업에서 문장 1과 2를 구별\n","            \n","            #모델의 logits의 출력을 가져옴. \n","            #softmax 함수를 적용하기 전 값\n","            (loss, logits) = model(b_input_ids,\n","                                  token_type_ids=None,\n","                                  attention_mask=b_input_mask,\n","                                  labels=b_labels,\n","                                  return_dict=False)\n","        #validation loss를 누적\n","        total_eval_loss += loss.item()\n","        \n","        #logits, labels 는 CPU로 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # test 문장 batch 정확도를 계산하고 모든 batch에 누적함\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","    # validation 실행에 대한 최종 정확도를 보고함.\n","    avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","    \n","    #모든 배치에 대한 평균 loss를 계산\n","    avg_val_loss = total_eval_loss / len(valid_dataloader)\n","    \n","    #validation실행에 걸린 시간을 측정\n","    validation_time = format_time(time.time()-t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    \n","    # epoch의 모든 통계를 기록\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i+1,\n","            'Training Loss' : avg_train_loss,\n","            'Valid. Loss' : avg_val_loss,\n","            'Valid. Accur.' : avg_val_accuracy,\n","            'Training Time' : training_time,\n","            'Validation Time' : validation_time\n","        }\n","    )\n","    \n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"code","execution_count":null,"id":"74fcc61e","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:58:44.966985Z","iopub.status.busy":"2022-10-17T07:58:44.966617Z","iopub.status.idle":"2022-10-17T07:58:44.982046Z","shell.execute_reply":"2022-10-17T07:58:44.981112Z"},"papermill":{"duration":0.077933,"end_time":"2022-10-17T07:58:44.984047","exception":false,"start_time":"2022-10-17T07:58:44.906114","status":"completed"},"tags":[],"id":"74fcc61e"},"outputs":[],"source":["import pandas as pd\n","df_stats = pd.DataFrame(data=training_stats)\n","df_stats = df_stats.set_index('epoch')\n","df_stats"]},{"cell_type":"code","execution_count":null,"id":"491789a4","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:58:45.106948Z","iopub.status.busy":"2022-10-17T07:58:45.106572Z","iopub.status.idle":"2022-10-17T07:58:45.338822Z","shell.execute_reply":"2022-10-17T07:58:45.337925Z"},"papermill":{"duration":0.296443,"end_time":"2022-10-17T07:58:45.340909","exception":false,"start_time":"2022-10-17T07:58:45.044466","status":"completed"},"tags":[],"id":"491789a4"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import seaborn as sns\n","\n","sns.set(style='darkgrid')\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"023c009e","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:58:45.465351Z","iopub.status.busy":"2022-10-17T07:58:45.464987Z","iopub.status.idle":"2022-10-17T07:58:45.480125Z","shell.execute_reply":"2022-10-17T07:58:45.479112Z"},"papermill":{"duration":0.078734,"end_time":"2022-10-17T07:58:45.482042","exception":false,"start_time":"2022-10-17T07:58:45.403308","status":"completed"},"tags":[],"id":"023c009e"},"outputs":[],"source":["df_INNER_JOIN = pd.merge(test, submission, left_on='text_id', right_on='text_id', how='inner')\n","df_INNER_JOIN.drop(['grammar','cohesion','syntax','vocabulary','phraseology'], inplace=True, axis=1)\n","labels = df_INNER_JOIN\n","labels"]},{"cell_type":"code","execution_count":null,"id":"093dcb56","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:58:45.614351Z","iopub.status.busy":"2022-10-17T07:58:45.613974Z","iopub.status.idle":"2022-10-17T07:58:45.667176Z","shell.execute_reply":"2022-10-17T07:58:45.666198Z"},"papermill":{"duration":0.119119,"end_time":"2022-10-17T07:58:45.670210","exception":false,"start_time":"2022-10-17T07:58:45.551091","status":"completed"},"tags":[],"id":"093dcb56"},"outputs":[],"source":["import pandas as pd\n","#test = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')\n","\n","#문장 수\n","print('Number of test sentences: {:,}\\n'.format(labels.shape[0]))\n","\n","#리스트 만들기\n","full_text = labels.full_text.values\n","labels = labels.conventions.values\n","\n","#모든 문장을 토큰화하고 토큰을 해당 단어 IDs에 매핑한다.\n","input_ids=[]\n","attention_masks = []\n","\n","for text in full_text:\n","    encoded_dict = tokenizer.encode_plus(\n","                    text,#문장을 encode\n","                    add_special_tokens = True, #[CLS][SEP]토큰 추가\n","                    max_length = 256, #모든 문장 자르고 채우기 \n","                    truncation=True,\n","                    pad_to_max_length = True,\n","                    return_attention_mask = True,#Attention mask 만들기\n","                    return_tensors = 'pt' #파이토치 텐서로 리턴\n","                    )\n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","batch_size = 32\n","\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"id":"da61798e","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:58:45.851964Z","iopub.status.busy":"2022-10-17T07:58:45.850878Z","iopub.status.idle":"2022-10-17T07:58:45.890540Z","shell.execute_reply":"2022-10-17T07:58:45.888978Z"},"papermill":{"duration":0.104204,"end_time":"2022-10-17T07:58:45.892762","exception":false,"start_time":"2022-10-17T07:58:45.788558","status":"completed"},"tags":[],"id":"da61798e"},"outputs":[],"source":["# 테스트 셋 예측\n","print('Prediction labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# put model in evaluation mode\n","model.eval()\n","\n","#Tracking Variables\n","predictions, true_labels = [], []\n","\n","#predict\n","for batch in prediction_dataloader:\n","    #GPU에 Batch 넣기\n","    batch= tuple(t.to(device) for t in batch)\n","    \n","    #DataLoader에서 input 압축풀기\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    #모델에 기울기를 계산하거나 저장하지 않도록 지시하여 메모리를 절약하고\n","    #예측속도향상\n","    with torch.no_grad():\n","        #Forward pass, calculate logit predictions\n","        outputs = model(b_input_ids, token_type_ids=None,\n","                       attention_mask=b_input_mask)\n","    logits = outputs[0]\n","    \n","    #Logits, labels를 cpu로 옮기기\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    #Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","    \n","print('    완료. ')\n","print(predictions)\n","print(true_labels)"]},{"cell_type":"code","execution_count":null,"id":"f2003c72","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:58:46.018666Z","iopub.status.busy":"2022-10-17T07:58:46.018314Z","iopub.status.idle":"2022-10-17T07:58:46.022922Z","shell.execute_reply":"2022-10-17T07:58:46.021916Z"},"papermill":{"duration":0.069496,"end_time":"2022-10-17T07:58:46.024896","exception":false,"start_time":"2022-10-17T07:58:45.955400","status":"completed"},"tags":[],"id":"f2003c72"},"outputs":[],"source":["convent_submission = predictions[0]"]},{"cell_type":"code","execution_count":null,"id":"2cbbf25a","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:58:46.150022Z","iopub.status.busy":"2022-10-17T07:58:46.149439Z","iopub.status.idle":"2022-10-17T07:58:46.157971Z","shell.execute_reply":"2022-10-17T07:58:46.157089Z"},"papermill":{"duration":0.073815,"end_time":"2022-10-17T07:58:46.160082","exception":false,"start_time":"2022-10-17T07:58:46.086267","status":"completed"},"tags":[],"id":"2cbbf25a"},"outputs":[],"source":["submission.cohesion = cohesion_submission\n","submission.grammar = grammar_submission\n","submission.syntax = syntax_submission\n","submission.vocabulary = vocab_submission\n","submission.phraseology = phrase_submission\n","submission.conventions = convent_submission"]},{"cell_type":"code","execution_count":null,"id":"28abd56b","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:58:46.284164Z","iopub.status.busy":"2022-10-17T07:58:46.283799Z","iopub.status.idle":"2022-10-17T07:58:46.295976Z","shell.execute_reply":"2022-10-17T07:58:46.295025Z"},"papermill":{"duration":0.075885,"end_time":"2022-10-17T07:58:46.297870","exception":false,"start_time":"2022-10-17T07:58:46.221985","status":"completed"},"tags":[],"id":"28abd56b"},"outputs":[],"source":["submission"]},{"cell_type":"code","execution_count":null,"id":"32776496","metadata":{"execution":{"iopub.execute_input":"2022-10-17T07:58:46.423409Z","iopub.status.busy":"2022-10-17T07:58:46.422730Z","iopub.status.idle":"2022-10-17T07:58:46.439479Z","shell.execute_reply":"2022-10-17T07:58:46.438491Z"},"papermill":{"duration":0.082079,"end_time":"2022-10-17T07:58:46.441577","exception":false,"start_time":"2022-10-17T07:58:46.359498","status":"completed"},"tags":[],"id":"32776496"},"outputs":[],"source":["submission.to_csv('submission.csv',index=False)\n","submission = pd.read_csv('submission.csv')\n","submission"]},{"cell_type":"code","execution_count":null,"id":"6d228a0a","metadata":{"papermill":{"duration":0.062117,"end_time":"2022-10-17T07:58:46.567161","exception":false,"start_time":"2022-10-17T07:58:46.505044","status":"completed"},"tags":[],"id":"6d228a0a"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":5947.984309,"end_time":"2022-10-17T07:58:50.208722","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-10-17T06:19:42.224413","version":"2.3.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bd0f95f7219840d79aa28b4c10ba7f9f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b4647c678b342cb90b0c3f12a2cea35","IPY_MODEL_f0c794c598354ff1a32f2693a4616d25","IPY_MODEL_08beaba592f24cffaa0bceb2cbf22543"],"layout":"IPY_MODEL_146c1ece5c934e4ead5a79e16c6c083b"}},"0b4647c678b342cb90b0c3f12a2cea35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b23c63eb24404de592859f483666f8ab","placeholder":"​","style":"IPY_MODEL_ad60a578e60e4ac8a45f4d57bb84553d","value":"Downloading: 100%"}},"f0c794c598354ff1a32f2693a4616d25":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cba5445cbf349d8ade2563d159c7367","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5178dc046714a459a7939c0c75da907","value":231508}},"08beaba592f24cffaa0bceb2cbf22543":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02ed582a2f8d4279b03eacaf17c608f1","placeholder":"​","style":"IPY_MODEL_6cf05efd7c2e49cabe5bbf9116d9db0d","value":" 232k/232k [00:00&lt;00:00, 269kB/s]"}},"146c1ece5c934e4ead5a79e16c6c083b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b23c63eb24404de592859f483666f8ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad60a578e60e4ac8a45f4d57bb84553d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cba5445cbf349d8ade2563d159c7367":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5178dc046714a459a7939c0c75da907":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"02ed582a2f8d4279b03eacaf17c608f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cf05efd7c2e49cabe5bbf9116d9db0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1b574c56b264ca2af87375c8fca9abd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8c93d3b58588453bbd096672d25f162e","IPY_MODEL_bf61d9eeff534fbca45a295f2e509b6e","IPY_MODEL_af9de7feb5fb4f1ba3ffbeea54754506"],"layout":"IPY_MODEL_6d55c4511dfa4a0a85882fec96a099c5"}},"8c93d3b58588453bbd096672d25f162e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9df4c73bb5f5459b82f13716325f809b","placeholder":"​","style":"IPY_MODEL_27b1dbce02a548828050665a820243ac","value":"Downloading: 100%"}},"bf61d9eeff534fbca45a295f2e509b6e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a20319801e91467fabf58c53254fac25","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58b0b9b603004a6abe0eeb2a67c8ae97","value":28}},"af9de7feb5fb4f1ba3ffbeea54754506":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e961b5d6068f4ed3b26e19538f7743da","placeholder":"​","style":"IPY_MODEL_db62e87b0a774949a42668ff62ee6cdb","value":" 28.0/28.0 [00:00&lt;00:00, 908B/s]"}},"6d55c4511dfa4a0a85882fec96a099c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9df4c73bb5f5459b82f13716325f809b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27b1dbce02a548828050665a820243ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a20319801e91467fabf58c53254fac25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58b0b9b603004a6abe0eeb2a67c8ae97":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e961b5d6068f4ed3b26e19538f7743da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db62e87b0a774949a42668ff62ee6cdb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05757a97bb81436b87385bc79ff1694e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2fdd488e020c4ace9b53859ebecddb46","IPY_MODEL_cfe4295a0ece412fbb4b21bc5effb740","IPY_MODEL_5c513beab1d04e78b7a7822aad19f394"],"layout":"IPY_MODEL_d54abf991b164dd38732ab5415cc6ec4"}},"2fdd488e020c4ace9b53859ebecddb46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8638a2e18dad493683d8f1bf6b1d5604","placeholder":"​","style":"IPY_MODEL_6f45571411e14bfe8172d4497f6f5e61","value":"Downloading: 100%"}},"cfe4295a0ece412fbb4b21bc5effb740":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88bf39642baa419ca39abb0f1b902236","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_704a26a08d564e84aab058fe887691b6","value":570}},"5c513beab1d04e78b7a7822aad19f394":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba04869b467f41bc8164de85d8411f8f","placeholder":"​","style":"IPY_MODEL_cf264868eb18473dabbdd3ed71052129","value":" 570/570 [00:00&lt;00:00, 5.97kB/s]"}},"d54abf991b164dd38732ab5415cc6ec4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8638a2e18dad493683d8f1bf6b1d5604":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f45571411e14bfe8172d4497f6f5e61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88bf39642baa419ca39abb0f1b902236":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"704a26a08d564e84aab058fe887691b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba04869b467f41bc8164de85d8411f8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf264868eb18473dabbdd3ed71052129":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c526452691664d61a7ba77028a2492a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6ba7f618de7489c8c9f3fa2223eaf68","IPY_MODEL_56b070ef2e484a3da315d74d1e111308","IPY_MODEL_c249421b8c824a7098b592adc91be5cf"],"layout":"IPY_MODEL_8847d3ac0118497485b222bc46ba74c6"}},"d6ba7f618de7489c8c9f3fa2223eaf68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c86a1fcdef84ffb8fd3ca1fb3f7a32a","placeholder":"​","style":"IPY_MODEL_dc6311baa40b4c93bde57073dde4ca8c","value":"Downloading: 100%"}},"56b070ef2e484a3da315d74d1e111308":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_084cc3a715174a76b17527ef22387e98","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_53fb3a206cca45bc98dee2aafbb9e7e7","value":440473133}},"c249421b8c824a7098b592adc91be5cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bf658ef6a464580882ca3e03f56e679","placeholder":"​","style":"IPY_MODEL_892b39e2a7964dbeb71d196e90120133","value":" 440M/440M [00:07&lt;00:00, 55.8MB/s]"}},"8847d3ac0118497485b222bc46ba74c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c86a1fcdef84ffb8fd3ca1fb3f7a32a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc6311baa40b4c93bde57073dde4ca8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"084cc3a715174a76b17527ef22387e98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53fb3a206cca45bc98dee2aafbb9e7e7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9bf658ef6a464580882ca3e03f56e679":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"892b39e2a7964dbeb71d196e90120133":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}